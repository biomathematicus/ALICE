{
  "CONFIG": {
    "general_instructions": "This project is an AI assistant for a course. The output of each request must be a response that guides a student in understanding a topic.",
    "description": "The Data and AI Intensive Research with Rigor and Reproducibility (DAIR-3) program is funded by Award 5R25GM151182 of the National Institute of General Medical Sciences, one of the 27 institutes of the National Institutes of Health of the United States. The principal investigators are Jing Liu (University of Michigan) and Juan B. Guti\u00c3\u00a9rrez (University of Texas at San Antonio). \\nThe rigor of scientific research and the reproducibility of research results are essential for the validity of research findings and the trustworthiness of science. However, research rigor and reproducibility remains a significant challenge across scientific fields, especially for research with complex data types from heterogeneous sources, and long data manipulation pipelines. This is especially critical as data science and artificial intelligence (AI) methods emerge at lightning speed and researchers scramble to seize the opportunities that the new methods bring.  \\n\\nWhile researchers recognize the importance of rigor and reproducibility, they often lack the resources and the technical know-how to achieve this consistently in practice. With funding from the National Institutes of Health, a multi-university team offers a nationwide program to equip faculty and technical staff in biomedical sciences with the skills needed to improve the rigor and reproducibility of their research, and help them transfer such skills to their trainees. \\n\\nTrainees will then be guided over a one-year period to incorporate the newly acquired mindset, skills and tools into their research; and develop training for their own institutions.  \\n\\nThe DAIR3 team and instructors include faculty and staff research leaders from the University of Michigan, the College of William and Mary, Jackson State University, and University of Texas San Antonio. This highly diverse team will model the culture of diversity that we promote, and will support trainees who are demographically, professionally and scientifically diverse, and are from a diverse range of institutions, including those with limited resources.",
    "harmonizer_code": "gpt-4o",
    "harmonizer_temperature": 0.1,
    "harmonizer_name": "Harmonizer"
  },
  "MODELS": [
    {
      "model_code": "gpt-4o",
      "model_name": "OpenAI GPT 4o",
      "temperature": 0.15,
      "agent_name": "ALICE AI Agent"
    }
  ],
  "knowledgeBase": [
    {
      "file": "C:\\temp\\Units\\UNIT_5 - Reproducible Workflows - Johann and Greg\\1_goals.pdf",
      "summary": "Reproducible:  Entir e analysis f ully and e xactly r eproducible User friendly:  Easy t o ac cess, inst all, run Transp arent: Easy t o inspect , under stand, modify Reusable:  Other s may build upon the pr oject Version c ontr olled Archiv edFULLY AND EXACTLY REPRODUCIBLE The \u201cmost original\u201d dat a should be av ailable Include all c ode nec essar y to get from the original dat a to the final results The c ode should dir ectly pr oduc e the plots / t ables / number s in the paper All so\u0000w are dependencies should be specified and ide ally included with the c ode Random seeds specified etc.USER FRIENDLY Code e asy t o ac cess and inspect , ide ally e ven without do wnlo ading Should r equir e minimal e\ufb00 ort f or a user t o inst all and run Should c ause minimal disruption t o a user \u2019s resour ces (e.g., not inst all unw anted so\u0000w are on their syst em) etc.T R A N S P A R E N T Code should be w ell-or ganiz ed and document ed, ide ally in a notebook f ormat ."
    },
    {
      "file": "C:\\temp\\Units\\UNIT_5 - Reproducible Workflows - Johann and Greg\\2_notebooks.pdf",
      "summary": "(Additionally , one c an dir ectly embed html) One c an also use e xtended mark down languag es lik e myst which enables f eatures lik e references, figur es, bibliogr aphies, \u2026 In any c ase, jupyter\u2019s mark down enables rich-t ext comment ary on both the c ode and the output (In f act, this pr esent ation is writt en in markdown)markdownBASIC MARKDOWN BASIC MARKDOWN BASIC MARKDOWN BASIC MARKDOWN BASIC MARKDOWN BASIC MARKDOWN BASIC MARKDOWN BASIC MARKDOWN CODE: BASIC OUTPUT Interweaved mark down c omment ary, int ersper ses code and in-line output , e.g., CODE: INTERACTIVE WIDGETS Example: in R using plotly: CODE: LANGUAGES Ther e ar e many  languag e backends that jup yter can use. Quarto documents c an be edit ed in Jupyter.QUARTO AND JUPYTER quarto and jupyter di\ufb00 erences: quarto uses mark down-lik e .qmd, jupyter uses JSON .ipynb quarto geared t owards publishing (e.g.\u00a0figur es, t ables, r eferences, \u2026), use myst for jupyter .qmd doesn\u2019t st ore output , .ipynb does A use ful idiom : do analysis in jupyter and mirr or int o ipynb, md, R, qmd,\u2026NOTEBOOKS AND REPRODUCIBILITY Why do not ebooks help reproducibility : literate pr ogramming: int erweaving c ode/ comment ary/output allo ws rich c omment ary on c ode, output develops a narr ativ e that is e asy t o read document diagnostic/ explor atory/micr o-decision analysis keeps c omment ary/output close t o code good t ool f or playing with c ode, immediat ely obser ving outputNOTEBOOKS AND REPRODUCIBILITY Why do not ebooks help reproducibility : good f or sho wcasing r esults and int eroper ability can be c onv erted t o many shar able f ormats (html, pdf , \u2026) can c onv ert among  the not ebook f ormats creates a r eproducible r ecord code aut omatic ally g ener ates results fr om dat a this f orces document ation on ho w the r esults w ere pr oduc edNOTEBOOKS AND REPRODUCIBILITY Why do not ebooks help reproducibility : promot es g ood c ode or ganiz ation via chunking so\u0000w are/formats not pr oprie tary, easy t o distribut eSOME POTENTIAL DOWNSIDES While c ode not ebooks c an be gr eat, ther e ar e some potential issues , including: 1. chunks c an be run in non-sequential or der, making them not reproducible (soln: r e-run all analysis at the end sequentially) 2. saved f ormat of not ebook may mak e version c ontr ol di\ufb00icult (soln: jupytext) 3. not gr eat for non-int eractiv e envir onments (soln: jup ytext) 4. conv ersion among v arious f ormats isn\u2019t 100% f ool-pr oofDISCUSSION Do y ou use not ebooks r egularly?"
    },
    {
      "file": "C:\\temp\\Units\\UNIT_5 - Reproducible Workflows - Johann and Greg\\3_programming.pdf",
      "summary": "In this module, w e cover five idioms that c an help enhanc e reproducibility e veryday: 1. writ e it in c ode, not the c onsole 2. don\u2019t r epe at y ourself, use f unctions 3. avoid magic number s, expose them 4. cache int ermediat e results 5. seed r andom number s1. Adv antages of DRY: 1. mak es y our c ode e asier t o chang e / maint ain (av oiding err ors) 2. mak es y our c ode e asier t o under stand 3. remo ves some clutt er fr om c ode2. \u201cPremat ure optimiz ation is the r oot of all e vil.\u201d \u2013 Don Knuth In this p articular c ase, f actoring out the c all t o lm and knnreg incr eased the number of lines of c ode and made it har der t o read.3. Naming magic number s and other p arame ter choic es 1. mak es the c ode e asier t o read and mor e self -documenting 2. enhanc es reproducibility b y flagging these analysis choic es, exposing them via an int erface for e asy (thir d party) experiment ation4. CACHE INTERMEDIATE RESULTS Ideally, reproducible analysis t akes dat a from (e.g.) raw sour ces, t o cleaned-up pr ocessed dat a, to final r esults/plots/ output If this is all one long script that pr ocesses dat a without saving any intermediat e results along the w ay, it c an be di\ufb00icult t o reproduc e the analysis.4. CACHE INTERMEDIATE RESULTS It is g ood pr actic e to cache  intermediat e results t o enhanc e reproducibility b y creating multiple entr y-points int o the analysis. BEWARE: PARALLELIZATION A very reproducible w ay is t o use the future.apply package: This will still w ork e ven if w e chang e paralleliz ation p arame ters.BONUS: LINTING AND STYLING linting  is checking y our c ode \u2019s adher ence to a stylistic guidelines depending on the languag e, ther e ar e packages that will do this automatic ally f or y ou e.g.\u00a0lintr in R ther e ar e also p ackages that will aut omatic ally find and fix these issues f or y ou e.g.\u00a0styler in RBONUS: LINTING AND STYLING This is ugly , let\u2019s fix it . Go b ack periodic ally, and do things lik e: dele te those c omment ed out lines refactor c opy-and-p asted c ode chunks, rename y our poorly named v ariables, break ap art c ode int o be tter logic ally struct ured chunks/scripts 2. testing y our pipeline (fr om soup t o nuts) dele te all y our c ached int ermediat e results clear y our not ebook outputs remo ve plots/ data pr oduc ed re-run y our whole analysis (ide ally via a makefile) 3. code r eview hav e someone else look at y our c ode 4. avoid pr oprie tary so\u0000w areDISCUSSION How o\u0000 en do y ou g o back and cle an-up c ode ?"
    },
    {
      "file": "C:\\temp\\Units\\UNIT_5 - Reproducible Workflows - Johann and Greg\\4_versioncontrol.pdf",
      "summary": "Onc e a file has been c ommitt ed t o your r eposit ory, it gener ally c an\u2019t be remo ved.GIT \u2013 DESIGNED FOR TEXT FILES When editing t ext files, git only sav es di\ufb00 er enc es This mak es ar chiving simple t ext files v ery sp ace e\ufb00icient Other files g et comple tely r e-sav ed at e ach c ommit git is designed f or c ode, not other asse ts (output , figur es, dat a, \u2026)WHAT TO KEEP (TRACK) mark down files scripts mak efiles simple t ext document ation etc.WHAT TO IGNORE binar y files pdf, jpg, e tc."
    },
    {
      "file": "C:\\temp\\Units\\UNIT_5 - Reproducible Workflows - Johann and Greg\\5_containers.pdf",
      "summary": "Put e verything on githubTHEN TIME PASSES\u2026 When r evising the p aper , we updat ed our c ode, r e-ran the analysis, and\u2026 got v ery di\ufb00 erent r esults f or a me thod w e had c omp ared ag ainst A\u0000er a gr eat de al of debugging, w e disc overed a dependenc y of a dependenc y of a dependenc y had chang ed Mor al: Saving y our c ode is not enough. Packages Fast Easy t o shar e Single file Cross platf orm (Linux, Mac, Windo ws)KEY INGREDIENTS Base imag e Dock erfile or Cont ainerfile Your e xisting analysisBASE IMAGE Minimal Ubunt u R (R ocker) Jup yter Many , many mor eDOCKERFILE R U N N I N G ANOTHER EXAMPLE BRIEF TUTORIAL Many gr eat tutorials online Our p aper: https:// jdssv .org/inde x.php/ jdssv /article/vie w/53T U T O R I A L Using podman, a light -weight impl."
    },
    {
      "file": "C:\\temp\\Units\\UNIT_5 - Reproducible Workflows - Johann and Greg\\6_pipeline.pdf",
      "summary": "Automat e everything Retain the original dat a, but k eep it cle arly sep arated fr om \u201c cleaned\u201d dataFINAL RESULTS (NUTS) \u201cFinal r esults \u201d: Plots Tables Numeric al results (e.g., \u201c \u201d) p=0.02 All final r esults should be output v erbatim b y your analysisFINAL RESULTS (NUTS) Numeric al results (e.g., \u201c \u201d) Embed in a not ebook Should be e asy t o find Tables Plots  (ggplot2) If you absolut ely must edit b y hand: diff and patchp=0.02 xtable kable stargazer annotate() tikzDeviceS H A R I N GEASILY ACCESSIBLE To mak e your analysis e asily ac cessible, Post the c ode some wher e it is e asy t o br owse (e.g., github) Post a f ully self -contained dock er imag e Post (or link t o) the dat a some wher e you c an do wnlo ad it dir ectly Post any p ackages on CR AN, PyPi, e tc."
    },
    {
      "file": "C:\\temp\\Units\\UNIT_5 - Reproducible Workflows - Johann and Greg\\Pre-reading material for participants\\JDSSV_V3_I1.pdf",
      "summary": "Open sharing not only allows results to be disseminated and built upon, but also allows scrutiny and verification of the research and is fundamental to the scientific process itself. For example, most of the journals sponsored by the Interna- tional Statistical Association and American Statistical Association require data and code be posted along with analysis (Journal of the American Statistical Association 2022). For example, the package may not be available for the current version of the language or dependencies of the package may fail to install. For example, troubleshooting failed installations of dependencies can often lead down a chain of fixing cryptic installation errors which is difficult even for an experienced user.Journal of Data Science, Statistics, and Visualisation 3 In addition to the challenges of taking analysis from one computer and running it on another, a second major challenge is difficulty understanding or interacting with code. Containerization is a flexible approach that allows one to encapsulate any format or organization of analysis according to their preferences and assessment of the best way to organize and share the analysis. While virtualization has been around for decades, containerization is the latest incarnation of the technology and comes with several key advantages over itsJournal of Data Science, Statistics, and Visualisation 5 predecessors. Containers only virtualize the high-level components of the operating system (e.g., code, configuration files, software and data) and seamlessly re-use the stable low-level processing components of the host operating system (Turnbull 2014). Indeed, starting up a container doesn\u2019t actually start up a second instance of an operating system; it largely just changes all references for re- sources, system libraries, files, and data, to refer to a particular isolated section of the computer. Furthermore, since starting a container largely just changes the references to resources in the environment, containers are user-friendly, start up nearly instantaneously, and run code at speeds nearly identical to the host computer (Felter et al. 2015). We will present an archetypal example of containerizing and sharing an analysis from three different perspectives: (1) the high- level view of sharing containerized analyses, (2) the end-user experience of interacting with a third-party containerized analysis, and (3) the first-party task of containerizing an analysis for dissemination. All of the data, code, dependencies, configurations and software are precisely set up as in the original environment, and thus set up to reproduce the analysis exactly. The goal of containerization is to ensure that if the code worked when containerized, it will work when the image is run by a third party. First, the container is downloaded and started with a single 1An \u201cimage\u201d refers to the actual file that may be uploaded, downloaded or shared, while a \u201ccon- tainer\u201d refers to an ephemeral instance running on the computer.6 Containerization for Reproducible Analysis imageimagecloudcontainer (Copy of Computing Environment) Original Computing Environment imagecontainerization(1) upload(2) download(3)run container(4)Original Analysis Third Party Figure 2: Typical sharing of containerized analysis. However, in addition to merely allowing inspection of the data or scripts, the container also comes with an installation ofRso that the user can actually run the code and analysis through the interactive notebook interface. It is important to keep in mind that while the end-user accesses the container and its contents through the web browser on the host computer, the data, code, software installations and back-end to the interface all actually reside in the container. The web browser merely provides a window into the running container through which one may use the tools installed in the container and interact with the code and data it contains. This is the power of sharing containerizing analyses \u2013 it allowsJournal of Data Science, Statistics, and Visualisation 7 (A) T erminalHost Computer Desktop (B) web browser code \ufb01les-p: ports for browser Figure 3: Example of interacting with a containerized analysis. The container has the necessary data and code files and an installation of Rto run the analysis through this web interface. While the end-user may interact naturally with the analysis through a web-browser on the host computer, all of the code, files, and software reside in the container\u2019s pre-configured environment. In five lines the configuration specifies a base image with RandJupyteralready installed, installs a desired Radd-on package, copies over data and analysis code, and starts the Jupyter lab interface. On top of this base image one needs only to install the necessary software packages or language add-ons and copy over the data and code. Once the configuration file has been written, the image needs to be built, after which, (B) Building(A)base image with R and jupyter desired name Figure 4: (A) Example configuration file for building an image using Docker. First argument to COPYis location on host, second argument is desired location in container, the flag \u2013chownsets the ownership of the file to the container\u2019s user. While all of the containerization tools we discuss in this section can help provide a10 Containerization for Reproducible Analysis Table 1: Comparison of Docker, Singularity, and Podman for containerization of repro- ducible analyses. Otherwise, the added effort of interacting with analyses through a container has the potential to hinder the accessibility of the anal- ysis and code. Consequently, notebooks not only encourage good coding practices, but also facilitate a rich discussion of the code, its relation to the output, and the bearing of this outputJournal of Data Science, Statistics, and Visualisation 11 (A) RStudio Server (B) Jupyter Lab (C) Zeppelin  (1)  text   (2)  code  (3)  plots(1)  text   (2)  code   (3)  plots(1)  text   (2)  code   (3)  plots Figure 5: Interactive notebook environments run through the web browser using (A) RStudio Server , (B) Jupyter Lab , and (C) Zeppelin. While different notebook formats and software tools exist, all notebooks share the feature of organizing analysis as a sequence of chunks of (1) text or (2) code and its associated (3) output. Indeed, embedding output directly alongside the code allows one to document the entire analysis pipeline including expository plots such as diagnostic and exploratory plots that may inform small decisions made in the course of analysis. Nonetheless, documentation of these types of micro-decisions is important for properly documenting an analysis pipeline and is necessary for transparent and reproducible research (National Academies of Sciences and Medicine 2019). This can be useful in a research context where both code and output evolve over time and it is easy to mismatch ver- sions of results/figures to the correct versions of the underlying analysis. Another advantage of using notebooks for explaining and showcasing analysis is that they give users the option to run the code and explore it interactively. For example, one can pick a segment of the analysis they wish to explore, edit the chunk of code, run it, and observe the subsequent change in output. This can be used to provide a natural way to play with code in order to build up an understanding of how the code works and test the robustness of the analysis to alterations. However, if we containerize notebook software in addition to the code, data, and other dependencies, then we can bring the full power of popular coding environments as an interactive interface to our containerized analysis. This combination is the best of both worlds as it brings the native feel of doing analysis on one\u2019s own computer to completely self-contained and reproducible analyses. Furthermore, RStudiohas extensive support forreticulate , which allows analysis using both RandPythonat the same time in a shared computing environment.Journal of Data Science, Statistics, and Visualisation 13 An important distinction is the format of the notebook file and how it interacts with third-party software. This is useful for showcasing results because, unlike a traditional code scripts, the notebook has output embedded and one need not re-run the code to view the results. In particular, one cannot easily track changes to these notebooks in a human-readable format using version control software like Git since small changes to output can prompt a cascading change to hundreds of lines of the dense encoding. An advantage of such an approach is that the input code and commentary are saved in a human-readable format which is more versatile for edit- ing by general software and can be meaningfully tracked by version control schemes. Despite these format differences, from the viewpoint of interacting and exploring analyses all three of RStudio,JupyterandZeppelin have broadly similar behavior, and allow users to edit and run code chunks one at time, viewing output in-line in the editor. A common challenge when using notebooks is that chunks need to be run sequentially and so to explore chunks later in the analysis one needs to run earlier time-intensive14 Containerization for Reproducible Analysis code. Once running, the container is accessible through the host computer\u2019s web browser where a start-page offers several options to interact with the analysis including browsing the files (e.g., to view the HTML rendering) or opening the notebooks in a graphical interface like JupyterorRStudio. While a small amount of time would need to be devoted to teaching students some simple mechanics of containerization, in our estimation this is not more complicated than other coding tasks required in many courses and would provide an opportunity for a discussion with students about research reproducibility, replicability as well as good coding practices. It also provides an opportunity to re-run the analysis in a hands-off manner to ensure that the notebooks and the entire code pipeline actually correctly produce the results when run sequentially."
    }
  ]
}