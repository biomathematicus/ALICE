{
  "CONFIG": {
    "general_instructions": "This project is an AI assistant for a given lesson. The output of each request must be response that guides an student in understanding a topic.  \\n",
    "description": "The Data and AI Intensive Research with Rigor and Reproducibility (DAIR\u00b3) program is funded by Award 5R25GM151182 of the National Institute of General Medical Sciences, one of the 27 institutes of the National Institutes of Health of the United States. The principal investigators are Jing Liu (University of Michigan) and Juan B. Guti\u00e9rrez (University of Texas at San Antonio). \\nThe rigor of scientific research and the reproducibility of research results are essential for the validity of research findings and the trustworthiness of science. However, research rigor and reproducibility remains a significant challenge across scientific fields, especially for research with complex data types from heterogeneous sources, and long data manipulation pipelines. This is especially critical as data science and artificial intelligence (AI) methods emerge at lightning speed and researchers scramble to seize the opportunities that the new methods bring.  \\n\\nWhile researchers recognize the importance of rigor and reproducibility, they often lack the resources and the technical know-how to achieve this consistently in practice. With funding from the National Institutes of Health, a multi-university team offers a nationwide program to equip faculty and technical staff in biomedical sciences with the skills needed to improve the rigor and reproducibility of their research, and help them transfer such skills to their trainees. \\n\\nTrainees will then be guided over a one-year period to incorporate the newly acquired mindset, skills and tools into their research; and develop training for their own institutions.  \\n\\nThe DAIR3 team and instructors include faculty and staff research leaders from the University of Michigan, the College of William and Mary, Jackson State University, and University of Texas San Antonio. This highly diverse team will model the culture of diversity that we promote, and will support trainees who are demographically, professionally and scientifically diverse, and are from a diverse range of institutions, including those with limited resources.",
    "harmonizer_code": "gpt-4o",
    "harmonizer_temperature": 0.1,
    "harmonizer_name": "Harmonizer"
  },
  "MODELS": [
    {
      "model_code": "gpt-4o",
      "model_name": "OpenAI GPT 4o",
      "temperature": 0.15,
      "agent_name": "ALICE AI Agent"
    }
  ],
  "knowledgeBase": [
    {
      "file": "C:\\temp\\Units\\UNIT_0 - Pre-Reading Materials (Kelly Only)\\Code Book for PDSA Dataset.pdf",
      "summary": "4 PDSA3  Char  1 3: Kind of work you do   5 PDSA4  Num  8 4: Last regular job for pay   6 PDSA5  Char  1 5: Are you currently working more than 1 job? 14 PDSA14  Num  8 14: Where were you in life 10 years ago (1 -10)  15 PDSA15  Num  8 15: Number best describes  where would  you like  to be  next year  (1-10)  16 PDSA16  Num  8 16: Number best describes where you expect to be next year (1 -10)  17 PDSA17  Char  1 17: Disappointed if you never reach 15   18 PDSA18A  Num  8 18a: Highest degree/years of school you completed? 21 PDSA20  Char  1 20: Ever had/has spouse/partner   22 PDSA21  Char  1 21: Spouse/partner work for pay   23 PDSA23A  Num  8 23a: Highest degree/years of school spouse/partner completed? 28 PDSA27A  Char  1 27a: Receive income from investments in past year? 35 PDSA27H  Char  1 27h: Receive Supplemental security income in past year   36 PDSA27I  Char  1 27i: Income from gambling in past year? 37 PDSA28B  Char  1 28b: You/family received +$35K   38 PDSA28C  Char  1 28c: You/family received +$50K   39 PDSA28D  Char  1 28d: You/family $75K   40 PDSA28E  Char  1 28e: You/family $100K   41 PDSA28F  Char  1 28f: You/family $10K   42 PDSA28G  Char  1 28g: You/family $25K   43 PDSA28A  Char  1 28a: What was total combined family income in past year?"
    },
    {
      "file": "C:\\temp\\Units\\UNIT_0 - Pre-Reading Materials (Kelly Only)\\Hu article on SDOH in JHS.pdf",
      "summary": "Annals of Behavioral Medicine, 2022, 56, 1300\u20131311 https://doi.org/10.1093/abm/kaac026 Advance access publication 5 October 2022 Regular Article Association Between Social Determinants of Health and  Glycemic Control Among African American People with  T ype 2 diabetes: The Jackson Heart Study Jie\u00a0Hu, PhD, RN, FAAN1, \u2219 David M.\u00a0Kline, PhD2,3, \u2219 Alai\u00a0T an, PhD1, \u2219 Songzhu\u00a0Zhao, MS3 \u2219   Guy\u00a0Brock, PhD3, \u2219 Lorraine C.\u00a0Mion\u00a0PhD, RN, FAAN1, \u2219 Jimmy T . Efird, PhD4,5, \u2219  Danxin\u00a0Wang,\u00a0PhD6, \u2219 Mario\u00a0Sims, PhD7, \u2219 Bei\u00a0Wu, PhD8, \u2219 Morgana\u00a0Mongraw-Chaffin, PhD9, \u2219  Joshua J.\u00a0Joseph, MD, MPH, FAHA10, 1College of Nursing, The Ohio State University, Columbus, OH, USA 2Department of Biostatistics and Data Science, Division of Public Health Sciences, Wake Forest School of Medicine, Winston-Salem, NC, USA 3Center for Biostatistics, College of Medicine Department of Biomedical Informatics, The Ohio State University, Columbus, OH, USA 4Department of Radiation Oncology, School of Medicine, Case Western Reserve University, Cleveland, OH, USA 5Boston VA Cooperative Studies Program Coordinating Center, Boston, MS, USA 6Department of Pharmacotherapy and Translational Research, College of Pharmacy, University of Florida, Gainesville, FL, USA 7University of Mississippi Medical Center, Jackson, MS, USA 8Rory Meyers College of Nursing, New York University, New York, NY, USA 9School of Medicine, Wake Forest University, Winston-Salem, NC, USA 10College of Medicine, The Ohio State University, Columbus, OH, USA  Jie Hu hu.1348@osu.edu Abstract  Background Social determinants of health have a significant impact on health outcomes. Purpose This study examined associations of socioeconomic position (income, education, and occupation), environmental (physical activity  facilities, neighborhood social cohesion, neighborhood problem, and violence), behavioral (physical activity, nutrition, and smoking), and psy - chological factors (depressive symptoms, stress, and discrimination) with glycemic control (hemoglobin A1c [A1c]) using the World Health  Organization Social Determinants of Health framework in African American adults with type 2 diabetes. Methods A secondary data analysis was conducted using a longitudinal cohort of 1,240 African American adults with type 2 diabetes who  participated in the community-based Jackson Heart Study. Results Our study presents the complex interplay of socioeconomic determinants of health and glycemic control over time. Key words: African Americans \u00b7 Glycemic control \u00b7 Jackson Heart Study \u00b7 Social determinants of health \u00b7 Type 2 diabetes Introduction African American (AA) populations are disproportionately  affected by a higher prevalence of type 2 diabetes mellitus  than White populations (12.7% vs. 11.0%) [1]. Social determinants of health (i.e., nonmedical factors)  have a significant impact on diabetes self-management and  contribute to health disparities in diabetes outcomes among AAs with diabetes [3]. SDH are \u201cconditions in the environ- ments in which people are born, live, learn, work, play, wor - ship, and age that affect a wide range of health, functioning,  and quality-of-life outcomes and risks\u201d [4]; these factors can  be more important than healthcare utilization or life style  changes in influencing health outcomes. The World Health  Organization (WHO) Social Determinants of Health frame- work posits a complex relationship among the various social  determinants of health that affect health equity and outcomes  in positive and negative ways [3]. SEP components are a crucial factor  components becoming widely recognized as important deter - minants of diabetes risk factors and outcomes [3,7], such as  glycemic control [8,9], and diabetes-related complications [3]. Studies  seeking to understand and intervene on socioeconomic  position have struggled with the complexity of multiple  contributors to diabetes and outcomes (e.g., biological, psy- chological) and comparison between studies is complicated  by the varying conceptual frameworks and definitions used  for socioeconomic position [3]. The American Diabetes Association (ADA) scientific review  on socioeconomic position and diabetes concluded that the  complexity and interaction of multiple factors influencing  diabetes control has been understudied, but that priority  should be on interventions that target the root causes of dia- betes inequality and embed social determinants of health in  implementation and dissemination [3]. The panel recom- mended several research strategies to examine the pathways  of socioeconomic position and impact among different popu- lations with diabetes. In line with these strategies and to ad- dress this knowledge gap, we conducted a secondary data  analysis using the Jackson Heart Study (JHS) in a sample of  AA adults with type 2 diabetes. Our goal was to test pathways  by which socioeconomic position influence glycemic control  (hemoglobin A1c [A1c]) among AA adults with type 2 dia- betes guided by the WHO Social Determinants of Health con- ceptual framework. Examining the impact of socioeconomic  position on diabetes outcomes in AAs with type 2 diabetes  and identifying potential factors in the pathway between  socioeconomic position and glycemic control will help lead to  the development of effective diabetes self-management inter - ventions that include addressing socioeconomic position to  improve diabetes outcomes and promote health equity among  AA people with type 2 diabetes. Our study was guided by the WHO Social Determinants  of Health conceptual framework including SEP (income, edu- cation, and occupation), material circumstances (living and  working conditions, food availability), behaviors (physical  activity, healthy eating and smoking), and psychosocial com- ponents (depressive symptoms, stress, and discrimination) as  pathways to health and well-being (Appendix Fig. The aims of this study were to examine the associations be- tween SEP and glycemic control and assess the extent to  which environmental, behavioral, and psychological factors  mediate the effects of SEP in predicting glycemic control over  time among AA adults with type 2 diabetes. Methods Data Source A secondary data analysis of a longitudinal cohort of AAs  with type 2 diabetes was used to examine the aims and test  the hypotheses. The Jackson Heart Study (JHS) is the largest  community-based, longitudinal, and prospective study that examined associations of genetic, environmental, psycho- social risk factors associated with causes of cardiovascular  disease (CVD) in AA adults. The JHS cohort includes 5,306  men and women between the ages of 21-94 at enrollment  residing in the tri-county metro area of Jackson, Mississippi  [15,16]. The JHS was reviewed and approved by the University  of Mississippi Medical Center Institutional Review Board  (IRB). Outcomes and Measures Variables The quality of data collection procedures and instruments in  the JHS is well demonstrated [19]. Although the WHO framework indicates that gender is a  component of SEP, we opted instead to include this variable  as a covariate in our model, given that gender may reflect dif- ferential access to socioeconomic resources [21]. Physical activity facilities were the number of indoor and out- door facilities within a 3 mile radius of the person\u2019s home. The Neighborhood Problems scale (six items) meas- ures neighborhood noise, heavy traffic and speeding cars, lack  of access to adequate food/or shopping and parks, trash, and  litter. Physical activity for this study was self-reported  as the type, frequency, and duration of the sports/physical ex- ercises conducted weekly. The five nutrition  components assessed in the questionnaire were based on the  2000-kcal diet according to the AHA dietary goals [27]. The  2000-kcal diet included (1) \u2265 4.5 cups of daily fruits and  vegetables; (2) >3.5-ounce servings of fish, twice a week; (3)  \u22653 one-ounce daily servings of whole grains; (4) <1500\u00a0mg/d  of sodium, and (5) \u2264450 kcal per week of sugar beverages. Ideal diet was defined as diet consisting of four to five com- ponents; intermediate diet consisting of two to three compo- nents; and poor diet consisting of 0 to 1 components [26]. Stress was measured with the Global Perceived Stress  Scale (GPSS) [28], an eight-item scale measuring the severity  of chronic stress over the past 12 months in employment,  relationships, the neighborhood, caring for others, legal  problems, medical problems, experiences of racism and dis- crimination, and meeting basic needs. The Weekly  Stress Inventory (87 items) measures recurrent irritants during  the past week in the major life domains (e.g., work tasks, fi- nances, and relationships). The Everyday Discrimination Scale (nine items) [32] meas- ures perceptions of everyday discrimination of being treated  with less respect and less courtesy, and among other factors  (e.g., \u201cYou are treated with less courtesy than other people\u201d;  \u201cPeople act as if they are afraid of you\u201d). Covariates factored into data analyses included baseline  data on age, sex, body mass index (BMI), comorbidities (myo- cardial infarction [MI], history of stroke, history of coronary  heart disease, hypertension, chronic kidney disease, and left  ventricular hypertrophy), and diabetes medication (none, oral  only, insulin only, both oral and insulin). We fit a Bayesian LSEM model [34, 35] to estimate struc- tural latent factors and latent growth parameters for the  longitudinal model of log A1c. For each latent factor (SEP,  environment, behavior, and psychological), we specified  a distributionally appropriate generalized linear model  including fixed effects of the covariates and the corres- ponding latent factor and loading. The growth model for log A1c in- cluded fixed effects of the covariates on the intercept and  slope terms as well as random intercepts and slopes for each  subject. The structural part of the model assumes that the  SEP factor is associated with the environmental, behavioral,  and psychological factors. It also assumes that all four fac- tors are associated with the slope and intercept terms in the  growth model. Coefficients in logistic and  multinomial logistic regression models were assigned normal  distributions with mean 0 and variance 100 and loadings are  assigned normal distributions with mean 0 and variance 1. The coefficients for the effect of SEP on each of the other  factors were assigned normal distributions with mean 0 and  variance 1. The algorithm was run for 200,000 iterations,  discarding the first 100,000 as burn-in and thinning the re- maining samples by 10. Missing Data We assume missing data in the indicators of the latent factors  are missing at random conditional on all other covariates and  variables in the model. Since the analysis is fully Bayesian,  missing values are seamlessly imputed within the MCMC al- gorithm and all variability is fully propagated through the  rest of the analysis. Results Characteristics of the Participants The sample of participants consisted of 1,242 adults with  type 2 diabetes at baseline. As shown in Table 1, the mean age  of the participants was 59.8 years old (SD = 10.9) and 66% of  participants were female. Table 2 presents the baseline data on mean A1c and longi- tudinal association between time and glycemic control (A1c)  by different levels of each factor in the unadjusted model,  and an adjusted model for age, gender, BMI, types of dia- betes medication and comorbidities. In the adjusted model  of the longitudinal analysis, there was a decreasing trend of  A1c over time among participants with less than high school  education, higher levels of perceived neighborhood problems,  and healthier nutritional intake. There was a negative trend  of A1c over time at lower levels of perceived stress and a lon- gitudinal increasing positive trend of A1c among participants  with higher perceived stress. Multivariate Model T esting We fit the latent growth model based on the theoretically as- sumed structure of the latent factors and structural equation  model as described above. Based on the posterior predictive  check, there was no indication of model lack-of-fit for A1c,  with a posterior p-value of 0.23. Higher values  of the SEP factor were associated with higher levels of in- come, education, and occupation. Higher values of the envir - onmental factors were associated with higher cohesion scores and lower scores for the facilities, problems, and violence  scales. Higher values of the psychological factor were associ- ated with higher levels of depression, perceived stress, weekly  stress, and discrimination. Higher values of the behavior fac- tors were associated with less physical activity, worse nutri- tion, and smoking (e.g., lower values indicate better health  behaviors). For the structural part of the model, we assessed the as- sociations between the latent factors. That is, we described  the effect of SEP on the environmental, psychological, and  behavioral factors. At baseline, we estimated a positive as- sociation between SEP and the environmental factor with  a posterior mean coefficient of 0.34 (95% credible interval  (CI): 0.27, 0.42). We estimated negative associations be- tween SEP and the psychological factor with a posterior  mean coefficient of \u22120.46 (95% CI: \u22120.59, \u22120.34) and the  behavioral factor with a posterior mean coefficient of \u22122.54 T able 1. Demographic and Clinical Characteristics of Participants with  Type 2 Diabetes at Baseline (N = 1,242)  Characteristic  n (%)* or Mean (\u00b1SD)  Age in years (n = 1,242) 59.8 (\u00b110.9) Sex (n = 1,242)   Male 421 (34%)   Female 821 (66%) Education (n = 1,236)   <high school education 337 (27%)   \u2265High school 899 (73%) Occupation (n = 1,240)   Management/Professional 382 (31%)   Other 858 (69%) Income (n = 1,029)   Low 205 (20%)   Lower middle 295 (29%)   Upper middle 288 (28%)    Affluent 241 (23%) Body Mass Index (kg/m) (n = 1,240) 34.1 (\u00b17.1) Comorbidities**   MI (n = 1,242) 124 (10%)   Stroke (n = 1,242) 100 (8%)   CHD (n = 1,242) 174 (14%)   HTN (n = 1,242) 997 (80%)   CKD (n = 1,240) 95 (7%)   LVH (n = 699) 97 (8%) Medication Blood pressure (n = 1,233) 975 (79%) Diabetes (n = 1,232) 837 (68%) Types of diabetes medication (n = 1,152)   No diabetes medication 395 (34%)   Oral antihyperglycemic agents (OAA) only 440 (38%)   Insulin only 210 (18%) Both OAA and insulin 107 (9%) * Percentages based on completed items. Longitudinal Association between Time and Glycemic Control (A1c) by Different Levels of Each Variable (N = 1,242) Parameter Level Mean baseline A1c SD Beta coefficient* 95% CI p-value for interaction Adjusted Beta coefficient* (95% CI) p-value for interaction  Education <HS 7.53 1.76 \u22120.0535 (\u22120.0864, -0.0205) 0.0031**\u22120.0491 (\u22120.0838, \u22120.0145) 0.0082** >HS+7.55 1.79 0.0037 (\u22120.0148, 0.0222) 0.0042 (\u22120.0147, 0.0232) Occupation  Management/ Professional 7.61 1.81 \u22120.0038 (\u22120.0320, 0.0244) 0.6565 0.0002 (\u22120.0286, 0.0290) 0.4903  Other 7.40 1.71 \u22120.0116 (\u22120.0314, 0.0082) \u22120.0122 (\u22120.0326, 0.0082) Family income <$25,000 7.54 1.83 \u22120.0229 (\u22120.0495, 0.0038) 0.1346 \u22120.0258 (\u22120.0534, 0.0019) 0.1258 $25,000\u2013$49,999 7.62 1.85 \u22120.0238 (\u22120.0563, 0.0087) \u22120.0201 (\u22120.0533, 0.0130) $50,000\u2013$74,999 7.45 1.71 0.0117 (\u22120.0333, 0.0568) 0.0216 (\u22120.0248, 0.0679) 75,000+7.41 1.60 0.0344 (\u22120.0149, 0.0838) 0.0297 (\u22120.0211, 0.0804) Family income category Poor 7.64 2.00 \u22120.0144 (\u22120.0580, 0.0292) 0.5546 \u22120.0194 (\u22120.0638, 0.0250) 0.4320 Lower-middle 7.41 1.72 \u22120.0191 (\u22120.0525, 0.0142) \u22120.0216 (\u22120.0564, 0.0132) Upper-middle 7.68 1.87 \u22120.0198 (\u22120.0524, 0.0127) \u22120.0168 (\u22120.0500, 0.0163) Affluent 7.41 1.61 0.0109 (\u22120.0235, 0.0453) 0.0073 (\u22120.0272, 0.0417) Neighborhood facilities++Q1 7.49 1.80 \u22120.0121 (\u22120.0447, 0.0205) 0.3071 \u22120.0084 (\u22120.0419, 0.0251) 0.3051 Q2 7.62 1.80 0.0028 (\u22120.0298, 0.0354) 0.0061 (\u22120.0273, 0.0396) Q3 7.63 1.83 \u22120.0329 (\u22120.0647, \u22120.0012) \u22120.0342 (\u22120.0671, \u22120.0013) Q4 7.43 1.71 0.0066 (\u22120.0257, 0.0390) 0.0041 (\u22120.0291, 0.0374) Neighborhood cohesion++Q1 7.63 1.98 \u22120.0252 (\u22120.0582, 0.0079) 0.4675 \u22120.0288 (\u22120.0628, 0.0053) 0.3093 Q2 7.47 1.62 0.0108 (\u22120.0211, 0.0428) 0.0149 (\u22120.0182, 0.0480) Q3 7.50 1.67 \u22120.0107 (\u22120.0432, 0.0219) \u22120.0047 (\u22120.0380, 0.0287) Q4 7.57 1.86 \u22120.0150 (\u22120.0467, 0.0168) \u22120.0171 (\u22120.0495, 0.0154) Neighborhood problems+Q1 7.53 1.90 0.0253 (\u22120.0062, 0.0568) 0.0116**0.0256 (\u22120.0066, 0.0578) 0.0134** Q2 7.51 1.74 \u22120.0290 (\u22120.0609, 0.0029) \u22120.0228 (\u22120.0553, 0.0097) Q3 7.50 1.61 0.0054 (\u22120.0270, 0.0379) 0.0067 (\u22120.0269, 0.0404) Q4 7.64 1.88 \u22120.0437 (\u22120.0766, \u22120.0108) \u22120.0471 (\u22120.0812, \u22120.0130) Neighborhood violence+Q1 7.59 1.94 0.0179 (\u22120.0135, 0.0494) 0.1561 0.0227 (\u22120.0094, 0.0549) 0.1294 Q2 7.47 1.64 \u22120.0339 (\u22120.0669, \u22120.0010) \u22120.0314 (\u22120.0650, 0.0021) Q3 7.60 1.70 \u22120.0170 (\u22120.0486, 0.0147) \u22120.0147 (\u22120.0472, 0.0178) Q4 7.50 1.84 \u22120.0095 (\u22120.0426, 0.0237) 0.1561 \u22120.0144 (\u22120.0491, 0.0202) Physical activity Poor 7.54 1.75 \u22120.0151 (\u22120.0366, 0.0064) 0.7118 \u22120.0159 (\u22120.0381, 0.0063) 0.5368 Intermediate 7.57 1.88 \u22120.0001 (\u22120.0300, 0.0298) 0.0055 (\u22120.0251, 0.0362) Ideal 7.49 1.72 \u22120.0050 (\u22120.0485, 0.0385) \u22120.0056 (\u22120.0500, 0.0387) Nutrition Poor 7.56 1.82 0.0070 (\u22120.0179, 0.0319) 0.1559 0.0156 (\u22120.0099, 0.0411) 0.0325** Intermediate 7.50 1.72 \u22120.0250 (\u22120.0468, \u22120.0033) \u22120.0290 (\u22120.0512, \u22120.0068) Ideal 7.33 1.47 \u22120.0323 (\u22120.1493, 0.0847) \u22120.0346 (\u22120.1504, 0.0813) Current smoker No 7.55 1.78 \u22120.0142 (\u22120.0313, 0.0029) 0.1362 \u22120.0122 (\u22120.0299, 0.0054) 0.3019 Yes 7.53 1.81 0.0270 (\u22120.0244, 0.0783) 0.0167 (\u22120.0353, 0.0687)Downloaded from https://academic.oup.com/abm/article/56/12/1300/6748867 by Sharon Kardia user on 30 April 2025 ann. Method: Linear mixed models with unstructured and structured covariance on the changes in HbA1c over time by different levels of each factor. Finally, we examined associations of the latent factors with  the log A1c conditional on the covariates included in the  model (Table 3). Higher SEP levels (0.35, 95% CI: 0.22,  0.50) were associated with higher log A1c at baseline, in part  explained by the relatively affluent nature of the sample or  perhaps residual effects not accounted for in our analyses. We estimated that the direct effect of SEP on slope of log A1c  over time (from Exams 1 to 3) (Fig. The in- direct effect of SEP on longitudinal A1c through behavioral factors was 0.03 (95% CI: 0.02, 0.04) with posterior prob- ability and the effect greater than 0 of 1. We did not observe  strong evidence for associations with the environmental or  psychological factors at baseline, with the slope of log A1c  over time nor as mediators of the relationship between SEP  and log A1c over time. Discussion Our study presents the complex interplay of socioeconomic  determinants of health and glycemic control over time. Associations of the Latent Factors with the Log A1c Conditional on the Covariates at Baseline (Intercept) and Over Time (Slope) Outcome Factor Estimate 95% CI P (estimate > 0)  Environmental factors SEP 0.344 (0.2677, 0.4236) 1 Behavioral factors SEP \u22122.537 (\u22123.4025, \u22121.7429) 0 Psychological factors SEP \u22120.4583 (\u22120.5867, \u22120.3389) 0 A1c Intercept SEP 0.3541 (0.2238, 0.5024) 1 Environmental factors \u22120.0082 (\u22120.0207, 0.0043) 0.0963 Behavioral factors 0.1433 (0.122, 0.1614) 1 Psychological factors \u22120.01 (\u22120.0278, 0.0074) 0.1392 A1c Slope SEP \u22120.0295 (\u22120.0447, \u22120.017) 0 Environmental factors 0.0013 (\u22120.0012, 0.0037) 0.8484 Behavioral factors \u22120.012 (\u22120.015, \u22120.0091) 0 Psychological factors0.002 (\u22120.001, 0.0051) 0.9002 *Note: Higher values of SEP indicate higher levels of SEP; higher values in environmental factors indicate better environmental factors, higher values  of behavioral factors indicate poor health behaviors; higher values of psychological factors indicate worse psychological health (higher stress and more  depressive symptoms). A model of the association between Social Determinants of Health and glycemic control (slope A1c over time) adjusting for the covariates among  African American adults with Type 2 diabetes using SEM. *Higher values of SEP indicate higher levels of SEP; higher values in environmental factors  indicate better environmental factors, higher values of behavioral factors indicate poor health behaviors; higher values of psychological factors indicate  worse psychological health (higher stress and more depressive symptoms). *The direct effect of SEP on slope of log A1c over time (from Exams 1 to  3) was negative (\u22120.03, 95% CI: \u22120.04, \u22120.02); however, poor behavior factors were negatively associated slope of log A1c over time (\u22120.01, 95% CI:  \u22120.02, \u22120.01). The indirect effect of SEP on longitudinal A1c through behavioral factors was 0.03 (95% CI: 0.004, 0.02) with posterior probability and the  effect was greater than 0 of 1 . There was no strong evidence for associations with the environmental or psychological factors at baseline, with the slope  of log A1c over time nor as mediators of the relationship between SEP and log A1c over time.Downloaded from https://academic.oup.com/abm/article/56/12/1300/6748867 by Sharon Kardia user on 30 April 2025 ann. (2022) 56:1300\u20131311 1307 Effect of Socioeconomic Position on Glycemic  Control Although higher SEP was associated with higher A1c at base- line, A1c decreased for higher values of the SEP longitudinally  in our study. This is consistent with other studies, where SEP  consistently predicts disease progression in diabetes and is the  key factor in determinants of diabetes health outcomes [3]. Previous studies showed that lower education, income, and  occupation were associated with increased risk of type 2 dia- betes and worse diabetes control [36,37]. Although impacts  of SEP on diabetes outcomes have been reported, successful  interventions targeting SEP for decreasing health disparities  and achieving health equity for those with diabetes are ur - gently needed [3]. While we con- sidered the effect of medication in our model, we are unable  to rule out residual confounding that participants with lower  SEP may have worse glycemic control requiring higher doses  of glucose lowering medication [38]. Environmental, Behavioral and Psychological  Factors as Mediators in the Relationship  Between SEP and Glycemic Control We evaluated the role of environmental factors, behavioral  factors, and psychosocial factors as mediators in the associ- ation of socioeconomic position (income education and oc- cupation) with longitudinal glycemic control. Environmental  and psychological factors failed to mediate the association  between SEP and glycemic control. In other words, lower socioeconomic  position was associated with increasing A1c levels in both the  overall and direct longitudinal analysis, but that the indirect  pathway through health behaviors (physical activity, nutrition,  and smoking) was in the opposite direction. Our finding that health be- haviors mediate the relationship between SEP and A1c changes  similarly supports our hypothesis; however, the finding that  poor health behaviors are associated with decreasing A1c over  time was unexpected. As we suspect the baseline association  of higher SEP and poor behaviors with lower A1c is influ- enced substantially by confounding by indication, the longi- tudinal association between health behaviors and A1c could  be biased in the same way. Treatment decisions are highly inter - connected with access to care, which is also impacted by SEP,  further highlighting the complexity of investigations into so- cial determinants of health. As noted before, those with low  SEP who also had lower levels of health behaviors likely had  more rapid progression of diabetes and received initiation and  increase in treatment regimens during follow up that subse- quently lowered their glucose levels. We also note an earlier study which observed that lifestyle  mediated the effect of health and SEP, while the effect of SEP on psychological health was not particularly noteworthy  [39]. On the other hand, significant downstream effects in the  positive direction for both physical and psychologic health  was observed for their lifestyle variable. While people with  good health tend to achieve better SEP, this does not neces- sarily imply that SEP conveys better health but rather is the  consequence of healthful lifestyle choices at the individual  level (e.g., social causation being the root cause of health in- equalities). Lifestyle would than appear to be a key mediating  factor linking SEP with one\u2019s health, but the range of possible  lifestyle choices are ultimately influences by SEP and social  determinants that may change over time. In effect, SEP on  health measures such as A1c levels, in theory, could signifi- cantly increase the latter if safeguards are not in place to cur - tail negative lifestyle behaviors associated with poor health. One reason why environmental factors possibly failed to  explain the association between SEP and glycemic control in  our study might be related to the demographic characteristics  of the sample from the JHS; most of participants had upper  middle or affluent income and higher than high school level  of education. Our findings are consistent with a pre- vious study examining neighborhood environment, including  walkability and neighborhood violence found that neighbor - hood environment factors were not associated with A1c but  with LDL cholesterol and exercise [10]. As found by others, higher SEP was associated with  more favorable environmental factors and residents living  in environments with greater resources that supported  physical activity and healthy food had lower incidence of  type 2 diabetes [40,41]. Walker and colleagues  found that socioeconomic and psychological components of  socioeconomic position were associated with glycemic control  in adults with type 2 diabetes [42]. A recent  study examining contribution of education to behavioral and  psychological antecedence of health in a national sample of  AA adults demonstrated that education was significantly as- sociated with health behaviors that partially explained health  disparities [43]. Although psychological factors failed to mediate the asso- ciation between SEP and glycemic control, higher levels of  perceived stress were significantly associated with increasing  A1c over time. Interestingly, a recent study by Walker and colleagues  [37] demonstrated similar findings in their study among older  individuals from the Health and Retirement Study. Neighborhood characteristics, spatial  variation, and dimensions of racial segregation (e.g., evenness,  exposure, clustering, concentration, and centralization) are  important considerations when evaluating glycemic control  in the context of social determinants of health [44]. Recent  developments in the use of spatial models for geographically  patterned health outcomes, such as the spatial index of ra- cial isolation, are informative tools for characterizing diabetes  risk in priority populations [45]. Limitations Our study has the following enumerated limitations: First,  our study was a secondary analysis of an existing cohort, so  we had to rely on prior selected instruments, the measures  and timing of data collection points. Second, care in inferring clinical  significance is warranted given some of the small direct and  indirect effects; however, small effects can have large popu- lation level impacts if the prevalence of the relevant expos- ures are large [46]. Third, the study was conducted in one  region of the southeastern US and the SEP of participants  in the JHS is marginally higher than the general AA popu- lations, potentially limiting generalizability. Although beyond the scope of the  current paper, in the future we plan to explore other analytic  approaches to incorporate time-varying covariates, multi- level (hierarchical) effects, and to assess alternative covari- ance forms for modeling random effects and residuals of our  structural equation models [48]. The impact of  low SEP and low health behaviors on increases to treatment  that we could not fully measure may explain our unexpected  findings. Finally, in a classical interpretation, a mediating vari- able should be measured after the main predictor variable and  ideally capture change in the mediating variable from baseline  to follow-up owing to the predictor variable [49]. Both the  exposure variable and the mediating variables in our analysis  were measured concomitantly in Exam 1. We attempted to  better understand this constraint by examining the consist- ency and specificity of mediation effects across different sub- groups and our outcome of interest. The interrelationship of the factors considered in our ana- lysis are complex and multidimensional in nature; and thus,  they may not depict all the potential interacting and mediating effects of variables of interest. As indicated in the literature,  how a patient perceives the disease diagnosis and progression  of diabetes is an important determinant, mediated by the ef- fectiveness of stress alleviation, avoidance coping, healthful  diet, resilience to depression, and the proper control of their  comorbidities and blood sugar levels with medication [50]. Diabetes self-management, access to care, and ability to  follow healthy lifestyle guidelines may all be impacted in a  variety of ways by SEP [48]. Nevertheless, our study provides insight into the as- sociation of SEP with glycemic control in a large sample of  AA adults over an 8-year period. However, the data were in part inconsistent  with the hypothesis that environmental and psychological  factors mediate the relationships between SEP and glycemic  control. Given the substantial influence of SEP, scien- tists and researchers need to consider interventions other than  those aimed solely at the individual\u2019s control. Acknowledgements The authors wish to thank the staff and participants of  the Jackson Heart Study. The Jackson Heart Study is sup- ported and conducted in collaboration with Jackson State  University (HHSN268201800013I), Tougaloo College  (HHSN268201800014I), the Mississippi State Department  of Health (HHSN268201800015I) and the University  of Mississippi Medical Center (HHSN268201800010I,  HHSN268201800011I and HHSN268201800012I) con- tracts from the National Heart, Lung, and Blood Institute  (NHLBI) and the National Institute on Minority Health  and Health Disparities (NIMHD). and the National Institute of Diabetes and Digestive  and Kidney Diseases (K23DK117041, J.J.J.) The views expressed in this manuscript  are those of the authors and do not necessarily represent the  views of the National Heart, Lung, and Blood Institute; the  National Institutes of Health; the U.S. Department of Health  and Human Services; or the United States Department of  Veteran Affairs. All procedures, including the informed consent pro- cess, were conducted in accordance with the ethical standards  of the responsible committee on human experimentation (in- stitutional and national) and with the Helsinki Declaration of  1975, as revised in 2000. Formal  analyses on model testing, interpretation of data, visualiza- tion, methodology, writing on analysis and results sections  and writing-review and editing. A.T. design of the study,  methodology, writing-original draft and writing-review and  editing. Ethical Approval All procedures performed in this study  were in accordance with the ethical standards of our institu- tional research ethics committee and with the 1964 Helsinki  declaration and its later amendments or comparable ethical  standards. The Estimated Loadings for the Indicators of Each Latent Factor Conditional on the Covariates Factor Variable Level Estimate 95% CI P(estimate>0)  Socioeconomic position Income Poor Lower-middle 0.43 (0.08, 0.8) 0.99 Upper-middle 2.04 (1.56, 2.54) 1 Affluent 3.36 (2.74, 4.08) 1 Education Less than HS HS+ 2.37 (1.85, 2.93) 1 Occupation Other Management/Professional 2.55 (2.04, 3.21) 1 Environmental factorsFacilities \u22120.09 (\u22120.11, \u22120.07) 0 Cohesion 0.1 (0.1, 0.11) 1 Problems \u22120.16 (\u22120.17, \u22120.15) 0 Violence \u22120.11 (\u22120.11, \u22120.1) 0 Psychological factorsDepression 6.18 (5.37, 7.04) 1 Perceived Stress 1.51 (1.19, 1.83) 1 Weekly Stress 39.02 (32.25, 46.13) 1 Daily Discrimination  1 2 0.21 (0, 0.43) 0.98 3 0.58 (0.33,0.85) 1 4 0.74 (0.4, 1.1) 1 5+ 1.24 (0.8,1.7) 1 Behavioral factorsPhysical activity Poor Health Intermediate Health \u22120.13 (\u22120.22, \u22120.07) 0 Ideal Health \u22120.31 (\u22120.46, \u22120.21) 0 Nutrition Poor Health Intermediate Health \u22120.14 (\u22120.22, \u22120.07) 0 Ideal Health \u22120.59 (\u22121, \u22120.32) 0 Current smoker No Yes 0.21 (0.11, 0.32) 1APPENDIXDownloaded from https://academic.oup.com/abm/article/56/12/1300/6748867 by Sharon Kardia user on 30 April 2025 1310 ann.behav. Assessing the rela- tionship between neighborhood factors and diabetes related health  outcomes and self-care behaviors. Carnethon MR, Pu J, Howard G, et al.; American Heart As- sociation Council on Epidemiology and Prevention; Council  on  Cardiovascular Disease in the Young; Council on Cardio- vascular and Stroke Nursing; Council on Clinical Cardiology;  Council on Functional Genomics and Translational Biology; and  Stroke\u00a0 Council. Associations of the Individual Latent Factor with the Log A1c Conditional on the Covariates Outcome Factor Estimate  95% CI P (estimate > 0)  A1c Intercept SEP \u22120.0126 (\u22120.0265, 0.0018) 0.0432 A1c Slope SEP 0.0011 (\u22120.0016, 0.0036) 0.7694 A1c Intercept Environment 0.011 (\u22120.0002, 0.0036) 0.9727 A1c Slope Environment \u22120.0015 (\u22120.0038, 0.0039) 0.0957 A1c Intercept Psychological \u22120.0026 (\u22120.0174, 0.0120) 0.3679 A1c Slope Psychological 0.0013 (\u22120.0015, 0.0039) 0.8143 A1c Intercept Behavior 0.0141 (\u22120.0022, 0.0310) 0.9543 A1c Slope Behavior \u22120.0006 (\u22120.0034, 0.0020) 0.3182 Appendix Fig. Hypothesized association between Social Determinants of Health and glycemic control among African American adults with type 2  diabetes.Downloaded from https://academic.oup.com/abm/article/56/12/1300/6748867 by Sharon Kardia user on 30 April 2025 ann. Taylor HA, Jr, Wilson JG, Jones DW, et al. Toward resolution of  cardiovascular health disparities in African Americans: design and  methods of the Jackson Heart Study. The impact of socioeconomic position (SEP) on women\u2019s health  over the lifetime. Neighborhood social  and physical environments and type 2 diabetes mellitus in African  Americans: The Jackson Heart Study. Stress and achievement  of cardiovascular health metrics: The American Heart Association  Life\u2019s Simple 7 in Blacks of the Jackson Heart Study. The longitu- dinal influence of social determinants of health on glycemic control  in elderly adults with diabetes. Rela- tionship between social determinants of health and processes and  outcomes in adults with type 2 diabetes: validation of a conceptual  framework. Unique contribution of education to behavioral and psychosocial  antecedents of health in a national sample of African Americans. J  Behav Med. Kaplan D. The impact of specification error on the estimation, test- ing, and improvement of structural equation models."
    },
    {
      "file": "C:\\temp\\Units\\UNIT_0 - Pre-Reading Materials (Kelly Only)\\Moresis article on Minimal Metadata.pdf",
      "summary": "Lab Animal | Volume 53 | March 2024 | 67\u201379  67 lab animalhttps://doi.org/10.1038/s41684-024-01335-0 Perspective A minimal metadata set (MNMS)  to repurpose nonclinical in vivo  data for biomedical research  Check for updates Anastasios Moresis   1,10, Leonardo Restivo2,10, Sophie Bromilow   3, Gunnar Flik4, Giorgio Rosati5,  Fabrizio Scorrano6, Michael Tsoory7, Eoin C. O\u2019Connor   8 , Stefano Gaburro   5  &  Alexandra Bannach-Brown   9  Although biomedical research is experiencing a data explosion, the accumulation of vast  quantities of data alone does not guarantee a primary objective for science: building upon existing knowledge. We conclude with a \u2018call for action\u2019 to key stakeholders in biomedical research to adopt and apply MNMS to accelerate both the advancement of knowledge and the betterment of animal welfare. Data-rich multiomics approaches and high-resolution functional  measures, such as multimodal imaging or recordings of physiology and  behavior, are routinely being employed across the entire lifespan of model  organisms in both health and disease states. On the other hand, the mere col - lection of vast amounts of data is not sufficient to ensure scientific  progress if these data cannot be interrogated and reintegrated into the  research cycle. In Europe and North America, legislation for animal experimenta- tion in biomedical research focuses heavily on implementation of the 3Rs  (see definition in Box 1), which encompasses the concepts of replace-ment, reduction and refinement 4. The objective of the 3Rs is to ensure  that animal experimentation achieves the highest level of welfare while  minimizing burden through well-designed and reviewed animal research  protocols and procedures. e-mail: eoin.oconnor@roche.com; stefano.gaburro@tecniplast.it; Alexandra.Bannach-Brown@charite.deLab Animal | Volume 53 | March 2024 | 67\u201379 68 Perspectivehttps://doi.org/10.1038/s41684-024-01335-0 In certain sectors, regulation mandates data sharing from studies  involving animals and progress is being made to ensure that in vivo data  can be repurposed with a view to generating virtual control groups (VCGs;  see definition in Box 1). For example, in the EU REACH (Registration,  Evaluation, Authorisation and Restriction of Chemicals)20, European  Union (EU) biocides21 and EU plant protection products22, there is a  legal requirement to share test and study reports from studies in animals  that are used for registration purposes (see, for example, Article 62 in  Regulation (EC) No. Regulatory submissions to the  US Food and Drug Administration (FDA) must adhere to the Standard  for Exchange of Nonclinical Data (SEND), which requires presentation  of data from nonclinical safety and toxicology studies in a consistent and  machine-readable format. To draw more awareness to concepts of validity, robustness and  reproducibility (see definition in Box 1), the 3Rs principles have since  been expanded to include the responsible use of animal research7\u20139. Meanwhile, the domain-agnostic and nonmandatory  guidelines PREPARE17 and ARRIVE18,19 (see definition in Box 1) were  also proposed as checklists for scientists when planning and reporting  in vivo experiments, respectively.Box 1 | Definitions of key terms   API: an acronym that stands for application programming interface. ARRIVE: the ARRIVE guidelines (Animal Research: Reporting of   In Vivo Experiments) were originally developed in 2010 to  improve the reporting of animal research. They consist of a checklist of information to include in publications describing in vivo experiments to enable others to scrutinize the work adequately, evaluate its methodological rigor and reproduce the methods  and results 18. Data repository: a data repository is a structure consisting of one  or more databases containing data for the purpose of analysis. Metadata: metadata are data on data (that is, information about the  data), and contain descriptive and administrative information about the dataset. The OBI helps communicate clearly about scientific investigations   by defining more than 2,500 terms for assays, devices, objectives and more.Ontology: an ontology is a system of carefully defined terminology, connected by logical relationships and designed for both humans and computers to use. Raw data: also known as primary or source data, raw data are data (for example, numbers, instrument readings, figures and so on) collected from a source that was not subjected to (1) processing, (2) \u2018cleaning\u2019 by researchers to remove, for example outliers and obvious instrument-reading errors, (3) any analysis (for example, determining central tendency aspects such as the average or median result) or (4) any other manipulation by a software program or a human researcher, analyst or technician. Note that raw data provide a great deal of flexibility in terms  of data repurposing, given that different questions can be asked from the original dataset that may not be possible after processing. Reproducibility: here we refer to reproducibility broadly, to include  both the stricter definition of reproducibility as \u2018reproducibility of analysis\u2019, referring to the re-analysis of an existing dataset, as well as \u2018reproducibility of experimental findings\u2019, which refers to the collection of new data in experiments as identical as possible to the initial experiment 40,41. When an experimental animal cohort undergoes a specific intervention, these amassed digital data, in conjunction with predefined algorithms, forecast the potential outcome for that cohort in the absence of the said intervention. Despite various regulatory frameworks, initiatives and guidelines,  data sharing and repurposing within the field of biomedical research  remains an exception rather than the rule25\u201327. Reasons for this limited  progress may include domain-specificity of approaches, technical barriers  to understanding FAIR data standards, a reluctance to share data, a lack  of awareness of potential benefits and an absence of incentives for data  sharing and repurposing. One critical element necessary for data sharing and repurposing is  to provide metadata (see definition in Box 1 ) that describe the raw or pri - mary data (see definition in Box 1 ). An ideal MNMS would build on existing  guidelines for in vivo data reporting that are established for biomedical research, while also expanding their impact and applicability by opening  the door toward effective data sharing and repurposing. In this Perspective, with this need in mind, a working group of scien - tists from academia and private industry was formed to propose a MNMS  to describe data generated from an in vivo biomedical research experiment. Additionally, we highlight opportunities, challenges and future actions  required to support the adoption of MNMS in biomedical research with a  view to ultimately enable data repurposing, the advancement of scientific  knowledge and the betterment of animal welfare. Conceptual understanding of minimal metadata  selection Before outlining a MNMS for in vivo data, it is first essential to understand  in more detail how metadata can aid the formation of a data repository  and the decision to repurpose data, thus contributing to a reduction  and replacement of animal use. Practically, only a small portion of this space is needed (that is,  necessary and sufficient) to effectively describe data from one experiment,  and researchers can freely choose metadata sets that are fit for purpose. Therefore, the lack of metadata overlap would not allow the researcher to  evaluate the potential to repurpose the data for their own needs. This practice would lead  to the aggregation of data collected either on incompatible sources   (for example, the attempt to aggregate data from distinct mouse strains), or  with discordant methodologies (for example, mice reared under different  housing conditions), which would be inappropriate for data repurposing. Under  our initial assumption, the experiments were originally logged with the  metadata strictly necessary to reproduce the family of experiments they  belong to. Adding the extra MNMS would not automatically extend the  repurposing of the raw data that lay in distant regions of the metadata  space. However, the presence of a complete MNMS would support the  decision of the researcher as to whether include the associated raw data  in a repurposing opportunity. Undoubtedly, the additional burden of log - ging the MNMS is eclipsed by the advantages that such a strategy offers  in terms of data repurposing, including the potential for replacement and  reduction of animal use, and reusage of critical data assets. As a more practical example for how metadata and database for - mation can support data repurposing, a schematic is shown of a data  repository that was implemented at the Roche Innovation Center Basel  (Switzerland) in 2022. With this simple example, one  begins to appreciate how new questions can be asked of data repositories,  while these questions were outside the scope of the original experiments  stored within those repositories (for example, how does locomotor activity  differ between different mouse strains and between sexes?). In this case, the researcher can confidently operate a choice  for repurposing the data.Lab Animal | Volume 53 | March 2024 | 67\u201379 70 Perspectivehttps://doi.org/10.1038/s41684-024-01335-0 Key principles for the deployment of a MNMS With the conceptual understanding of a MNMS in place, we next high- light key principles required for deploying a MNMS for in vivo research. With the emergence of automated machine learning and  artificial intelligence pipelines, emphasis is increasingly given to deploying  FAIR principles to render data machine-actionable (that is, computational  systems can find, access, interoperate and reuse data with no or minimal  human intervention). To enable the interoperable aspect, a structure must be imposed on the  metadata using a well-defined conceptual model to describe relationships  and constraints between the different entities (for example, an animal or a  study). In addition to  having harmonized and standardized metadata, according to the FAIR principles, each metadata term should also be adequately defined, with a  description of its use, and be given a unique identifier. Integrity metric Once a MNMS is agreed upon, checking the coverage and quality of the  items reported in this minimal set would yield a \u2018completeness score\u2019 attached to each dataset, which would serve as an integrity metric. A completeness score associated to the metadata set is critical to enable a threshold-based decision concerning the rejection or inclusion of the  associated raw data within a data repository. In addition, knowing the com - pleteness of the metadata set would support researchers in assessing the  generalizability of inferences that can be drawn from any repurposed data. Prespecification  avoids \u2018post-mortem solutions\u2019 that lead to both low-quality data report - ing and an unwarranted confidence in the rigor of the methods adopted. In these situations, the task of  recovering a comprehensive set of metadata based on a limited amount  of metadata available becomes unsurmountable and error prone. Provenance Provenance and ownership of data are important aspects for a MNMS and  are essential for curation in the context of large-scale usage of data and  metadata sets. To enable identification of provenance and ownership, each  data entry in a prospective repository must have the following operational  metadata associated to it: creator of the record (which is also treated as a  FAIR object with an assigned unique identifier), date of creation and date  of modification. Specifically, the ARRIVE \u2018Essential 10\u2019 (Table 1 ) and the  \u2018Recommended Set\u2019 (Table 2) served as the foundation for building the MNMS for biomedical research. The schematic represents a real-world example of a repository of data  from recordings of locomotor activity in mice, where all data are annotated with metadata. Data  can be further segregated (for example, by strain, age and so on), enabling the experimenter to understand the potential for repurposing these data for their  specific questions and needs.Lab Animal | Volume 53 | March 2024 | 67\u201379  71 Perspectivehttps://doi.org/10.1038/s41684-024-01335-0 and are endorsed by peer-reviewed journals, which may facilitate future  adoption of MNMS. Previous research on the ARRIVE 1.0 guidelines  operationalized them into a list of over 100 items29, which can be difficult  for authors to fully comply with; this extensive list also complicates the editorial staff \u2019s task to check for compliance. With the revision of the  guidelines in 2020, the ARRIVE 2.0 \u2018Essential 10\u2019 was put forward as the  minimum information required for reporting of animal experiments19. Thus, we prioritized alignment of MNMS with the \u2018Essential 10\u2019 , which  would also help to minimize the workload for MNMS and thus further  support its uptake. The MNMS proposed here would substantially con- tribute to making in vivo data from biomedical research FAIR compliant,  in line with the original goals of the ARRIVE guidelines. Indeed, ensur - ing data from animal experiments is actionable through the adoption of  MNMS further increases the impact of the ARRIVE guidelines. Details on experimental animals constitute the main aspect of MNMS, because insufficient or inaccurate reporting of these  attributes is considered one of the main challenges for data reproducibil- ity. This will enable not only FAIR datasets  but, along with other potentially unique identifiers entered (for example, Table 1 | The MNMS (ARRIVE 2.0 Essential 10) ARRIVE topic-essential 10 MNMS MNMS detailed Data type Existing ontology Study design Yes, but only start and end  date of the in-life phaseStart and end date of the in-life phase Date ISO8601 Sample size NA NA NA NA Inclusion/exclusion criteria NA NA NA NA Randomization NA NA NA NA Blinding NA NA NA NA Outcome measuresYes, extended, assay  specificOutcome measure (including any   descriptive statistics if applicable,   for example, average speed)Controlled vocabulary Multiple, assay specific per domain Unit of measurement per measure Controlled vocabulary UO, OBI Statistical methods NA NA NA NA Experimental animals Yes, extended, all assaysUnique animal identifier URI NA Local identifiers/other IDs String NA Species Controlled vocabulary OBI, NCIt Strain (ILAR and short name) Controlled vocabulary ILAR a, MGI Sex Controlled vocabulary NCIt, CDISC/SEND Transgenic Boolean NA Genotype information Controlled vocabulary MGI Allele information Controlled vocabulary MGI Animal vendor (site and location) information Controlled vocabularya Date of birth Date ISO8601 Developmental stage Controlled vocabulary OBO Foundry Animal weight at start of experiment and unit Number + controlled  vocabularyUO Severity grade of manipulation Numbera Experimental proceduresYes, but focused on  compound treatment;   terms for procedures  beyond require  domain-specific alignmentTest substance (common name) Controlled vocabulary ChEBI, DrON Test substance (CAS number) Number NA Numerical dose Number NA Dose unit (ideally mg/kg or mM) Controlled vocabulary UO, OBI Vehicle composition Controlled vocabulary ChEBI Route of administration Controlled vocabulary OBI Administration method Controlled vocabulary OBI Results NA NA NA NA ChEBI, Chemical Entities of Biological Interest; CDISC, Clinical Data Interchange Standards Consortium; DrON, drug ontology; NA, not applicable; NCIt, National Cancer Institute thesaurus; OBO,  Open Biological and Biomedical Ontologies; UO, units of measurement ontology. aFurther development needed to provide a formal ontology.Lab Animal | Volume 53 | March 2024 | 67\u201379 72 Perspectivehttps://doi.org/10.1038/s41684-024-01335-0 radio frequency identification (RFID) tags), also provide unambiguous  identification of the animal record in the data repository and the associ- ated datasets. The latter is especially important if an animal is included  in more than one study, to avoid false duplicates and the introduction of  artificial confounders of variability in any repurposed datasets. With respect to the basic attributes for experimental animals, having  the standardized Institute for Laboratory Animal Research (ILAR) name  in addition to any short strain names or synonyms should be mandatory. MGI is the  authoritative source in the field of mouse genomics given that its nomen - clature follows the rules and guidelines established by the International  Committee on Standardized Genetic Nomenclature for Mice. Furthermore, for multiple  transgenic/mutant alleles additional care should be taken to unambigu-ously map the correct genotype to each allele in the sequence that would  constitute the full genotype (for example, allele 1: Tg/+; allele 2: Tg/Tg).The last essential immutable information is the date of birth of  the animal. From the set of nonimmutable animal metadata, animal weight at  the start of the experiment (with controlled vocabulary for the unit) is  mandatory in the MNMS. This information may  serve as a proxy for stress levels and suffering, and may explain deviations  in the experimental data, allowing exclusion of the animal(s) from further  analysis if necessary. With regard to study-design terms, MNMS requests the experiment  start and end dates (defined as the start and end of the in-life phase). These metadata can (1) signify which animals belonged to the same study;  (2) facilitate the reuse of longitudinal data sets, where entries linked to body weight and age can change over the course of the experiment; and  (3) ensure awareness of when the data were produced, which may be an  important consideration in the case of factors such as genetic drift. Since pharmacological manipulations are common in biomedical  research, different forms of exposure to an active substance and for dif- ferent purposes (for example, as a therapeutic, or to induce a specific  condition like disease or transgene induction) are represented exten - sively in MNMS. Synonyms such as the common  drug name, pointing to the same entity, can complement a digital record  of the substance identifier and its existing ontologies. Other important factors in the admin- istration of a compound are the method of administration and the  administration route (for example, intraperitoneally, per os and so on) and the vehicle composition. To avoid extending  terminology to different diets or feeding schedules and enrichment types,  which may be more reflective of specific experimental procedures and  thus beyond the scope of MNMS, both dietary status and enrichment are  represented with one of only two states (that is, fasted or fed/not fasted,  and the presence of enrichment or not). Last, but not least, data access in MNMS extends beyond  the scope of the ARRIVE guidelines and constitutes a critical aspect for identifying dataset ownership. To this end,  any prospective user-facing system cannot be completely public but rather  based on a registration and authentication service.Box 2 | Key terms related to structured  metadata fields proposed in the MNMS   Administration method: indicates the method that is used for  exposure and excludes the route of administration. Strain: a population or type of organism that is genetically different  from others of the same species and shares a set of defined characteristics. Foreign DNA (the transgene) is defined here as DNA from another species, or recombinant DNA from the same species that has been manipulated in the laboratory before being reintroduced.Lab Animal | Volume 53 | March 2024 | 67\u201379  73 Perspectivehttps://doi.org/10.1038/s41684-024-01335-0 ARRIVE topics not included in MNMS These include study design, study sample size, experimental unit, inclusion  and exclusion criteria, randomization and blinding. More specifically, using the single-animal level for  metadata (and data reporting) negates the need for reporting the sample  size and experimental unit. Aspects of study design before experimental  manipulations, such as the definition of groups being compared, ran - domization, blinding and inclusion and exclusion criteria are clearly  important for reproducing the study itself. Benefits of adopting MNMS With the MNMS outlined, in the following section we highlight the ben- efits that may come from adopting MNMS within the biomedical research  community. The efficient description and retrieval of raw data enables their  application for scopes beyond those intended during the original collec- tion, therefore reducing the number of animals or, in some cases, replacing  animals needed for novel experiments. The opportunities and challenges for implementing  VCGs were the focus of the IMI eTRANSAFE project, which has collected  and analyzed drug safety and toxicology data from more than 60,000  rats, 1,300 dogs and 500 monkeys (see ref. Furthermore,  implementation of VCGs can address some of the ethical needs for respon - sible animal research by minimizing the number of animals required for biomedical studies. Using VCGs to standardize animal experiments  can assist regulatory agencies in monitoring and enforcing regula - tions, improving data transparency and accessibility, and reducing the number of animals utilized in research. Notably, VCGs can eliminate  confounding variables, increase data quality and precision, harmonize data collection and reporting and create a more regulated and uniform  environment for animal research. Meta-research in animal research Meta-analysis of data from existing in vivo studies (within and beyond the scope of systematic reviews) provides a powerful tool to explore the  impact of variations in experimental design and can reduce the need for  further animal use. Meta-analyses of  animal data are also used to inform the optimal design of experiments in  a number of ways, including comparing performance and evaluating the  necessity of outcome tests34, informing sample-size calculations35, refining  the duration of experiments and humane endpoints36 and optimizing the  choice of model induction technique37. The conclusions and accuracy of meta-analyses  are therefore limited by the quality of primary data available. Adoption of  MNMS as a reporting standard, for both control and experimental data,  would provide a large step forward to improve the reporting of in vivo  studies and facilitate the overall conduct of meta-analysis. The use of  MNMS together with standardized repositories for data collection would  also allow for complex meta-analyses that are currently prohibitive due to the absence of raw or primary data. A further benefit for adopting MNMS is that they can streamline the  process for generating a richer set of metadata and insights with minimal  additional effort, which can further support meta-analysis. The age-at-test can be obtained by intersecting the date-of-birth and  the date of study start (both items included in MNMS) or the date of  testing that may be embedded in the raw data file (for example, raw data timestamp). Challenges for adopting MNMS The concept of deploying a MNMS to enable the creation of structured data repositories and the opportunity to repurpose data is highly appeal- ing from both a scientific (for example, incremental knowledge and new  insights) and an ethical point of view (that is, replacing and/or reducing  animal use). However, intrinsic to the concept of repurposing is the idea  that a large user base is necessary to fully realize the opportunities for data  reuse. Adoption within the scientific community Several initiatives have emerged to increase replicability and reproduc- ibility, and to support reuse of data derived from in vivo studies18,19,39. One expla - nation is that the efforts required to follow such guidelines, and the  rewards gained from doing so, may not be of immediate interest to the  data producer in the laboratory. Historical  practices within laboratories, combined with finite resources, may reflect  the need for further change within the scientific community to recognize  the importance of data sharing and the responsibilities of scientists to report their data. To minimize the effort needed for providing MNMS and maximize  their impact, we have proposed a MNMS that aligns with ARRIVE 2.0,  as set of guidelines that are gaining increasing acceptance within the bio - medical research community and are accepted by peer-reviewed journals. By providing a mandatory and minimal set of controlled terminologies to describe the data, the MNMS would overcome challenges of \u2018free text\u2019  entries required by ARRIVE, which can result in entries of variable quality  that are not easily comparable between publications, or no entries at all. Such  tools may include the use of artificial intelligence to support identification  and reporting of metadata from publications in standardized formats, and  to highlight where certain metadata are not found and must be provided before publication. Thus, data and metadata sets that have  supported reuse will receive a higher reference rate and provide recogni-tion to the contributing scientists, research groups and their institutions. However, this size and diversity of the research space  also presents a tremendous opportunity to better leverage the knowledge  being generated and further support the 3Rs. For example, in the chemical industry, a legal requirement exists to share  data from in vivo studies in reports that are used for registration purposes  (see, for example, Article 62 (ref. Constraining the experimental space We believe that it is possible to identify a minimal set of metadata for  animal experiments such that raw data can be repurposed across distinct  families of experiments. This vast space does not allow the identification of a minimal set of  metadata that guarantees a decision is always taken on the reuse of data  from across diverse disciplines (for example, cardiology, neuroscience,  oncology and so on). From the perspective of (bio)statisticians, MNMS increases the precision and replicability of statistical analyses by eliminating confounding variables, harmonizing data collection and ensuring a more regulated research  environment. This comprehensive representation underscores MNMS\u2019s role in  advancing responsible, efficient and high-quality research practices.Lab Animal | Volume 53 | March 2024 | 67\u201379  75 Perspectivehttps://doi.org/10.1038/s41684-024-01335-0 and their housing may range over a limited number of dimensions   (for example, genotype, age, weight, light cycle and so on), metadata from  the domain of experimental manipulations cover a far more extensive  space. This issue quickly emerged in the context of the present work - group while searching for a consensus on the items to include in MNMS. To address the challenge given by the vast experimental space in  biomedical research, we suggest that an a priori decision must be made  concerning the range of experimental questions (and their associated  outcome measures) that would be of interest to address in a repurposing initiative, such as in the generation of VCGs. This, in turn, will allow researchers  to gauge the repurposing potential of the collected raw data in different  disciplines and across institutions. Indeed, it is critical that multiple par - ties (for example, domain experts, data architects, funding agencies and so on) join in a concerted action to define the boundaries and bridges of these minimal sets of metadata and to make repurposing of data across  disciplines a successful reality. Legal considerations for data sharing The objective of deploying MNMS to facilitate data sharing and repurpos - ing raises some important questions from a legal perspective. The idea of incentivizing  and recognizing contributors and users of data for repurposing would  also require that individuals are associated with a FAIR identifier such  as an ORCID ID. However, the ORCID ID is a code that can be linked  back to an identifiable person, which is likely to make the code \u2018personal  data\u2019 in some parts of the world. To  protect the value of the data, the data should be accessible only by those with genuine intent to conduct further research. Where  requests for access are made by a competitor of the contributor, protective  measures are likely to be necessary to protect such commercially sensitive  information and to ensure compliance with competition law40. Although companies are not forced to share data, it would  be prudent to ensure that all the terms of access are fair, reasonable and  nondiscriminatory (that is, those in a similar position should be able to  access the data on equivalent terms). Should data be available for others to access immedi- ately, or is it reasonable to impose a time delay to protect the commercial  interests of the contributor? Contributors are likely to want to keep  ownership of their data and only give recipients a right to use the data for  certain uses (with limited rights to share with others). However, contributors  are likely to want the requestor to accept full responsibility for confirming  that the data are suitable for the purposes for which they want to use it  and to accept all liability associated with reuse of the data. If the decision is yes, then the  guidance published by the European Chemicals Agency on data sharing41  may be a helpful reference point for how to calculate reasonable sums. Contributors will need to ensure that their  agreements with third-party contract research organizations (CROs) or research collaborators permit the sharing of the relevant data into any  data-sharing scheme and the further reuse by third parties. This issue is  particularly important where the sharing of the data is not a clear legal  requirement and where there is an intent to charge third parties for access  to the data (which may be viewed as commercial use of the results). Scenarios for MNMS implementation Despite the recognized challenges discussed above, the following sec- tion highlights opportunities to leverage MNMS to support in vivo  data repository creation and data repurposing across different in vivo  research contexts, including behavioral core facilities in academia,  CROs, pharmaceutical companies and in vivo research equipment  providers. Core facilities provide local users (typically principal investigators and  laboratories from the university in which they are hosted) with access to  a range of assays and equipment that facilitate functional in vivo research  (see ref. The unique position and resources of a core facility within  academic research may allow data sharing and repurposing in a very  straightforward and applicable manner. To support implementation of MNMS, several metadata fields (for example, strain, sex, age and so on)  may be automatically drawn from the animal facility records with support  from husbandry staff to provide information regarding rearing conditions  (for example, diurnal light cycle). Core unit staff (if in place) can provide information regarding the experimental protocol (for example, appara- tus setups if included in an extended metadata set) and the researcher  can provide additional fields (for example, treatment, dose and route). CROs typically offer a catalog of standardized in vivo assays that are accepted  within the respective biomedical research field and follow local and  international standards of regulation concerning animal experimentation. Particularly for CROs that have  implemented SEND, as required by the FDA for safety studies, a frame- work for the standardized, electronic representation of individual animal  study data is already in place. With the experience gained from data storage standardiza - tion within drug safety studies for SEND, a pull-through to in vivo studies  for discovery biomedical research is an easy step for many CROs. There  would be numerous benefits for adoption of MNMS and data repurposing  in the CRO space for discovery research. First, by making control data or  other experimental data (for example, from standard reference treatments)  available for repurposing, sponsoring clients could avoid repetition of  some experiments and choose to use VCGs instead of newly generated  control data. Second, the exchange of data from client to CRO, and vice versa, should enhance the inherent robustness of  a given assay, which would benefit both the CRO by emphasizing assay  quality and the client by making robust decisions based on the assay result. VCGs and optimized study designs will  not only result in a reduction in the number of animals and ensure more ethical research, but it will also speed up the drug development trajec- tory and reduce costs, which is often a critical factor for smaller-sized  biotechnology companies. While numerous benefits are envisioned in the CRO space for adop - tion of MNMS and data repurposing, sharing data remains a point of  attention. Restrictions on data use may be contained in the agreement  between the CRO and the sponsoring client, and sharing data may require  additional terms to be agreed (as discussed above in legal considerations for data sharing). Pharmaceutical research and development While the vast majority of experimentation in pharmaceutical research and development (Pharma R&D) is undertaken in non-animal studies,  animal research remains an important component to discover new biology  and treatment opportunities, and to predict the efficacy and safety of new  medicines before entering human trials. In this context,  FAIR data storage and the need to avoid continuous data replication of  historical data is of paramount importance to ensure research progress  and optimize insights discovery. To support implementation of MNMS into Pharma R&D, a multi- disciplinary team consisting of data scientists, scientific researchers and  laboratory animal specialists is essential for effectively implementing  a global standardized approach. Laboratory  animal specialists can provide insights into the operational challenges  of working with different systems and help ensure that the strategy is  compatible with local and global animal welfare regulations. By  implementing a comprehensive approach that addresses the challenges of working with multiple in vivo systems, researchers in Pharma R&D  can repurpose and combine data from different studies, leading to greater  insights and discoveries. Animal research software and/or hardware developers   and providers Animal research is enabled by a rich ecosystem of hardware and software,  including colony animal management software and laboratory information  management systems, tools used in the laboratory for various measure- ments (for example, heart rate monitors or imaging devices) and software  that enables biospecimen analysis and data visualization (for example, image analysis or statistical tools). By incorporating MNMS and especially harmonized vocab- ulary terms into their platforms, businesses engaged in providing experi - mental equipment and software for in vivo research can greatly contribute  to improving data sharing and repurposing. Particularly important in this  context are companies that offer animal management software, which  can provide users with access to MNMS and create and support software  interfaces to push MNMS to other systems involved in data collection and  analysis. By adopting MNMS, commercial system providers can contribute  substantially to the scientific community by making important informa- tion readily accessible for a variety of purposes, including grant applica- tions, academic publications, animal license applications and patent  filings related to animal research. In addition to enhancing the quality and supporting reproducibility of research, this advance would further Lab Animal | Volume 53 | March 2024 | 67\u201379  77 Perspectivehttps://doi.org/10.1038/s41684-024-01335-0 encourage ethical considerations and ultimately facilitate the process of  animal-data management and repurposing. The use of VCGs is a currently underexploited opportunity that could substantially reduce the number of animals used  in biomedical research when applied at scale. Taken together, we believe that the deployment of MNMS alongside  existing initiatives (ARRIVE) represents the next frontier for advancing  the ethical use of animals in research. This step could involve refining MNMS with  co-creation and community-consensus communication processes between  participants from each of the key groups (scientists, regulatory bodies and  (bio)statisticians) across academic, pharmaceutical and contract research  sectors, to identify further potential barriers and enablers to implementa - tion. A technique such as a modified Delphi allows for community input and consensus when deciding on adding and refining other terms that  are critical in specific fields, such as the importance of health status or  animal microbiomes. These efforts could focus on topics such as the following: joint develop- ment of tools to support integration with existing data infrastructure;  development of new documentation and education to support uptake; and  integration with existing initiatives in 3Rs. Examples of these initiatives include  protocol registration (for example, animalstudyregistry.org and preclini - caltrials.eu) and initiatives to improve experimental design quality and  reporting standards for animal experiments (for example, PREPARE and  ARRIVE guidelines). Obtaining insights  on what tools the research community currently uses (for example,  Research Data Alliance initiatives or domain ontologies and vocabular - ies), and how MNMS can best align and integrate with these tools, will enable the development of new strategies to tackle barriers to adoption  of MNMS and facilitate data sharing. A critical need is likely to be the harmonization of terminologies and controlled vocabularies, which could support MNMS  adoption by key stakeholders, including companies providing scientific  research equipment and software to in vivo researchers across sectors. Indeed, the private industry sector has already made  considerable progress in the advancement and implementation of con- trolled vocabularies, and close collaboration between private and public  institutions is likely to accelerate progress in this field. With feedback from the primary stakeholders, a roadmap to MNMS  dissemination can be put in place, probably moving from local to global  use in a stepwise, incremental manner and ensuring alignment between  these efforts at each stage. Dissemination activities could include hosting  workshops to showcase MNMS functionality with exemplar projects;  outlining the impact MNMS on research outcomes for each stakeholder  group; developing targeted marketing materials, such as infographics  to highlight the benefits of MNMS; producing educational materials  and documentation to support training efforts on how to effectively use MNMS; and performing pilot experiments to demonstrate the utility of  MNMS in common settings. These strategies can accelerate the uptake  and use of this tool into existing workflows and increase awareness of the  benefits of MNMS for a broad audience. However,  in the absence of tools to effectively reintegrate the vast quantities of  data generated into the research cycle, researchers face a situation of  massive resource inefficiency with minimal scientific gain. This issue is  particularly concerning in the context of research with animals where  researchers are committed to follow the 3Rs that also recognize the  need for responsible use of animals. A first and necessary step toward  effective repurposing of data from in vivo studies in biomedical research  is to ensure that raw data are effectively described with metadata. The  MNMS that we propose here aligns with existing guidelines for reporting  in vivo studies and, if adopted, would provide an important step toward advancing scientific knowledge and ensuring the continued ethical use  of animals in biomedical research. Regulation (EC) No 1907/2006 of the European Parliament and of the Council of 18 December 2006 Concerning the Registration, Evaluation, Authorisation and Restriction of Chemicals (REACH), Establishing a European Chemicals Agency, Amending Directive 1999/45/EC and Repealing Council Regulation (EEC) No 793/93 and Commission Regulation (EC) No 1488/94 as Well as Council Directive 76/769/EEC and Commission Directives 91/155/EEC, 93/67/EEC, 93/105/EC and 2000/21/EC (Text with EEA Relevance)   (Publications Office of the European Union, 2006). Regulation (EU) No 528/2012 of the European Parliament and of the Council of 22 May 2012 Concerning the Making Available on the Market and Use of Biocidal productsText with EEA Relevance. Regulation (EC) No 1107/2009 of the European Parliament and of the Council of 21 October 2009 Concerning the Placing of Plant Protection Products on the Market and Repealing Council Directives 79/117/EEC and 91/414/EEC. Communication from the Commission\u2014Guidelines on the Applicability of Article 101 of the Treaty on the Functioning   of the European Union to Horizontal Co-Operation Agreements   Text with EEA Relevance (Publications Office of the European Union, 2011). Additional information Correspondence and requests for materials should be addressed to Eoin C. O\u2019Connor, Stefano Gaburro or Alexandra Bannach-Brown.Peer review information Lab Animal thanks Sabine H\u00f6lter, Paul A.  Garner and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Open Access This article is licensed under a Creative Commons  Attribution 4.0 International License, which permits use, sharing,  adaptation, distribution and reproduction in any medium or format,  as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright  holder."
    },
    {
      "file": "C:\\temp\\Units\\UNIT_0 - Pre-Reading Materials (Kelly Only)\\Unit-3-design.pdf",
      "summary": "\u2013For those interested, here is a readable account of the work of the very in\ufb02uential philosopher of science Karl Popper. \u2013There are many other lines of work contributing to the philososphy of science including the work of Greeks such as Aristotle and Persians including Ibn Sina. \u2013Design refers to the process of developing a rationale and goals for the research, stating (falsi\ufb01able) hypotheses, and documenting how the data will be collected, analyzed, and interpreted. \u2013Data analysis refers to the execution of a data analysis plan, which in- cludes carrying out the analysis, interpreting the results, and formally assessing the uncertainty of all \ufb01ndings. For example, if we are interested in a certain type or category of human beings, all the people in this category are viewed as \"replicates\" 1of the person of interest. The set of all humans of interest is the population and the speci\ufb01c humans that we observe and (potentially) manipulate is the sample. \u2013Acontrolled study involves a comparison to a group (arm) that is untreated, or that is treated with a conventional well-understood treatment (e.g. \"standard of care\" in clinical research). \u2013A randomized controlled study (or \"trial\", RCT) is a controlled study in which the subjects are assigned to the treatment or control group by randomization. Exposures happen naturalistically and we can assess at the end of the study which exposures occurred and what outcomes followed the occurrence of these exposures. 2\u2013In a case/control study units are deliberately selected at the outset to belong to contrasting states of a factor of interest. \u2013An adaptive interventional study is one in which the treatment as- signments depend on the results of previously collected data within the same study, based either on an interim analysis or oncontinual reassessment of \ufb01ndings. For example, in the context of measuring blood pressure, there is a well-known systematic error known as \"white coat hypertension\" in which some people\u2019s blood pressure rises due to the stress of interacting with the person doing the measurement. Using blood pressure again as an example, there are substantial random errors due to the skill required to accurately detect a pulse as the blood pressure cu\ufb00 is depressurized. The variance of these random errors may depend on the skill of the person doing the measurement. A typical example is where we wish to study the yield of a chemical reaction at a speci\ufb01c temperature. We set the temperature to a desired level $T$ and use a heating device to produce the desired temperature. On warm days with little rain, people enjoy eating ice cream and these are the same conditions that might lead people to go to the pool or the beach. \u2013When designing a study, one or more of the following strategies are usually employed to mitigate the risks of confounding. They are listed 5in decreasing order of rigor: \u2217Randomization is usually the best way to limit or eliminate the risk of confounding. If the treatment is assigned at random, it is impossible for it to be causally in\ufb02uenced by any factor (regardless of whether the factor is known or unknown, measured or unmeasured). \u2013Both of the above examples can be viewed in a stylized way by considering the partial correlation coe\ufb03cient, which in the population satis\ufb01es the relationship ${\\rm Cor}(X, Y | Z) = ({\\rm Cor}(X, Y) - {\\rm Cor}(X, Z)\\cdot{\\rm Cor}(Y,Z))/\\sqrt{(1-{\\rmCor}(X,Z)\u02c62)\\cdot(1-{\\rmCor}(Y,Z)\u02c62)}$ Suppose that $X$ is $BMI$ and $Y$ is mortality risk, with marginal correlation ${\\rm Cor}(X, Y) > 0$. If the latter two correlations are strong compared to the \ufb01rst, conditioning on smoking can swing the apparent association between BMI and mortality from positive to negative. 6\u2022Formal analysis of confounding often takes place in the context of the Neyman-Rubin causal model, which posits the existence of potential out- comes. \u2013In mediation analysis, a third variable $M$ called the mediator is posited as a causal intermediary between the exposure and the out- come. As a simple example, $Z$ could be sex, and it may be that the relationship between the exposure and outcome di\ufb00ers between females and males. Confounding will generally lead to a lack of balance, but even if there is no confounding, there can be a lack of balance, especially when the sample size is small. \u2022The most basic type of randomization is simple randomization , where each unit is independently assigned to a treatment group, either with uniform probabilities (equal probability of assignment to each arm) or with probabilities that are pre-determined to achieve desired relative group sizes (e.g. assigning to treatment with twice the probability of assigning to control). In 1:1 strati\ufb01ed randomization we would assign exactly 30 of the people with hypertension to the treatment arm, and exactly 15 of the people without hypertension to the treatment arm. \u2013Minimization is a class of methods that addresses the practical issue that in many research studies, subjects are recruited over time, and we do not have a listing of the subjects and their measured confounders at the outset of the study. \u2022A concern in studies where units are recruited sequentially is bias on the part of the research team in any decisions that could in\ufb02uence recruitment of subjects. If subjects do not actually receive the treatment to which they are randomized, one of the following three approaches can be adopted: \u2013In an intention to treat analysis, subjects are analyzed as belonging to 8thetreatmentgrouptowhichtheywereassigned, regardlessofwhether they received and fully complied with the treatment. It is the only option that retains the bene\ufb01ts of randomization in terms of eliminating confounding from both measured and unmeasured variables. \u2013In anas-treated analysis, subjects are analyzed as belonging to the treatment that they received and complied with, even if this treatment is di\ufb00erent from the one to which they were assigned. That is, we posit that our data are a random sample from a probability distribution $P_\\theta$, where $\\theta$ is a parameter that captures aspects of the scienti\ufb01c research question. Formally, this in- volves devising a function $\\hat{\\theta}(D)$, where $D$ is the observed data, such that $\\hat{\\theta}(D)$ is likely to be close to the true parameter value $\\theta$. \u2022It is common to refer to $P_\\theta$ as the population, $D$ as the sample, and $\\hat{\\theta}$ as the parameter estimate . \u2022There are many ways to obtain parameter estimates, two of the most common are the method of moments, and maximum likelihood analysis. It is the standard deviation of the sampling 9distribution of the random variable $\\hat{\\theta}$, which is induced by the underlying distribution of the data, $P(D)$. \u2022If the sampling distribution of $\\hat{\\theta}$ is approximately Gaussian, then the standard error is all one needs to fully characterize the estimation errors of an unbiased estimator. Statistical power \u2022Statistical power is a measure of how likely a study is to yield a positive \ufb01nding, if a positive \ufb01nding is the true state of the system being studied. Usually this refers to the power of a hypothesis test, referring to the probability of rejecting the null hypothesis when the null hypothesis is false. For example, we could have high power to reject the null hypothesis in a situation where, due to bias, rejecting the null does not re\ufb02ect the claimed level of evidence. \u2022Power analysis is strongly linked to the notion of e\ufb00ect size, which quanti\ufb01es the strength of an e\ufb00ect relative to all sources of variation. Typically this involves one of the following: \u2013Find the sample size necessary to have a de\ufb01ned power (usually 80%) to detect a given e\ufb00ect size. \u2022Using statistical theory, it is possible to develop a detailed understanding of the factors that in\ufb02uence the power in any given setting. \u2022As a basic example, the standard error of the mean is $\\sigma/\\sqrt{n}$, where $\\sigma\u02c62$ is the variance and $n$ is the sample size. Based on this expression, we know that the only factors that determine the standard error for mean estimation are the sample size and the variance, and that they combine as a speci\ufb01c rational function. While greater sample sizes in any group corresponds to improved power, the sample size of the smallest group generally has the greatest impact on power. That is, if the treatment group has sample size $n_t$ and residual variance $\\sigma_t\u02c62$, the power for many tests is related to the ratio $n_t/\\sigma_t\u02c62$. Doubling the sample size or reducing the residual variance by a factor of two have equal impacts on the power. To address this, each plot can be divided in half, and the two halves are randomly assigned, one to the treatment and one to the control. Surveys \u2022Asample survey is a research tool in which the goal is to quantify the state of a population, with a primary focus on achieving low bias for a de\ufb01ned target population. In the case of surveys that involve interviewing human subjects, a questionnaire ,survey form , orsurvey instrument refers to the actual assessment items that are used for each unit. the goal of a survey is not usually to assess the e\ufb00ect of an exposure or intervention, and a survey does not usually have \"arms\" corresponding to di\ufb00erent treatments or exposures. \u2022A special type of survey is a census, which aims to measure the entire population, rather than measuring only a sample of a population. Sampling \u2022In surveys, as well as in some other contexts, it is important to carefully sample units from a population in such a way that unbiased and precise results can be obtained from the sample. 12\u2022The most basic type of sampling is to obtain a simple random sample (SRS), which is a sample of size $k$ from a population of size $n$ in which any subset of size $k$ is equally likely to be selected. For example, if we want to sample the employees of a company or the students enrolled in a school, a sampling frame would generally be available and it would be practical to obtain a simple random sample from it. \u2022In some cases sampling probabilities are adjusted to increase statistical power for comparisons of interest, even if this produces a sample that is not representative of the overall population. For example, suppose that one of the research goals is to compare outcomes between Black and White subpopulations, in a setting where the White population is, say, 3 times greater than the Black population. One possibility would be to sample the same number of Black and White subjects, which would maximize statistical power for comparisons between these two races (if the variances within the two races are equal -- if the variances are unequal we would want $n_b/\\sigma_b\u02c62 = n_w/\\sigma_w\u02c62$, where $n_b$ and $n_w$ are the Black and White sample sizes and $\\sigma_b\u02c62$ and $\\sigma_w\u02c62$ are the response variances for Black and White subjects). If we, say, over-sample Black compared to White subjects at a rate of 3 to 1, then when estimating population parameters (not 13race-speci\ufb01c parameters), we would weight the White respondents 3 times more than the Black respondents to compensate for the biased sampling. Left truncation refers to a form of selection bias in which an observation cannot be made unless the event of interest occurs after a truncation time. For example, if we are considering death due to a speci\ufb01c disease, using records from a particular health care system, we can never observe people who died of the disease without enrolling in the health care system. \u2013Right censoring refers to a form of partially observed data in which we know that an event of interest did not occur before a speci\ufb01c time, but we do not know when the event occurred (if ever). Right censoring is very common in health studies where some units in the sample have not yet had an event at a particular time, e.g. the time when the data were obtained for analysis. \u2013Left censoring is less common than right censoring, but it can occur, for example, if a measurement falls below a \"limit of detection\" (e.g. the concentration of a chemical in a blood sample is below the limit of detection for the assay used to assess the chemical). \u2013Multiple imputation is a framework in which multiple datasets are created, each one of which has the missing data imputed randomly from a distribution that correctly re\ufb02ects its conditional mean and variance, i.e. from $P(X_{\\rm miss} | X_{\\rm obs})$. The analysis is them conducted independently on each imputed data set, and the point estimates and sampling variances for each imputed data set are pooled to a single point estimate and single sampling variance using acombining rule . \u2013Full information maximum likelihood (FIML) is an approach in which a likelihood for the \"complete data\" is marginalized to a likelihood for the observed data. \u2022Since ${\\rm Var}(d) = {\\rm Var}(a) + {\\rm Var}(b) - 2{\\rm Cov}(a, b)$, if $a$ and $b$ are positively correlated, the variance of $d$ is less than the sum of the variance of $a$ and the variance of $b$. The value of $\\beta_0$ captures the systematic change from baseline to follow- up, and the term $\\beta_1 a$ adjusts for baseline severity. \u2022Let $a\u02c6c_i$ and $b\u02c6c_i$ denote the baseline and follow-up measurements for subjects in the control arm and let $a\u02c6t_i$ and $b\u02c6t_i$ denote the baseline and follow-up measurements for subjects in the treated arm. \u2022The most basic way to conduct a strati\ufb01ed analysis is to compute the parameter of interest separately within each stratum, and then to pool these stratum-level estimates into an overall estimate. A com- mon situation is that the treatment e\ufb00ect estimate is attenuated (shifted toward the null value) when comparing the estimate obtained using strati- \ufb01cation to the naive estimate. Regression adjustment \u2022Regression analysis is a broad class of techniques that can be used to relate the value of an outcome to the values of one or more explanatory variables . The most basic form of regression analysis is linear regression , usingordinary least squares to \ufb01t the models to data (i.e. to estimate the model parameters). \u2022A basic example is the linear model $E[Y|X,Z] = \\beta_0 + \\beta_1 X + \\beta_2 Z$, where $Y$ is the outcome, $X$ is the treatment or exposure, and $Z$ is a potential confounder. Under certain rather strong assumptions, an estimate of the coe\ufb03cient $\\beta_1$ can be used to assess the relationship between the exposure $X$ and the outcome $Y$, while controlling for the confounder $Z$. One may argue that this is a natural experiment since the factors that led to earlier introduction of cable services may be primarily driven by logistical factors so that the people who gained access to cable TV in, say, 1984 may not be systematically di\ufb00erent from those who gained access to cable TV in 1985. However, we can consider only the subset of people with, say, baseline SBP equal to 129 compared to the subset with baseline SBP equal to 130. Given the inherent measurement error in a single SBP reading, we might argue that for people with baseline SBP between 129 and 130, the treatment assignment is e\ufb00ectively random. Thegoalof1:1matching is to identify a set of pairs $1 \\le j_{1i}, j_{2i} \\le n$ such that subject $j_{1i}$ is exposed, i.e. $x_{j_{1i}}=1$, subject $j_{2i}$ is not exposed, i.e. $x_{j_{2i}}=0$, and subjects $j_{1i}$ and $j_{2i}$ are similar in terms of the covariates, i.e. $z_{j_{1i}}\\approx z_{j_{2i}}$. \u2022As a consequence of the matching, the matched treated subjects should be approximately balanced with respect to the matched untreated subjects. \u2022Matching requires the treated and untreated subjects to have a common supportin the domain of the covariates. If the treated units are much older than the untreated units, and only 5 of the treated units are younger than the oldest untreated unit, then there is very little common support and matching is unlikely to be e\ufb00ective. \u2022In a matching analysis, some of the controls may be unused, and some of the controls may be used in multiple matched sets. \u2013Strati\ufb01ed analyses can be conducted in which we partition the sample into, say, 5 strata based on the propensity scores, estimate treatment e\ufb00ects within each stratum, and then pool the results to yield an overall estimate. \u2013Propensity scores can be used as weights in inverse probability weight- ing Synthetic controls \u2022A synthetic control is a calculated value for each treated unit that estimates the \"counterfactual\" value that the unit would have had had it not been treated. \u2022Let $x\u02c6\\star$ denote the $p$-dimensional vector of covariates associated with a treated unit and let $X$ denote the $m\\times p$ matrix of covariate valuesforallunitsassignedtothecontrolarm(inmanycasesonlyuntreated units with covariates \"su\ufb03ciently similar\" to $x\u02c6\\star$ are included). \u2022Once the weight vector $w$ is constructed, we estimate the counterfactual control value $y\u02c6*$ using $w\u02c6\\prime y$, where $y$ is the $m\\times 1$ vector containing the response values for the controls in $X$. However when using linear regression, the weights $w\u02c6*$ can be negative, and therefore the synthetic control may not lie within the domain of the data (i.e. it may be an extrapolation)."
    },
    {
      "file": "C:\\temp\\Units\\UNIT_0 - Pre-Reading Materials (Kelly Only)\\Unit-4-Lecture01a_basics.pdf",
      "summary": "Introduction \u00a0to\u00a0Biomedical \u00a0Data\u00a0 Science Lecture\u00a01:\u00a0Basic\u00a0IdeasAbout\u00a0me \u2022Electrical\u00a0Engineering \u00a0Degree\u00a0from\u00a0India\u00a0 \u2022MSE,\u00a0Electrical\u00a0&\u00a0Computer \u00a0Engg.,\u00a0UT\u00a0Austin \u2022Michigan \u00bbAM\u00a0Statistics \u00bbIIDP\u00a0PhD\u00a0(EE:\u00a0Systems\u00a0and\u00a0 Bioinformatics), \u00a0Focus:\u00a0Transcriptional \u00a0 Regulatory \u00a0Elements \uf0a7 Carnegie\u00a0Mellon:\u00a0Lane\u00a0Fellow\u00a0of\u00a0Computational \u00a0Biology\u00a0 (Focus:\u00a0image\u00a0analysis) \uf0a7 MD\u00a0Anderson: \u00a0Cancer\u00a0Imaging\u00a0and\u00a0Bioinformatics \uf0a7 Michigan\u00a0Medicine: \u00a0Bioinformatics \u00a0and\u00a0Radiation \u00a0 Oncology Research \u00a0Interests: Image Informatics Genomic Data Integration (i) H&E images for Pathology (ii) Immunofluorescence  images for High Content  Screening  (iii) Radiology data(i) Modeling Transcriptional  Regulation (ii) Infectious Disease Informatics  Build predictive statistical models that: (i) Correlate image-derived (radiology/pathology) features with genomics or  clinical outcomes (response etc. )(ii) Combine imaging and genomics data to predict outcomecredits \u2022Ralph\u00a0Gottardo, \u00a0FHCRC\u00a0(http://www.rglab.org/ ) \u2022Boris\u00a0Steipe,\u00a0University \u00a0of\u00a0Toronto \u2022Daniele\u00a0Merico,\u00a0University \u00a0of\u00a0Toronto \u2022Canadian \u00a0Bioinformatics \u00a0Workshops \u00a0 (www.bioinformatics.ca )Statistics\u00a0references \u00a0on\u00a0the\u00a0Web \u2022http://www.unt.edu/rss/class/Jon/ \u2022http://davidmlane.com/hyperstat/index.html \u2022http://faculty.chass.ncsu.edu/garson/PA765/statnote.htm \u00a0 \u2022http://www.khanacademy.org/math/statistics \u00a0Notes\u00a0on\u00a0Teaching\u00a0style \u2022Meant\u00a0to\u00a0be\u00a0conversational, \u00a0please\u00a0ask\u00a0questions \u2022Please\u00a0feel\u00a0free\u00a0to\u00a0raise\u00a0topics\u00a0you\u00a0may\u00a0want\u00a0to\u00a0see;\u00a0we\u00a0 can\u00a0cover\u00a0those\u00a0later. \u2022Office\u00a0hours:\u00a0available\u00a0by\u00a0request\u00a0(please\u00a0email:\u00a0ukarvind@umich.edu)Machine Learning: Wikipedia,  \u201cMachine learning is a field of  computer science that uses statistical  techniques to give computer systems the ability to \" learn \" (i.e., progressively  improve performance on a specific task)  with data, without being explicitly  programmed.\u201d What is ML? : learning from data,  about data (i.e. learning  representations)AI: Wikipedia,  \u201cArtificial intelligence  (AI, also machine intelligence , MI) is intelligence  demonstrated by machines, in contrast to the natural intelligence  (NI)  displayed by humans and other animals. Colloquially, the term \"artificial intelligence\" is applied  when a machine mimics \"cognitive\" functions that humans associate with  other human minds, such as \"learning\" and \"problem solving\".\u201d The traditional problems (or goals) of AI research include reasoning ,  knowledge representation , planning , learning , natural language processing ,  perception  and the ability to move and manipulate objects.And AI? ML is an  algorithmic field that blends ideas from statistics, computer science and many other disciplines (see below) to design algorithms that process data, make predictions and help make decisions.\u201d \u201cThe phrase \u201cData Science\u201d began to be used to refer  to this phenomenon, reflecting the need of ML algorithms experts to partner with database and distributed-systems experts to build scalable, robust ML systems, and reflecting the larger social and environmental scope of the resulting systems.\u201d \u201cThis confluence of ideas and technology trends has  been rebranded as \u201cAI\u201d over the past few years. : classification \u2022If the targets of some subset of dr ugs is known, can the targets of  others be determined based on their similarity to known drugs? Power \u00a0calculations 7. Number\u00a0of\u00a0petals\u00a0on\u00a0the\u00a0daisies\u00a0in\u00a0the\u00a0gardens\u00a0of\u00a0Houston \u2022Daisies\u00a0(in\u00a0the\u00a0gardens\u00a0of\u00a0Houston): \u00a0population \u00a0units \u2022Number\u00a0of\u00a0petals:\u00a0discrete\u00a0variable\u00a0(numerical) Car\u00a0brands\u00a0in\u00a0Houston \u2022Cars\u00a0(in\u00a0Houston): \u00a0population \u00a0units \u2022Car\u00a0brand:\u00a0discrete\u00a0variable\u00a0(categorical)Discrete\u00a0and\u00a0Continuous \u00a0Variables \u2022Continuous Any\u00a0real\u00a0value\u00a0in\u00a0a\u00a0range\u00a0(continuous) e.g. Blood\u00a0pressure\u00a0of\u00a0overweight \u00a0Americans \u2022Overweight \u00a0Americans: \u00a0population \u00a0units \u2022Blood\u00a0pressure: \u00a0continuous \u00a0variable\u00a0(numerical) Liters\u00a0of\u00a0wastewater \u00a0produced \u00a0by\u00a0each\u00a0Houston\u00a0inhabitant \u00a0in\u00a02010 \u2022Houston\u00a0inhabitants: \u00a0population \u00a0units \u2022Liters\u00a0of\u00a0wastewater \u00a0(2010):\u00a0continuous \u00a0variable\u00a0(numerical)Discrete\u00a0Analytical \u00a0Distributions \u2022Cast\u00a0a\u00a0(fair)\u00a06\u2010face\u00a0dice,\u00a0observe\u00a0the\u00a0number\u00a0on\u00a0the\u00a0top\u00a0face \u2013Population \u00a0units:\u00a0all\u00a0the\u00a0possible\u00a0dice\u2010casting\u00a0events\u00a0for\u00a0that\u00a0(fair)\u00a06\u2010face\u00a0dice \u2013Discrete\u00a0variable:\u00a0number\u00a0on\u00a0the\u00a0top\u00a0face\u00a0of\u00a0the\u00a0dice \u2022Probability \u00a0distribution P\u00a0(1)\u00a0=\u00a01/6 P \u00a0(4)\u00a0=\u00a01/6 P\u00a0(2)\u00a0=\u00a01/6 P \u00a0(3)\u00a0=\u00a01/6 P\u00a0(5)\u00a0=\u00a01/6 P \u00a0(6)\u00a0=\u00a01/6 \u2022This\u00a0is\u00a0a\u00a0uniform\u00a0discrete\u00a0distribution \u2013It\u2019s\u00a0mathematically \u00a0simple,\u00a0 but\u00a0not\u00a0all\u00a0discrete\u00a0analytical \u00a0distributions \u00a0are\u00a0as\u00a0simplePx\uf028\uf029\uf03d16,x\uf0ce{ 1 ,2 ,3 ,4 ,5 ,6 }Continuous \u00a0Analytical \u00a0Distributions \u2022Since\u00a0the\u00a0variable\u00a0can\u00a0have\u00a0any\u00a0possible\u00a0value\u00a0in\u00a0a\u00a0range,\u00a0the\u00a0 probability \u00a0of\u00a0a\u00a0single\u00a0value\u00a0is\u00a0not\u00a0finite \u2022We\u00a0need\u00a0calculus\u00a0to\u00a0correctly\u00a0handle\u00a0the\u00a0probability \u00a0distribution, \u00a0 which\u00a0is\u00a0called\u00a0density\u00a0functionHypothesis \u00a0Testing Hypothesis \u00a0Testing Empirically  observed frequency (count the number of values observed)Analytical  probability density (area under the curve) Pxa\uf03cx\uf03cxb\uf028\uf029 \uf03df(x)dx xaxb \uf0f2Normal\u00a0DistributionNormal\u00a0Distribution \u2022The\u00a0Normal\u00a0is\u00a0a\u00a0very\u00a0important \u00a0distribution \u2013Often\u00a0found\u00a0when\u00a0measuring \u00a0a\u00a0physical\u00a0property\u00a0multiple\u00a0times (variability \u00a0due\u00a0to\u00a0random\u00a0instrumental \u00a0errors) \u2013Often\u00a0found\u00a0for\u00a0anthropometric \u00a0indexes\u00a0in\u00a0human\u00a0populations \u2013The\u00a0sampling\u00a0mean follows\u00a0the\u00a0normal\u00a0distribution fx\uf028\uf029\uf03d1 \uf0732\uf070e\uf02d(x\uf02d\uf06d)2 2\uf0732Parameters: -\u03bc = Mean (x) -\u03c3 = StDev (x)Hypothesis \u00a0Testing \u03c3 affects the  width of the curve \u03bc affects the  position of the center of the curve\u03c3 = 1 \u03c3 = 2 \u03c3 = 3The normal is  symmetric and centered on \u03bc  \u03bcNormal\u00a0Distribution \u00a0in\u00a0R:\u00a0 Find\u00a0P\u00a0given\u00a0x P (x < b) = \u2026  P (x < a) = \u2026 pnorm (x = \u2026, mean = \u2026, sd = \u2026)a bNormal\u00a0Distribution \u00a0in\u00a0R:\u00a0 Find\u00a0P\u00a0given\u00a0x P (a < x < b) = P (x < b) - P (x < a) pnorm (x = xb.n, \u2026) \u2013 pnorm (x = xa.n, \u2026)Assignment: verify that for any mean and  standard deviation, the probability of x falling within \u03bc \u00b1 2 \u03c3 is about 95% abNormal\u00a0Distribution \u00a0in\u00a0R:\u00a0 Find\u00a0x\u00a0given\u00a0P P (x < \u2026) = P1 qnorm (p = \u2026, mean = \u2026, sd = \u2026)Area = P1Normal\u00a0Distribution: The\u00a0Effect\u00a0of\u00a0Symmetry P (x < \u03bc + k) = 1 - Pk P (x < \u03bc - k) = Pk \u03bc\u03bc\u03bc -k \u03bc + k Assignment: test this property using qnorm ()Pk Pk1-PkThe\u00a0Standard \u00a0Normal and\u00a0the\u00a0z\u2010score \u2022The\u00a0Standard\u00a0Normal\u00a0distribution \u00a0has\u00a0\u03bc\u00a0=\u00a00,\u00a0\u03c3\u00a0=\u00a01 \u2022The\u00a0z\u2010score\u00a0is\u00a0used\u00a0to\u00a0transform \u00a0normally\u00a0distributed \u00a0 variables\u00a0into\u00a0a\u00a0standard\u00a0normal \u2013Z\u00a0follows\u00a0the\u00a0standard\u00a0normal\u00a0\u00a0 \u2013The\u00a0z\u2010score\u00a0is\u00a0often\u00a0interpreted \u00a0as\u00a0the\u00a0number\u00a0of\u00a0standard\u00a0 deviations \u00a0from\u00a0the\u00a0mean \u2013The\u00a0reverse\u00a0formula\u00a0is\u00a0also\u00a0importantz\uf03dx\uf02d\uf06d \uf073 x\uf03d\uf06d\uf02bz\uf0d7\uf073Normal\u00a0Distribution: Find\u00a0x\u00a0given\u00a0P\u00a0using\u00a0the\u00a0Standard P (x < x1) = P1 \u03bc x1P1 0 z1P1P (z < z1) = P1 x1 = \u03bc + z1*\u03c3Hypothesis \u00a0Testing\u2022Test\u00a0this\u00a0relation:\u00a0x1\u00a0=\u00a0\u03bc\u00a0+\u00a0z1*\u03c3 using\u00a0the\u00a0R\u00a0commands \u00a0you\u00a0have\u00a0learnt # Normal x1.n <- qnorm (p = \u2026, mean = \u2026, sd = \u2026) # Standard Normal z1.n <- qnorm (p = \u2026)\uf0e0QQplot \u2022The\u00a0QQ\u2010plot\u00a0of\u00a0an\u00a0observed \u00a0 distribution \u00a0versus\u00a0the\u00a0 normal\u00a0 can\u00a0be\u00a0used\u00a0to\u00a0evaluate\u00a0how\u00a0 close\u00a0the\u00a0observed \u00a0 distribution \u00a0is\u00a0to\u00a0the\u00a0normal \u2013The\u00a0point\u00a0should\u00a0be\u00a0lying\u00a0on\u00a0 a\u00a0line https://en.wikipedia.org/wiki/ Normal_probability_plotHypothesis \u00a0Testing # quasi-normal x.nv <- c (-1.8, -1, -0.75,  -0.5, -0.3, 0, 0.3, 0.45, 0.8, 1.1, 1.6) qqnorm (x.nv, pch = 19)qqline (x.nv)# not normal x.nv <- 2 ^ (1: 12)qqnorm (x.nv, pch = 19)qqline (x.nv)Population \u00a0and\u00a0Sample \u2022Population set\u00a0of\u00a0entities\u00a0(individuals, \u00a0objects,\u00a0events) mean: \u03bc stdev: \u03c3 \u2022Sample subset\u00a0of\u00a0a\u00a0population mean: m stdev: sCorrection \u00a0for\u00a0Sample\u00a0Stdev \u2022Population \u2022Sample\uf073\uf03d1 NM(x)\uf02dxi\uf028\uf0292 Ni\uf03d1 \uf0e5 s\uf03d1 N\uf02d1M(x)\uf02dxi\uf028\uf0292 Ni\uf03d1 \uf0e5 The R function sd ()  uses by default the second definitionHypothesis \u00a0Testing As N increases, the sample means of the statistic become  closer to the population value of the statisticSampling \u00a0Mean\u00a0Distribution \u2022If\u00a0the\u00a0distribution \u00a0of\u00a0x\u00a0is\u00a0normal, the\u00a0distribution \u00a0of\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0is\u00a0normal\u00a0as\u00a0well \u2022Even\u00a0if\u00a0the\u00a0distribution \u00a0of\u00a0x\u00a0is\u00a0not\u00a0normal, when\u00a0sample\u00a0size\u00a0N\u00a0is\u00a0sufficiently \u00a0large the\u00a0distribution \u00a0of\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0is\u00a0normal (Central\u00a0Limit\u00a0Theorem) \u2022For\u00a0practical\u00a0purposes, \u00a0sufficiently \u00a0large\u00a0corresponds \u00a0to\u00a0N\u00a0>\u00a030x x Confidence \u00a0Interval\u00a0of\u00a0the\u00a0Mean \u2022Solution 1. However in R we can use any normal distribution to compute x given the probabilityx  x Hypothesis \u00a0Testing \u2022Given\u00a0a\u00a0sample\u00a0(with\u00a0known\u00a0mean\u00a0and\u00a0stdev),\u00a0we\u00a0want\u00a0 to\u00a0test\u00a0whether\u00a0it\u00a0may\u00a0belong\u00a0or\u00a0not\u00a0to\u00a0a\u00a0population \u00a0 (with\u00a0known\u00a0mean) \u2022We\u00a0can\u00a0use\u00a0the\u00a0framework \u00a0we\u00a0have\u00a0derived\u00a0for\u00a0 confidence \u00a0interval,\u00a0and\u00a0reshape\u00a0it\u00a0as\u00a0a\u00a0test \u2013Application \u00a0example: Monsanto \u00a0claims\u00a0that\u00a0a\u00a0new\u00a0crop\u00a0variety\u00a0has\u00a0a\u00a0higher\u00a0yield Compare\u00a0the\u00a0yield\u00a0of\u00a0a\u00a0sample\u00a0of\u00a0Monsanto\u2019s \u00a0new\u00a0variety\u00a0 versus\u00a0the\u00a0historical\u00a0yield\u00a0average\u00a0of\u00a0the\u00a0traditional \u00a0variety\u00a0and\u00a0 test\u00a0Monsanto\u2019s \u00a0claim\uf0e0\u00a0Hypothesis \u00a0Testing:\u00a0 Null\u00a0and\u00a0Alternative \u00a0Hypothesis Monsanto \u00a0claims\u00a0that\u00a0a\u00a0new\u00a0crop\u00a0variety\u00a0has\u00a0a\u00a0higher\u00a0yield Compare\u00a0the\u00a0yield\u00a0of\u00a0a\u00a0sample\u00a0of\u00a0Monsanto\u2019s \u00a0new\u00a0variety\u00a0versus\u00a0the\u00a0 historical\u00a0yield\u00a0average\u00a0of\u00a0the\u00a0traditional \u00a0variety\u00a0and\u00a0test\u00a0Monsanto\u2019s \u00a0 claim \u2022Test\u00a0Statistic:\u00a0Mean\u00a0 \u2013Distribution: \u00a0t\u2010student \u2022Null\u00a0Hypothesis \u00a0H0:\u00a0\u03bc\u00a0\u2264\u00a0\u03bc0 \u2022Alternative \u00a0Hypothesis \u00a0H1:\u00a0\u03bc\u00a0>\u00a0\u03bc0\u03bc: mean yield of the  new variety \u03bc0: mean yield of the  traditional varietyNull Hypothesis (\u201cstatus quo\u201d): the sample being tested could  have been drawn form the population being testedP (x < m) = 1 - p \u03bc0 t-Student mP (x > m) = pHypothesis \u00a0Testing:\u00a0p\u2010value \u2022Set\u00a0the\u00a0confidence \u00a0interval\u00a0so\u00a0that \u2022p\u00a0=\u00a0probability \u00a0of\u00a0observing \u00a0a\u00a0 population \u00a0sample\u00a0as\u00a0extreme\u00a0or\u00a0 more\u00a0extreme\u00a0than\u00a0the\u00a0one\u00a0being\u00a0 tested\u00a0when\u00a0drawing\u00a0from\u00a0the\u00a0 population \u00a0with\u00a0mean\u00a0\u03bc0 \u2013p\u00a0>>\u00a00:\u00a0null\u00a0hypothesis \u00a0likely \u2013p\u00a0~\u00a00:\u00a0null\u00a0hypothesis \u00a0not\u00a0likelym\uf03d\uf06d0\uf02bt(N\uf02d1)ps N How much do we have to \u201cstretch\u201d the confidence  interval to \u201cexplain\u201d the observed sample mean?Hypothesis \u00a0Testing:\u00a0p\u2010value \u2022Null\u00a0Hypothesis :\u00a0 \u2013statistical \u00a0model\u00a0where\u00a0differences \u00a0 are\u00a0only\u00a0due\u00a0to\u00a0random\u00a0fluctuations \u00a0(sampling) \u2022P\u2010value : \u2013Probability \u00a0that\u00a0the\u00a0null\u00a0hypothesis \u00a0model\u00a0 does\u00a0not\u00a0explain\u00a0the\u00a0data \uf0e0The\u00a0differences \u00a0observed \u00a0are\u00a0probably\u00a0due\u00a0 to\u00a0some\u00a0underlying \u00a0phenomenonHypothesis \u00a0Testing:\u00a0Error\u00a0Types \u2022Depending \u00a0on\u00a0the\u00a0p\u2010value,\u00a0 you\u00a0can\u00a0decide\u00a0to\u00a0reject\u00a0or\u00a0not\u00a0the\u00a0null\u00a0hypothesis \u2013P\u2010value\u00a0threshold \u00a0for\u00a0rejection: \u00a0\u03b1\u00a0(common \u00a0values\u00a00.05,\u00a00.01) \u2013There\u00a0has\u00a0to\u00a0be\u00a0sufficient \u00a0evidence\u00a0to\u00a0reject\u00a0the\u00a0null\u00a0hypothesis (in\u00a0the\u00a0criminal\u00a0trial,\u00a0the\u00a0defendant \u00a0is\u00a0not\u00a0guilty,\u00a0unless\u00a0proved\u00a0guilty) \u2013Multiple\u00a0testing\u00a0issuesH0: FALSE H0: TRUE Type-II Error (False Negative)OK (True Negative)H0NOT REJECTED OK (True Positive)Type-I Error (False Positive)H0REJECTEDHypothesis \u00a0Testing:\u00a0Error\u00a0Types \u2022Depending \u00a0on\u00a0the\u00a0p\u2010value,\u00a0 you\u00a0can\u00a0decide\u00a0to\u00a0reject\u00a0or\u00a0not\u00a0the\u00a0null\u00a0hypothesis \u2013Using\u00a0the\u00a0p\u2010value\u00a0for\u00a0the\u00a0decision\u00a0 \u2022P\u2010value\u00a0<\u00a0\u03b1:\u00a0reject\u00a0H0\u00a0 \u2022P\u2010value\u00a0\u2265\u00a0\u03b1:\u00a0do\u00a0not\u00a0reject\u00a0H0\u00a0 enables\u00a0to\u00a0control\u00a0the\u00a0Type\u2010I\u00a0Error\u00a0but\u00a0not\u00a0the\u00a0Type\u2010II\u00a0ErrorH0: FALSE H0: TRUE Type-II Error (P = \u03b2| H0FALSE)True Negative (P = 1-\u03b1 | H0TRUE)H0NOT REJECTED True Positive (P = 1-\u03b2 | H0FALSE)Type-I Error (P =\u03b1 | H0TRUE)H0REJECTEDOne\u2010tail\u00a0Test P = 1 - p \u03bc0P = p \u03bc0-\u2026 \u03bc0+ \u2026 P = 1 - p \u03bc0P = p \u03bc0-\u2026 \u03bc0+ \u2026\u2022Null\u00a0Hypothesis: \u00a0\u03bc\u00a0\u2264\u00a0\u03bc0 \u2022Alternative \u00a0Hypothesis: \u00a0\u03bc\u00a0>\u00a0\u03bc0 R:\u00a0set\u00a0input\u00a0argument \u00a0of\u00a0the\u00a0test alternative = \"greater\" \u2022Null\u00a0Hypothesis: \u00a0\u03bc\u00a0\u2265\u00a0\u03bc0 \u2022Alternative \u00a0Hypothesis: \u00a0\u03bc\u00a0<\u00a0\u03bc0 R:\u00a0set\u00a0input\u00a0argument \u00a0of\u00a0the\u00a0test alternative = \"less\"Two\u2010tail\u00a0Test \u2022Null\u00a0Hypothesis: \u00a0\u03bc\u00a0=\u00a0\u03bc0 \u2022Alternative \u00a0Hypothesis: \u00a0\u03bc\u00a0\u2260\u00a0\u03bc0 R:\u00a0set\u00a0input\u00a0argument \u00a0of\u00a0the\u00a0test alternative = \"two.sided\" P = 1 - p \u03bc0P = p/2 P = p/2 \u03bc0-\u2026 \u03bc0+\u2026One\u2010sample\u00a0t\u2010Test\u00a0(Mean\u00a0Difference): \u00a0R \u2022Goal:\u00a0does\u00a0the\u00a0sample\u00a0belong\u00a0to\u00a0a\u00a0population \u00a0with\u00a0mean\u00a0 larger/smaller/different \u00a0than\u00a0a\u00a0reference \u00a0population \u00a0with\u00a0mean\u00a0 \u03bc0? \u2022Input \u2013Reference \u00a0population \u00a0mean\u00a0(\u03bc0) \u2013Sample\u00a0values \u2022Assumptions \u2013Independence \u2022The\u00a0sample\u00a0has\u00a0been\u00a0randomly \u00a0drawn,\u00a0 \u2022There\u00a0is\u00a0no\u00a0dependence \u00a0between\u00a0sample\u00a0units \u2013Distribution \u2022Small\u00a0samples\u00a0(N\u00a0<\u00a030):\u00a0population \u00a0normally\u00a0distributed \u2022Large\u00a0samples\u00a0(N\u00a0\u2265\u00a030):\u00a0noneOne\u2010sample\u00a0t\u2010Test\u00a0(Mean\u00a0Difference): \u00a0R \u2022Example\u00a0(Monsanto\u2019s \u00a0new\u00a0variety) \u2013Reference \u00a0yield\u00a0mean:\u00a02400 \u2013Sample\u00a0yields:\u00a02531,\u00a02659,\u00a02487,\u00a02398,\u00a02771 \u2013Alternative: \u00a0Monsanto \u00a0larger\u00a0than\u00a0reference t.test (x = c (2531, 2659, 2487, 2398, 2771),  mu = 2400, alternative = \"greater\") # t = 2.5756, df = 4, p-value = 0.03081 # 95 percent confidence interval:#  2429.151      Inf  # output class: list, with slots: t.test (\u2026)$p.value; t.test (\u2026)$statisticTwo\u2010sample\u00a0t\u2010Test\u00a0(Mean\u00a0Difference) \u2022Goal:\u00a0do\u00a0the\u00a0samples\u00a0belong\u00a0to\u00a0populations \u00a0with\u00a0mean\u00a0 larger/smaller/different? patients\u00a0before\u00a0and\u00a0after\u00a0treatment t.test (x = \u2026, y = \u2026, alternative = \u2026,paired = T)Non\u2010parametric \u00a0Test\u00a0(Mean\u00a0Difference) \u2022When\u00a0the\u00a0sample\u00a0is\u00a0small\u00a0and\u00a0the\u00a0normality \u00a0distribution \u00a0 assumption \u00a0is\u00a0not\u00a0met,\u00a0 Use\u00a0the\u00a0Wilcoxon \u00a0test\u00a0(a.k.a. Mann\u2010Whitney\u00a0test) \u2013one\u2010sample \u2013Two\u2010samples wilcox.test (\u2026) \u2013The\u00a0test\u00a0works\u00a0on\u00a0the\u00a0ranks\u00a0of\u00a0the\u00a0values \u2013The\u00a0input\u00a0and\u00a0output\u00a0is\u00a0the\u00a0same\u00a0as\u00a0the\u00a0t\u2010testTests\u00a0Based\u00a0on\u00a0Permutations \u2022In\u00a0the\u00a0previous\u00a0tests\u00a0we\u00a0have\u00a0always\u00a0tested\u00a0the\u00a0 difference \u00a0of\u00a0means\u00a0 \u2013between\u00a0populations, \u00a0 but\u00a0using\u00a0limited\u00a0knowledge \u00a0from\u00a0samples \u2022Thanks\u00a0to\u00a0the\u00a0central\u00a0limit\u00a0theorem, \u00a0we\u00a0knew\u00a0how\u00a0the\u00a0 sampling\u00a0mean\u00a0is\u00a0supposed \u00a0to\u00a0be\u00a0distributed \u2013normal\u00a0or\u00a0t\u2010student,\u00a0depending \u00a0on\u00a0sample\u00a0size \u2022What\u00a0if\u00a0we\u00a0are,\u00a0but\u00a0we\u00a0don\u2019t\u00a0know\u00a0how\u00a0the\u00a0sampling\u00a0 distribution?Tests\u00a0Based\u00a0on\u00a0Permutations \u2022A\u00a0common\u00a0approach \u00a0consists\u00a0of\u00a0permuting \u00a0the\u00a0class\u00a0 labels \u2022and\u00a0computing \u00a0the\u00a0count\u00a0ratio\u00a0of \u2013how\u00a0many\u00a0times\u00a0the\u00a0difference \u00a0observed \u00a0for\u00a0real\u00a0data\u00a0is\u00a0 also\u00a0observed \u00a0for\u00a0permuted \u00a0data \u2013the\u00a0number\u00a0of\u00a0permutations \u2022The\u00a0resulting\u00a0index\u00a0is\u00a0called\u00a0empirical \u00a0p\u2010valueTest\u00a0Summary \u00a0Tables Small Sample Population not  normally distr.Small Sample Population  normally distr.Large Sample (N \u2265 30) Wilcoxon test t-Test (t-Student, df = N- 1)z-Test (Standard Normal) Two-tail One-Tail SmallerOne-Tail Greater \u03bc \u2260 \u03bc0 \u03bc < \u03bc0 \u03bc > \u03bc0 One-sample \u03bc1\u2260 \u03bc2 \u03bc1< \u03bc2 \u03bc1> \u03bc2 Two-samplesALTERNATIVE HYPOTHESISTEST AND DISTRIBUTIONTest\u00a0Summary \u00a0Tables Sample units: dependentSample units: independent Paired Not PairedTYPE OF TWO-SAMPLE TEST  (T-TEST OR WILCOXON TEST ALIKE)Other\u00a0Tests \u2022Proportion \u00a0Test\u00a0(Bernoullian \u00a0Probability) \u2022Fisher\u2019s\u00a0Exact\u00a0Test\u00a0(2x2\u00a0contingency \u00a0tables) \u2022X2\u00a0Test\u00a0(2x2\u00a0or\u00a0larger\u00a0contingency \u00a0tables) \u2022Kolmogorov \u2010Smirnov\u00a0(distribution \u00a0inequality) \u2022\u2026Multiple\u00a0Testing \u2022Previously, \u00a0we\u00a0have\u00a0always\u00a0focused\u00a0on\u00a0single\u00a0tests \u2022If\u00a0we\u00a0test\u00a0many\u00a0independent \u00a0samples\u00a0from\u00a0the\u00a0same\u00a0 population, \u00a0some\u00a0of\u00a0them\u00a0will\u00a0lead\u00a0to\u00a0the\u00a0null\u00a0hypothesis \u00a0 rejection\u00a0 \u2022However, \u00a0even\u00a0if\u00a0the\u00a0null\u00a0hypothesis \u00a0is\u00a0TRUE,\u00a0 we\u00a0do\u00a0expect\u00a0a\u00a0rejection\u00a0rate\u00a0>\u00a00: M*\u03b1,\u00a0where\u00a0M\u00a0is\u00a0the\u00a0number\u00a0of\u00a0tests\u00a0performed \u2022How\u00a0to\u00a0account\u00a0for\u00a0this?Multiple\u00a0Testing:\u00a0 Bonferroni \u00a0Correction \u2022The\u00a0Bonferroni \u00a0correction \u00a0is\u00a0very\u00a0conservative: after\u00a0correction, \u00a0the\u00a0probability \u00a0of\u00a0finding\u00a0at\u00a0least\u00a0one\u00a0 false\u00a0positive\u00a0at\u00a0p\u2010value\u00a0\u2264\u00a0\u03b1\u00a0will\u00a0be\u00a0exactly\u00a0\u03b1 \u2022p\u2019\u00a0=\u00a0MIN\u00a0(p\u00a0*\u00a0M,\u00a01) \u2022This\u00a0correction \u00a0is\u00a0usually\u00a0overly\u00a0conservative \u00a0for\u00a0most\u00a0 genomic\u00a0applications \u00a0(e.g.\u00a0gene\u00a0expression \u00a0microarrays) \u2022It\u00a0is\u00a0sometimes \u00a0recommended \u00a0for\u00a0biomarkers \u00a0and\u00a0risk\u00a0 factorsMultiple\u00a0Testing:\u00a0 Benjamini \u2010Hochberg\u2019s \u00a0FDR \u2022The\u00a0Benjamini \u2010Hochberg \u00a0FDR\u00a0transforms \u00a0the\u00a0p\u2010value\u00a0into\u00a0 a\u00a0q\u2010value \u2022Let\u2019s\u00a0consider\u00a0the\u00a0q\u2010value\u00a0qi,\u00a0 that\u00a0is\u00a0the\u00a0false\u00a0positive\u00a0rate\u00a0when\u00a0considering \u00a0all\u00a0tests\u00a0 with\u00a0q\u00a0\u2264\u00a0qi \u2022qi=\u00a0MIN\u00a0(pi*\u00a0M\u00a0/\u00a0i,\u00a01) followed\u00a0by\u00a0monotonicity \u00a0correction \u00a0(i.e.\u00a0values\u00a0have\u00a0to\u00a0be\u00a0 monotonically \u00a0increasing)Multiple\u00a0Testing:\u00a0 Benjamini \u2010Hochberg\u2019s \u00a0FDR \u2022For\u00a0each\u00a0p\u2010value\u00a0pi \u2013Expected\u00a0number\u00a0of\u00a0false\u00a0positives\u00a0if\u00a0the\u00a0null\u00a0hypothesis \u00a0is\u00a0true: pi\u00a0*\u00a0M\u00a0(\u03b1\u00a0=\u00a0pi) \u2013Observed \u00a0number\u00a0of\u00a0positives: i( p1,\u00a0\u2026,\u00a0pi\u00a0\u2264\u00a0\u03b1) \u2013Ratio\u00a0between\u00a0expected\u00a0false\u00a0positives\u00a0and\u00a0observed \u00a0positives: pi\u00a0*\u00a0M\u00a0/\u00a0i\u00a0BH\u00a0FDR\u00a0correction http://en.wikipedia.org/wiki/False_discovery _rate#Benjamini.E2.80. 93Hochberg_procedureMultiple\u00a0Testing\u00a0in\u00a0R \u2022Input:\u00a0vector\u00a0of\u00a0p\u2010values # Bonferroni p.adjust (pvalue.nv, method = \"Bonferroni\") # Benjamini-Hochberg FDR p.adjust (pvalue.nv, method = \"BH\")Application \u00a0to\u00a0Microarray \u00a0Analysis \u2022For\u00a0the\u00a0typical\u00a0two\u2010class\u00a0design\u00a0 (e.g.\u00a0disease\u00a0vs.\u00a0control,\u00a0treated\u00a0vs.\u00a0untreated) \u00a0 we\u00a0can\u00a0test\u00a0every\u00a0gene\u00a0using\u00a0a\u00a0two\u2010sample\u00a0t\u2010test\u00a0 (not\u2010paired\u00a0or\u00a0paired) \u2013Each\u00a0biological \u00a0replicate\u00a0corresponds \u00a0to\u00a0a\u00a0sample\u00a0unit \u2022Since\u00a0the\u00a0number\u00a0of\u00a0replicates \u00a0is\u00a0typically\u00a0small,\u00a0 the\u00a0stdev\u00a0estimate\u00a0is\u00a0usually\u00a0unreliable Part\u00a0II:\u00a0Lecture\u00a0by\u00a0Dr\u00a0.Ben\u00a0Chandler\u00a0on\u00a0Oncomine \u00a0and\u00a0 Pathway\u00a0analysisStatistics\u00a0references \u00a0on\u00a0the\u00a0Web \u2022http://www.unt.edu/rss/class/Jon/ \u2022http://davidmlane.com/hyperstat/index.html \u2022http://faculty.chass.ncsu.edu/garson/PA765/statnote.ht m\u00a0 \u2022http://www.khanacademy.org/math/statistics \u00a0Statistical \u00a0Rethinking, \u00a0 McElreath Multivariate \u00a0generalizations \u00a0of\u00a0the\u00a0Wald\u2010 Wolfowitz \u00a0test\u00a0(example \u00a0in\u00a02D) \u2022Minimal\u00a0spanning\u00a0trees\u00a0(MST) Multivariate \u00a0generalizations \u00a0of\u00a0the\u00a0 Wald\u2010Wolfowitz \u00a0test\u00a0(example \u00a0in\u00a02D) \u2022Minimal\u00a0spanning\u00a0trees\u00a0(MST) Test statistic: how many edges of the MST, crossover between  two groups?"
    },
    {
      "file": "C:\\temp\\Units\\UNIT_0 - Pre-Reading Materials (Kelly Only)\\Unit-4-Lecture01b-Exploratory Data Analysis.pdf",
      "summary": "- Adapts to the data's story, using creativity to explore and interpret findings.Historical Context and Techniques -Originated with John Tukey's seminal work  \"Exploratory Data Analysis\" in 1977. https://www.amazon.com/Exploratory-Data- Analysis-John-Tukey/dp/0201076160Stem and Leaf Diagrams A stem-and-leaf plot of prime  numbers under 100 shows that the most  frequent tens digits are 0 and 1 while the  least is 9 https://en.wikipedia.org/wiki/Stem-and- leaf_display#/media/File:Stem-and- leaf_time_tables_in_Japanese_train_stations.jpgTechniques EDA Techniques: -Primarily graphical,  supplemented by  quantitative methods. https://sebastiz.github.io/gastrodatascience/Statisti csExploratoryDataAnalysis.htmlGraphical Techniques in Depth -Visualizing Raw Data: - Techniques like bihistograms and Youden plots reveal data distribution and variance. - Strategic positioning of plots leverages our natural pattern-recognition abilities.Bihistograms Purpose of the adjoining histograms: \u25cfBi-histograms serve as a great alternative to two- sample t-test as it is a graphical way of  representing a variety of features of the subgroups  simultaneously on a single plot such as: \u2022Skewness \u2014 shows how distorted the normal  distribution of the two subgroups is. \u2022Location \u2014 shows at which point on the x-axis the  data of the two subgroups is centred. \u2022Scale \u2014 shows if the plot of the two subgroups is  stretched or squeezed. https://www.geeksforgeeks .org/introduction-to-bi- histograms/ Used to observe any changes occurring in the two  subgroups of data in terms of: \u2022Location (observed when performing t-test) \u2022Variation (observed when performing f-test) \u2022Distribution (observed when performing Kolmogorov- Smirnov test)Youden Plots The Youden plot can be used to answer the  following questions: \u25cfAre all labs equivalent? - Not optional but mandatory for deep data insight.The Imperative of Graphics in EDA -Graphical Techniques in EDA: - Essential, not optional, for insightful data analysis. - Provide the clearest path to: - Test assumptions - Select and validate models - Identify relationships and outliers -Without Graphics: - Missed insights into data's structure. https://towardsdatascience.com/exploratory-data- analysis-visual-790990f64c7cExample: Anscombe quartet -Quantitative Analysis Shortcomings: - Identical summary statistics for all datasets: - Mean of X = 9.0, Mean of Y = 7.5 - Intercept = 3, Slope = 0.5 - Residual standard deviation \u2248 1.237 - Correlation \u2248 0.816 - Fails to reveal the true nature of the datasets! - Assumes uncorrelated data with a constant deterministic component and a random  component of fixed variation.Extending Univariate to Multivariate & Model Validation -Generalizing the Model: - The univariate model scales to multivariate scenarios where the deterministic component is a  complex function of variables. - Residuals that deviate from these assumptions suggest an inadequate model, indicating the  need for refinement.Significance of Predictability and Control -Goal of Predictability: - Essential for valid and controllable processes. - Invalid assumptions lead to unpredictable processes and unreliable outcomes.Four Techniques to Test Underlying Assumptions -The following EDA techniques are simple, efficient, and powerful for the routine  testing of underlying assumptions: - run sequence plot (Yiversus i) - lag plot (Yiversus Yi-1) - histogram (counts versus subgroups of Y) - normal probability plot (ordered Y versus theoretical ordered Y)Interpreting the Run Sequence and Lag Plots -Testing Fixed Location and Variation: - Run Sequence Plot: - A flat, non-drifting pattern indicates a stable mean (fixed location). -Assessing Randomness: - Lag Plot: - A lack of discernible structure or pattern implies true randomness in the data.Interpreting the Histogram and Normal Probability Plot -Evaluating Distribution: -Histogram: - A bell-shaped distribution suggests symmetry and potential normality. -Normal Probability Plot: (or Q-Q plot with a reference normal) - Linearity in the plot supports the assumption of a normal distribution. - This indicates predictability and reliability in the process being analyzed.Example: Assumptions Hold This plot of 500 normal random numbers reveals a process that has fixed location, fixed variation, is random,  apparently has a fixed approximately normal distribution, and has no outliers. Example: Assumptions Don\u2019t Hold (has effect on model  choices for e.g, in regression) This 4-plot reveals a process that has fixed location, fixed variation, random, has a non-normal, a Uniform  distribution, and has several outliers. - Unreliable model parameters.Implications of Autocorrelation and Non-Randomness -Autocorrelation: A Common Non-Randomness: - Correlation of a variable with its past values (Yt with Yt-k). -Addressing Non-Randomness: - Recognizing non-randomness is the first step to rectifying model assumptions and ensuring  the integrity of the data analysis.Consequence of Non-Fixed Location Parameter -Location Parameter - The Mean: - Standard estimate of location:  -When Location Isn't Fixed: -Potential Drift: Location may change over time, making a single estimate unreliable. Consequences of Non-Fixed Variation Parameter -Variation Parameter - The Standard Deviation: - Commonly used estimate of variation: -Challenges with Non-Constant Variation: -Variation Drift: If variation changes over time, a single standard deviation loses its meaning. -Quality of Estimate: The standard deviation may not accurately reflect the true variability of the  data. Consequences Related to Distributional Assumptions -Consequences of Distributional Assumptions in Data Analysis - The Role of Distribution in Estimation: - Choice of location estimator (mean, median, etc.) -Impact of Distributional Misconceptions: - A changing distribution invalidates the use of a single, static estimate. - An incorrect model of error distribution compromises the entire analysis.Consequences Related to Distributional Assumptions -Modeling Consequences: - A model based on incorrect distributional assumptions may be unstable or misleading. -Model: - y = constant + error -Output: - A number (the estimated constant in the model). - An estimate of the distribution for the error. https://en.wikipedia.org/wiki/ Normal_probability_plotProbability Plot -Transforming Medians to Fit Your Distribution - Apply the inverse CDF (percent point function) of your distribution to each median. -Using the Plot -Intercept and Slope:  Estimates for location and scale parameters of the distribution. - What are the parameter estimates for the distribution?PPCC (probability plot correlation coefficient) Plot -Purpose: To find the best shape parameter for a  distribution family. -Interpretation: - A peak in the PPCC plot suggests the optimal shape  parameter. https://scipy.github.io/devdocs/reference/generated/scipy.stat s.ppcc_max.htmlComparative -Data: - A single response variable and k independent variables (Y, X1, X2, ... , Xk), primary focus is on  one (the primary factor) of these independent variables. -Model: - y = f(x1, x2, ..., xk) + error -Output: - A \"yes\" or \"no\" to the question \"Is the primary factor significant?\" -Example Analysis: In a scenario with 12 different  conditions, if weld method 2 is better in 10, the chance  of this happening randomly is about 2%. -What is the best level of the primary factor? -Definition:  - Box plots graphically depict the median, quartiles, and  extremes of the data. - The box represents the interquartile range (middle 50%  of data). -Importance: - Box plots are crucial for assessing the impact of factors on data distribution. -Scanning all plots helps identify the strongest pairwise relationships.https://blogs.sas.com/content/graphicallyspeaking/2012/1 0/07/scatter-plot-matrix-with-a-twist/Scatter Plot Matrix -Structure: - Each cell in the matrix represents a scatter plot, with one variable on the x-axis and another on  the y-axis. -Note: - The scatterplot matrix concept can be adapted for other types of pairwise plots, such as  quantile-quantile plots or bihistograms.Conditioning Plot -Structure: -The conditioning variable (Z) is divided into groups, which can be based on natural groupings, equal  sizes, clusters, etc. -The layout consists of rows and columns, with each cell representing a scatter plot for a subset of Z. -Note: -The conditioning plot goes beyond the scatterplot matrix by revealing the influence of a third variable  on the relationship between two other variables.Conditioning Plot -Purpose: - To investigate the relationship between two variables while considering the influence of a third variable. - Realtionship between age and survival, conditioned on fare class in the Titanic https://www.geeksforgeeks.org/conditioning-plot/ Sankey plots \u25cfStudying group variations,  for modeling  Parallel Coordinate plots \u25cfThis type of visualization is used for  plotting multivariate, numerical data. Parallel Coordinates Plots are ideal for  comparing many variables together and  seeing the relationships between them.https://datavizcatalogue.com/methods/parallel_coordinates.html Screening -Data: - A single response variable and k independent variables (Y, X1, X2, ... , Xk). -Techniques: - Block Plot, Probability Plot, BihistogramBihistogram -Purpose:  - The bihistogram is a visual tool for evaluating shifts in location,  variation, or distribution due to an intervention or change. -Assessment: -Location: Shifts in the central tendency are easily visualized by the horizontal displacement  between the two histograms. -Distribution: The shape of the histograms can suggest changes in the distribution, such as  skewness.https://www.geeksforgeeks.org/introduction-to-bi-histograms/Bihistogram -Detailed Observations: -From the sample bihistogram, batch 1 (top) shows a rightskew, while batch 2 (bottom) appears more  symmetric, possibly with a slight right skew. -Particularly useful for before-and-after comparisons in scientific and engineering contexts.Regression (More in the future) -Data: - A single response variable and k independent variables (Y, X1, X2, ... , Xk). -Techniques: - Least Squares Fitting, Scatter PlotHomogeneity in Variance  (affects regression  assumptions) Plot with random data showing homoscedasticity: at  each value of x, they-value of the dots has about the  samevariance .Plot with random data showing heteroscedasticity: The  variance of the y-values of the dots increases with  increasing values of x.https://en.wikipedia.org/wiki/Homosceda sticity_and_heteroscedasticityTime Series -Data: - A column of time dependent numbers, Y. If the data are not equi-spaced, the time variable  should be explicitly provided. - Affects the validity of standard statistical tests and formulas.Spectrum Plot -Purpose: -To analyze cyclic patterns in data using frequency domain  techniques. -Measures of Location, Confidence Limits for the Mean and One Sample t-Test, Two Sample t-Test  for Equal Means, One Factor Analysis of Variance, Multi-Factor Analysis of Variance -Scale (or variability or spread): (covered in Lec 1?) -Measures of Scale, Bartlett's Test, Chi-Square Test, F-Test, Levene Test, Skewness and Kurtosis,  Measures of Skewness and Kurtosis -Randomness:  -Autocorrelation -Distributional Measures:  -Chi-Square Goodness-of-Fit Test -Kolmogorov-Smirnov Test -Outliers:  -Grubbs\u2019s Test -Generalized Extreme Deviate TestRandomness: Autocorrelation -Purpose of Autocorrelation - Detect non-randomness in data. - Guides the transformation or differencing needed to achieve stationarity.Distributional Measures: Chi-Square Goodness-of-Fit Test -Purpose:  - Assess if a sample comes from a population with a specific distribution. -Test Framework -Hypotheses:   - H0: Data do not follow the specified distribution - H1: Data do not follow the specified distribution. - Helps in selecting the correct model for analysis and forecasting.Distributional Measures: Kolmogorov-Smirnov Test -Purpose - To determine if a sample comes from a population with a specific, continuous distribution. - More sensitive near the center of the distribution than at the tails.Distributional Measures: Kolmogorov-Smirnov Test -Test Statistic - F is the theoretical CDF of the distribution being tested. - D is the maximum vertical distance between the EDF and the theoretical CDF. - Requires the distribution to be fully specified; parameters estimated from the data necessitate  simulation for critical values. Distributional Measures: Kolmogorov-Smirnov Test -Statistical Significance - Validates the assumption of a specific distribution for a given dataset. -Interpretation - A small D value suggests no significant difference between the sample and the theoretical  distribution. - A large D value indicates a likely discrepancy, leading to the rejection of the null hypothesis. -Critical Values - The null hypothesis is rejected if D is greater than the critical value for the given sample size at  a chosen significance level \u03b1. - Critical values are typically obtained from K-S tables or through simulation if parameters are  estimated from the data.Outliers: Introduction -Definition: Outliers are unusual points in data, distinct from the majority. - To understand the nature of the data. -Validity: Use robust methods if outliers are valid.CUSUM plots (change points in time series) https://ctruong.perso.math.cnrs.fr/ruptures-docs/build/html/detection/window.html3 Missing Data AnalysisIntroduction to Missing Data -What is Missing Data? -Why it Matters: -Impact on Analysis: Missing data can lead to biased  estimates, reduce the efficiency of analyses, and can  invalidate the results. https://www.anyamemensah.com/blog/set-missing-valuesTypes and Patterns of Missing Data -Types of Missing Data: -MCAR (Missing Completely at Random): The missingness of data is independent of both  observed and unobserved data. -MAR (Missing at Random):  The propensity for a data point to be missing is not related to the  missing data, but is related to some of the observed data. -MNAR (Missing Not at Random): The missingness is related to the value of the variable that's  missing. -Patterns of Missing Data: -Univariate vs. Multivariate:  Missing data can occur in a single variable or across multiple  variables. -Pros and Cons: Simple but can lead to significant data loss and biased results if the data is not MCAR. -Mean/Median/Mode Imputation: -Description: Replaces missing values with the mean, median, or mode of the observed values. -Pros and Cons: Easy to implement but distorts the distribution and underestimates variability.Advanced Imputation Techniques -K-Nearest Neighbors (KNN) Imputation: -Description: Imputes missing values based on the k-nearest neighbors found in the observed  portion of the dataset. -Multiple Imputation: -Description: Involves creating multiple imputations (predictions) for missing values to reflect  the uncertainty around the true value. -Pros and Cons: Provides a robust way to handle missing data by incorporating randomness  and producing valid statistical inferences.Model-Based Methods for Missing Data -Maximum Likelihood Estimation (MLE): -Description:  Estimates parameters by maximizing the likelihood function, accounting for the  missing data pattern. -Application: Particularly useful when the data is MAR and can be used with a variety of  statistical models. -Expectation-Maximization (EM) Algorithm: -Description: An iterative process that estimates missing data by maximizing the expected  likelihood found using the observed data. -Pros and Cons: Can provide accurate estimates but assumes that the missing data  mechanism is correctly specified.Iterative Imputation and Machine Learning -Iterative Imputation: -Description: A more sophisticated form of imputation that models each feature with missing  values as a function of other features in a round-robin fashion. -Machine Learning for Imputation: -Description: Utilizes algorithms like Random Forests and Neural Networks to predict missing  values based on patterns found in the data. -Pros and Cons: Can capture complex patterns and interactions but may overfit or be  computationally intensive.Imputation via matrix completion for tabular  data https://www.semanticscholar.org/paper/Imputing-Structured-Missing-Values-in-Spatial-Data-Wang- Tan/82585e95cd4d8882886d784d664c583b0e59734dImputation via GPT for tabular data? -Pros and Cons: Useful when internal data is insufficient, but risks of incompatibility with  current data patterns.Practical Considerations in Handling Missing Data -Assessing Missing Data Mechanism: - Importance of understanding the underlying mechanism to choose the appropriate handling  method. - Consideration of the learning curve and community support for each tool.Sensitivity Analysis and Validation -Sensitivity Analysis: -Description: A method to test how sensitive conclusions are to different assumptions about  the missing data. -Importance:  Helps to understand the potential impact of missing data on the study's findings. -Importance: Ensures that the imputation method provides reliable and accurate results.Conclusion -Recap of Key Points: - Missing data is a common issue that can significantly impact statistical analysis. - A variety of methods exist to handle missing data, each with its own set of assumptions and  implications. Thank you!Quantile-quantile plot and KS test to test  equality of distributions A Q\u2013Q plot comparing the distributions  ofstandardized daily maximum  temperatures at 25 stations in the US  state of Ohio in March and in July. The  curved pattern suggests that the  centralquantiles are more closely  spaced in July than in March, and that  the July distribution is skewedto the left  compared to the March distribution. \u2022 Illustration with gene expression data.EDAWhat is PCA The goal of principal component analysis (PCA) is to  transform  a number of possibly correlated variables into a  smaller number of uncorrelated variables called principal  components. The possibly smaller number of variables can be used for  data reduction and visualization . PCs are orthogonal (i.e. uncorrelated).EDAWhat does it mean for data to be  \"correlated\"?set.seed(2707) x1 <- rnorm(500,0,1) y1 <- rnorm(500,0,1) y2 <- 2*x1 + y1 y2 <- y2-mean(y2) y2 <- y2 / sd(y2) plot(x1, y1) plot(x1, y2) Opar <- par(no.readonly=TRUE) par(mfrow = c(2,2)) hist(x1) hist(y2) plot(x1, y1) plot(x1, y2) par(Opar) Correlations between real world data are ubiquitous, due to causal  relationships or confounding factors.EDAPCA and correlation Principal component analysis (PCA) converts a set of observations of possibly correlated  variables into a set of values of uncorrelated variables called principal components . The first principal component has as high a variance as possible  (that is, accounts for as  much of the variability in the data as possible); each succeeding component in turn has  the highest variance possible under the constraint that it be orthogonal  to (uncorrelated  with) the preceding components. Therefore the PCs provide a view on the structure of the data that best explains its  variance. Wikipedia: Principal component analysisEDAPCA and correlation The example data is  two-dimensional, but  most of the  information is  contained along a  dimension shown  here by the red vector. EDAInterpretation of PCs Given a set of points in Euclidean  space, the  first principal component   corresponds to a line that passes  through the multidimensional mean  and minimizes the sum of squares of  the distances of the points from the  line. The second principal component  is  calculated in the same way, after all  correlation with the first principal  component has been subtracted out  from the points. [...] EDAPCA scaling problem PCA is sensitive to the scaling of the variables. This means that whenever the different variables have different units  (like temperature and mass), PCA is a somewhat arbitrary method of  analysis. One way to address this is to scale  variables to have unit variance.EDAapplication to gene expression (when you want to..) \u2022 Dimension reduction (simplify a dataset) \u2022 Clustering (too many samples) \u2022 Discriminant analysis (find a group of genes that  discriminates two categories)  \u2022 Exploratory data analysis tool (are there correlations) \u2022 Find the most important signal in data 2D projections  (clusters? )EDACrabs data The crabs data frame has 200 rows and 8 columns, describing 5  morphological measurements on 50 crabs each of two colour forms and  both sexes, of the species Leptograpsus variegatus  collected at Fremantle,  W.Australia. plot(crabs[, 4:8], pch=as.numeric(fac)) plot(crabs[, 4:5], pch=as.numeric(fac)) EDACrabs data There are four distinct groups of crabs in the data, but it seems difficult to tell them apart based on the  data.EDACrabs data pcaCrabs <- princomp(crabs[, 4:8]) plot(pcaCrabs) summary(pcaCrabs) > summary(pcaCrabs) Importance of components: Comp.1      Comp.2      Comp.3       Comp.4       Comp.5 Standard deviation     11.8322521 1.135936870 0.997631086 0.3669098284 0.2784325016 Proportion of Variance  0.9824718 0.009055108 0.006984337 0.0009447218 0.0005440328 Cumulative Proportion   0.9824718 0.991526908 0.998511245 0.9994559672 1.0000000000 EDACrabs data biplot(pcaCrabs, xlabs=as.numeric(fac)) legend(81, -63,c(\"1: B.F\", \"2: B.M\", \"3: O.F\", \"4: O.M\"), box.col=1, bg=\"lightgrey\") EDA Crabs data biplot(pcaCrabs, xlabs=as.numeric(fac), choices = c(1, 3)) legend(84, -63, c(\"1: B.F\", \"2: B.M\", \"3: O.F\", \"4: O.M\"), box.col=1, bg=\"lightgrey\")EDA crabs data biplot(pcaCrabs, xlabs=as.numeric(fac), choices = c(2, 3)) legend(-14.8, 16.2, c(\"1: B.F\", \"2: B.M\", \"3: O.F\", \"4: O.M\"), box.col=1, bg=\"lightgrey\")EDAapplication to gene expression CHO cell cycle data set for 384 genes (Cho et.al,  Molecular Cell, 1998). cho.data <- read.table(\"logcho_237_4class.txt\", skip=1)[, 3:19] dim(cho.data) head(cho.data) cho.mean <- apply(cho.data, 1, \"mean\") cho.sd <- apply(cho.data, 1, \"sd\") cho.data.std <- (cho.data-cho.mean)/cho.sd # Change the column names names(cho.data.std) <- paste(\"t\", 1:17, sep=\"\")      class c1       c2       c3       [...]   c14      c15      c16      c17 YAL040c  1  6.98934  6.92067  6.62274  [...]  6.99118  7.13966  7.12287  6.94794 YAL062w  2  5.55683  5.45532  5.34233  [...]  4.41884  3.58352  4.04305  3.63759 YAL067c  2  3.71357  3.29584  3.17805  [...]  2.19722  3.09104  2.07944  3.17805 YAR007c  1  5.26269  5.72359  6.76619  [...]  5.96101  5.46806  5.22036  5.48480 YBL023c  1  4.09434  4.90527  5.17048  [...]  4.27667  4.72739  4.94164  5.09375EDAapplication to gene expression pcaCho <-princomp(cho.data.std) plot(pcaCho, n=17) Dimension reductionEDAapplication to gene expression plot(princomp(rU), n=17,  ylim = c(0, 3))EDAapplication to gene expression Opar <- par(no.readonly = TRUE) par(mfrow = c(2,2)) biplot(pcaCho) biplot(pcaCho, choices = c(1, 3)) biplot(pcaCho, choices = c(2, 3)) biplot(pcaCho, choices = c(3, 4)) par(Opar) Are any of these  projections \"better\"?EDA application to gene expression Examine the Principal Components (\"eigengenes\"). Exercise: include the first fivePCs in the same  plot.matplot(pcaCho$loadings[, 1:3],         type=\"b\", lwd=3,         ylab=\"PCs\") legend(14, -0.4,         c(\"PC1\", \"PC2\", \"PC3\"),        bg=\"lightgrey\",        col=1:3, lty=1:3, lwd=2,        pch=as.character(1:3))EDAapplication to gene expression Choose some  gene indices  and plot the  selected  expression  profiles  separately.EDAapplication to gene expression Sel1 <-c(73, 235, 83, 216) Sel2 <-c(86, 148, 72, 104) Opar <-par(no.readonly = TRUE) matplot(t(cho.data.std[Sel1, ]),  type=\"b\", lwd=3, col=\"1\", ylab=\"Clustered genes\", xlab = \"Timepoints\", ylim = c(-2, 3)) par(new=TRUE) matplot(t(cho.data.std[Sel2, ]), type=\"b\", lwd=3, col=\"2\", ylab=\"\", xlab=\"\", ylim = c(-2, 3)) par(Opar)EDAConclusion \u2022 PCA is a powerful tool for Exploratory Data Analysis (may need to  standardize data) \u2022 Can be very useful to detect confounding variables. \u2022PCA of genes (eigen-genes: linear combinations of columns, i.e.  genes) \u2022PCA of samples (eigen-assays: linear combination of rows, i.e.  samples) \u2022 Downside: You lose the interpretation of variables. \u2022Model based methods can be applied on the fly.EDAMulti-Dimensional Scaling (MDS) \u25cfSlides courtesy Prof. George Michailidis, University of Michigan EDAOverview of MDS \u25cfOriginated in psychometrics, where researchers wanted to visualize the \u2018similarity\u2019  space of various stimuli \u25cfAlso, widely used as a dimension reduction technique, when one is particularly  interested in visualizationEDAMDS as a dimension reduction technique \u25cfGoal is to map multidimensional space into few dimensions (usually 2-3) so as to be  able to visualize the results \u25cfIn addition, we want \u25cbCapture as much as possible of the underlying variation \u25cbHowever, the technique works very differently than PCAEDAMDS as a dimension reduction technique \u25cfMDS works by minimizing differences between inter- object distances in high and low dimensional space \u25cfFrom a mathematical point of view, borrows ideas from  distance geometryA cool application  (from Tenenbaum et  al.) EDASome additional remarks \u25cfThe only requirement in applying MDS is that some  basis exists for rating or ranking the objects in terms of  similarity/dissimilarity \u25cfObjects can be either observations or variables \u25cfAs long as the \u201cdistance\u201d between the objects can be  assessed in some fashion, MDS can be used to find the  lowest dimensional space that still adequately captures  the distances between objects. \u25cfOnce the number of dimensions is identified, a further  challenge is identifying the meaning of those  dimensions.Look at a trivial example (distances between European cities) Athens Berlin Dublin London Madrid Paris Rome Warsaw Athens 0 1119 1777 1486 1475 1303 646 1013 Berlin 1119 0 817 577 1159 545 736 327 Dublin 1777 817 0 291 906 489 1182 1135 London 1486 577 291 0 783 213 897 904 Madrid 1475 1159 906 783 0 652 856 1483 Paris 1303 545 489 213 652 0 694 859 Rome 646 736 1182 897 856 694 0 839 Warsaw 1013 327 1135 904 1483 859 839 0-3.00-2.00-1.00.001.002.003.00 -3.00 -2.00 -1.00 .00 1.00 2.00 3.00BerlinWarsaw RomeParis MadridLondonDublin AthensEDAExample (contd) \u25cfIn this example, the meaning of the dimensions is quite clear  (north-south, east-west) \u25cfThe technique successfully reconstructs the physical map. \u25cfMetric MDS \u25cfNon-metric MDS (more to come\u2026)EDAAn interesting application \u25cf30 biological samples and 7000 variables \u25cfCalculate correlation matrix (treating the objects as \u2018variables\u2019) \u25cfWe can examine the heatmap of the correlation matrix \u25cfThe correlation matrix can be thought of as a \u2018similarity\u2019 mapEDAA look at the math of MDS \u25cfMDS requires as input an NxN matrix of dissimilarities \u25cfIf we have similarities, then need to turn them to dissimilarities \u25cfIf dissimilarities satisfy the triangle inequality, then we are dealing with metric scaling,  otherwise with non-metric scaling \u25cfQuestions: how many dimensions are enough for embedding? For each flower, 4 attributes: Sepal  Length, Sepal Width, Petal Length and Petal  Width are collected Also referred to as \u201cAnderson\u2019s Iris Flower Data set\u201d,  after Edgar Anderson (a famous American  botanist, who collected this data) http://en.wikipedia.org/wiki/Iris_flower_data_set EDAMDS code example data(iris)  library(cluster)  iris.dist <- daisy(iris[,1:4])  library(stats)  ##two-dimensional MDS iris.mds <- cmdcale(iris.dist,k=2)  ##computing original distances fitted.dist <- daisy(iris.mds)  ##assess goodness-of-fit sum((iris.dist-fitted.dist)\u02c62)/sum(fitted.dist) [1] ##plot MDS solution plot(iris.mds[,1],iris.mds[,2])"
    },
    {
      "file": "C:\\temp\\Units\\UNIT_0 - Pre-Reading Materials (Kelly Only)\\Unit-5-JDSSV_V3_I1.pdf",
      "summary": "For example, it is now commonplace for underlying analysis code and data to be proffered alongside journal publications and conference talks. This makes it difficult to assess the code and its findings, for example, in a peer review process. These technologies are: (1) analysis containerization, which leverages virtualization to fully encap- sulate analysis, data, code and dependencies into an interactive and shareable format, and (2) notebooks, a literate programming format for interacting with analyses. This review surveys how the combination enhances the accessibility and reproducibility of code, analyses, and ideas. Open sharing not only allows results to be disseminated and built upon, but also allows scrutiny and verification of the research and is fundamental to the scientific process itself. The modern notion of sharing research encompasses not only sharing prose and proofs, but also sharing code and data. It is now commonplace for data and the accompanying analysis code to be shared through online repositories. For example, most of the journals sponsored by the Interna- tional Statistical Association and American Statistical Association require data and code be posted along with analysis (Journal of the American Statistical Association 2022). However, CRAN and PyPI are intended for software, not to host full analyses for the purposes of reproducibility. Zenodo is operated by CERN and allows hosting up to 50GB of data while Figshare is operated by Digital Science and has a limit of 20GB. Among these, two important issues are (1) actually running the shared code, and (2) understanding and interacting with the code. For example, the package may not be available for the current version of the language or dependencies of the package may fail to install. Modern analysis often relies on a large and complex collection of interdependent software packages and thus there are many places for such version or dependency issues to arise. For example, troubleshooting failed installations of dependencies can often lead down a chain of fixing cryptic installation errors which is difficult even for an experienced user.Journal of Data Science, Statistics, and Visualisation 3 In addition to the challenges of taking analysis from one computer and running it on another, a second major challenge is difficulty understanding or interacting with code. Containerization is a virtualization technology that allows encapsulation of an entire computing environment including data, code, dependencies, and programs into a reproducible, shareable, and self-contained format (N\u00fcst et al. 2020). Containerization is a flexible approach that allows one to encapsulate any format or organization of analysis according to their preferences and assessment of the best way to organize and share the analysis. Containerizing notebooks makes for some of the most clear, concise, and intuitive ways of documenting and interacting with analysis. In this work, we will review how the merging of containerization and notebook soft- ware can be used to create interactive and reproducible analyses. In addition to an overview, we will also make concrete recommendations for what we believe to be the most straight-forward tools and workflows to enhance reproducibility through con- tainerized notebooks. The remainder of this paper is organized as follows: Section 2 reviews barriers to computational reproducibility in statistics, how containerization helps, and the landscape of available tools. This was emphasized by the American Statistical Association\u2019s 2017 recommendations on reproducible research, which noted that \u201c[reproducible code] may initially sound like a trivial task but experience has shown that it\u2019s not always easy to achieve this seemingly minimal standard.\u201d(Broman et al. 2017) One major source of trouble is ensuring correct code dependencies. The most familiar example of this is installing add-on packages for a language like ggplot24 Containerization for Reproducible Analysis forRornumpyforPython. While add-on dependencies are easy to install in some cases, this can quickly become complicated, for example, if the original analysis used a now out-of-date version of a package. The blue box indicates the system-level dependency of the package for Linux OS Ubuntu ver. The CRAN task view on reproducible research (Blischak and Hill 2021) lists several packages for this purpose like checkpoint (Ooi et al. 2021), ground- hog(Simonsohn and Gruson 2021), and renv(Ushey 2021). One can think of virtualization as making a copy of the computer where the code was originally written. While virtualization has been around for decades, containerization is the latest incarnation of the technology and comes with several key advantages over itsJournal of Data Science, Statistics, and Visualisation 5 predecessors. Containers only virtualize the high-level components of the operating system (e.g., code, configuration files, software and data) and seamlessly re-use the stable low-level processing components of the host operating system (Turnbull 2014). Indeed, starting up a container doesn\u2019t actually start up a second instance of an operating system; it largely just changes all references for re- sources, system libraries, files, and data, to refer to a particular isolated section of the computer. The light-weight nature of such containers means that the resource foot- print is small making them quick to upload, download, and share. Furthermore, since starting a container largely just changes the references to resources in the environment, containers are user-friendly, start up nearly instantaneously, and run code at speeds nearly identical to the host computer (Felter et al. 2015). Containerization in Practice Containerization has been an increasingly adopted tool for reproducibility widely across the scientific community including areas such as geography, psychology, environmental science, metagenomics and many others (Knoth and N\u00fcst 2017; Wiebels and Moreau 2021; Essawy et al. 2020; Visconti et al. 2018; N\u00fcst et al. 2020; Olaya et al. 2020). To set the stage for a review of containerization technology we will first illustrate how containerization is used in practice. We will present an archetypal example of containerizing and sharing an analysis from three different perspectives: (1) the high- level view of sharing containerized analyses, (2) the end-user experience of interacting with a third-party containerized analysis, and (3) the first-party task of containerizing an analysis for dissemination. Fromthere, theimagemaybedownloadedbyathirdparty and with just a few keystrokes, the third party is placed into a duplicate of the original computing environment (called a \u201ccontainer\u201d1). All of the data, code, dependencies, configurations and software are precisely set up as in the original environment, and thus set up to reproduce the analysis exactly. The goal of containerization is to ensure that if the code worked when containerized, it will work when the image is run by a third party. First, the container is downloaded and started with a single 1An \u201cimage\u201d refers to the actual file that may be uploaded, downloaded or shared, while a \u201ccon- tainer\u201d refers to an ephemeral instance running on the computer.6 Containerization for Reproducible Analysis imageimagecloudcontainer (Copy of Computing Environment) Original Computing Environment imagecontainerization(1) upload(2) download(3)run container(4)Original Analysis Third Party Figure 2: Typical sharing of containerized analysis. (4) From there, the third party may use the image to re-create the original computing environment. We can see from Figure 3 that the container\u2019s environment contains all of our files necessary for analysis including data and code scripts. However, in addition to merely allowing inspection of the data or scripts, the container also comes with an installation ofRso that the user can actually run the code and analysis through the interactive notebook interface. It is important to keep in mind that while the end-user accesses the container and its contents through the web browser on the host computer, the data, code, software installations and back-end to the interface all actually reside in the container. The web browser merely provides a window into the running container through which one may use the tools installed in the container and interact with the code and data it contains. This is the power of sharing containerizing analyses \u2013 it allowsJournal of Data Science, Statistics, and Visualisation 7 (A) T erminalHost Computer Desktop (B) web browser code \ufb01les-p: ports for browser Figure 3: Example of interacting with a containerized analysis. The flag -pallows us to specify the port forwarding to enable interaction through the web browser. (B) The container may now be interacted with through the web browser on the host computer via a graphical interface running from thecontainer. The container has the necessary data and code files and an installation of Rto run the analysis through this web interface. While the end-user may interact naturally with the analysis through a web-browser on the host computer, all of the code, files, and software reside in the container\u2019s pre-configured environment. users to bring to bear the full power and convenience of popular graphical interfaces to fully encapsulated analysis environments with a single command. To set up an image, a configuration file must be written giving instructions of which files and programs to be copied and installed. Such repositories make containerizing analyses simple as one can choose a nearly-complete image, with desired software like Jupyter lab andRalready installed,8 Containerization for Reproducible Analysis and simply add a small amount of project-specific code, data, and documentation. In five lines the configuration specifies a base image with RandJupyteralready installed, installs a desired Radd-on package, copies over data and analysis code, and starts the Jupyter lab interface. On top of this base image one needs only to install the necessary software packages or language add-ons and copy over the data and code. Once the configuration file has been written, the image needs to be built, after which, (B) Building(A)base image with R and jupyter desired name Figure 4: (A) Example configuration file for building an image using Docker. First argument to COPYis location on host, second argument is desired location in container, the flag \u2013chownsets the ownership of the file to the container\u2019s user. The Containerization Landscape While virtualization can trace its roots all the way back to early mainframe computers, modern lightweight containerization was largely popularized with the software Docker starting in 2013 (Graziano 2011; Docker Inc. 2021b). While other tools have been developed since then, the present space of user-friendly containerization software for statisticians and scientists has two major players: (1) Docker (Docker Inc. 2021b), and (2) Singularity (Sylabs 2021). Consequently, they both work on Linux.Journal of Data Science, Statistics, and Visualisation 9 However, Singularity does not have native support on Windows or MacOS while Docker has both support and a graphical interface for these systems. As containerization is fundamentally a refinement of older existing FOSS virtualization technology (itself built upon the FOSS Linux kernel) the core software defining Docker, Singularity, and Podman are publicly available under copyleft/permissive licenses. This leaves open the question of where to store images for the purposes of reproducibility. Other researchers will simply need to locate the files using the DOI and download/run the images. For containerizing shareable and reproducible analyses we recommend Podman or Docker as they are widely used con- tainerization software with cross-platform support, a user-friendly interface, and a huge ecosystem of base images off of which one may build. While all of the containerization tools we discuss in this section can help provide a10 Containerization for Reproducible Analysis Table 1: Comparison of Docker, Singularity, and Podman for containerization of repro- ducible analyses. Otherwise, the added effort of interacting with analyses through a container has the potential to hinder the accessibility of the anal- ysis and code. In Section 3 we discuss notebooks and how they can be used to help provide an accessible and intuitive graphical interface to writing and interacting with containerized analyses. In the remainder of this section we will describe notebooks, highlight some of their advantages, and review popular options. While there are several variants of notebooks, they all structure analysis as a sequence of \u201cchunks\u201d that can be edited and evaluated one at a time. In the software development community there is a long history of discussion of best coding practices and development of tools aimed at promoting them. Notebooks build upon this history, promoting chunked code organization and rich com- mentary, but go a step further and embed analysis output along the code and com- mentary. Consequently, notebooks not only encourage good coding practices, but also facilitate a rich discussion of the code, its relation to the output, and the bearing of this outputJournal of Data Science, Statistics, and Visualisation 11 (A) RStudio Server (B) Jupyter Lab (C) Zeppelin  (1)  text   (2)  code  (3)  plots(1)  text   (2)  code   (3)  plots(1)  text   (2)  code   (3)  plots Figure 5: Interactive notebook environments run through the web browser using (A) RStudio Server , (B) Jupyter Lab , and (C) Zeppelin. While different notebook formats and software tools exist, all notebooks share the feature of organizing analysis as a sequence of chunks of (1) text or (2) code and its associated (3) output. Indeed, embedding output directly alongside the code allows one to document the entire analysis pipeline including expository plots such as diagnostic and exploratory plots that may inform small decisions made in the course of analysis. These types of plotsareoftennotincludedinmanuscriptsorsupplementarymaterialsbecausetheyare difficult to motivate and connect to the analysis when divorced from the actual code. Nonetheless, documentation of these types of micro-decisions is important for properly documenting an analysis pipeline and is necessary for transparent and reproducible research (National Academies of Sciences and Medicine 2019). This can be useful in a research context where both code and output evolve over time and it is easy to mismatch ver- sions of results/figures to the correct versions of the underlying analysis. Both of these practices have the potential to make confusing notebooks where re-running the code sequentially does not reproduce the immortalized output (or may even produce errors). Similarly, Jupyter lab maintains a numeric label for each code chunk to indicate the order in which the chunks have been run. Nonetheless, we still recommend that before sharing notebooks they are re-run sequentially to ensure they work as intended12 Containerization for Reproducible Analysis and, in particular, containerizing the analysis provides a good opportunity to do this. Another advantage of using notebooks for explaining and showcasing analysis is that they give users the option to run the code and explore it interactively. Since chunks can be edited and run one at a time, each chunk provides a natural entry-point into a small portion of the analysis. For example, one can pick a segment of the analysis they wish to explore, edit the chunk of code, run it, and observe the subsequent change in output. This encourages one to experiment with small changes to code, e.g., testing different tuning parameters or optional arguments to functions, and immediately observe the changes to local output. This can be used to provide a natural way to play with code in order to build up an understanding of how the code works and test the robustness of the analysis to alterations. By default, a containerized analysis requires the user to interact with the code entirely through the command line. However, if we containerize notebook software in addition to the code, data, and other dependencies, then we can bring the full power of popular coding environments as an interactive interface to our containerized analysis. In this case, we can run the notebook back-end from within the container but access the interactive computing interface from the host computer\u2019s browser. This combination is the best of both worlds as it brings the native feel of doing analysis on one\u2019s own computer to completely self-contained and reproducible analyses. A summary of this comparison is presented in Table 2 and examples of the software interfaces are illustrated in Figure 5. While Jupyterrequires that all code chunks in a notebook use the same language, both RStudioandZeppelin allow notebooks to mix and match languages across chunks. Furthermore, RStudiohas extensive support forreticulate , which allows analysis using both RandPythonat the same time in a shared computing environment.Journal of Data Science, Statistics, and Visualisation 13 An important distinction is the format of the notebook file and how it interacts with third-party software. This is useful for showcasing results because, unlike a traditional code scripts, the notebook has output embedded and one need not re-run the code to view the results. In particular, one cannot easily track changes to these notebooks in a human-readable format using version control software like Git since small changes to output can prompt a cascading change to hundreds of lines of the dense encoding. An advantage of such an approach is that the input code and commentary are saved in a human-readable format which is more versatile for edit- ing by general software and can be meaningfully tracked by version control schemes. The shareable HTML rendering also contains an embedded downloadable copy of the R Markdown file if one wishes to download the underpinning R Markdown code. Despite these format differences, from the viewpoint of interacting and exploring analyses all three of RStudio,JupyterandZeppelin have broadly similar behavior, and allow users to edit and run code chunks one at time, viewing output in-line in the editor. To allow conversion between formats, Jupyterhas the Jupytext plugin which allows one to conduct analysis using Jupyter, whilst maintaining a simultaneous synchronized version in R markdown or as a simple executable script. A common challenge when using notebooks is that chunks need to be run sequentially and so to explore chunks later in the analysis one needs to run earlier time-intensive14 Containerization for Reproducible Analysis code. Here, we conduct analysis with Jupyterand then use Jupytext to mirror the analysis into R Markdown , a code script, and a HTML rendering for showcasing. Once running, the container is accessible through the host computer\u2019s web browser where a start-page offers several options to interact with the analysis including browsing the files (e.g., to view the HTML rendering) or opening the notebooks in a graphical interface like JupyterorRStudio. (B) We build the image and name it adding the tag :2to indicate it is version 2 of our previous example. Subsequently, we may run the image interactively with-it, naming it with \u2013nameand correctly mapping ports with -p. (C) The start page for the interactive container. (D) We may browse the files or (E) open the notebooks with one of several choices of graphical web-based interfaces running from the container. Containerized notebooks may also be used as a tool for teaching allowing distribution of identical code, data, and a computing environments to all students. While a small amount of time would need to be devoted to teaching students some simple mechanics of containerization, in our estimation this is not more complicated than other coding tasks required in many courses and would provide an opportunity for a discussion with students about research reproducibility, replicability as well as good coding practices. Beyond the direct benefits of making code more easily shareable, the act of container- izing analyses can itself serve as a helpful review step in a scientific pipeline. This encourages simpli-16 Containerization for Reproducible Analysis fication and refactoring of code, as well as writing of the associated documentation and commentary. It also provides an opportunity to re-run the analysis in a hands-off manner to ensure that the notebooks and the entire code pipeline actually correctly produce the results when run sequentially. If the final results included in a manuscript are the output of a container then one can be ensured the results are computation- ally reproducible. N\u00fcst, D., Eddelbuettel, D., Bennett, D., Cannoodt, R., Clark, D., Daroczi, G., Ed- mondson, M., Fay, C., Hughes, E., Kjeldgaard, L., Lopp, S., Marwick, B., Nolis, H., Nolis, J., Ooi, H., Ram, K., Ross, N., Shepherd, L., S\u00f3lymos, P., Swetnam, T. L., Turaga, N., Van Petegem, C., Williams, J., Willis, C., and Xiao, N. (2020).18 Containerization for Reproducible Analysis The Rockerverse: Packages and Applications for Containerization with R. R Jour- nal, 12(1):437\u2013461, DOI: 10.32614/rj-2020-007 ,https://arxiv.org/abs/2001. Hunt Department of Mathematics William & Mary Williamsburg, VA 23185 E-mail:ghunt@wm.edu Journal of Data Science, Statistics, and Visualisation https://jdssv.org/ published by the International Association for Statistical Computing http://iasc-isi.org/ ISSN 2773-0689 March 2023, Volume III, Issue I Submitted: 2021-08-04 doi:10.52933/jdssv.v3i1.53 Accepted: 2022-06-09"
    },
    {
      "file": "C:\\temp\\Units\\UNIT_0 - Pre-Reading Materials (Kelly Only)\\Unit-6-A_meta-analysis_of_GFR_slope_as_a_surrogate_endpoint_for_kidney_failure.pdf",
      "summary": "University of Groningen A meta-analysis of GFR slope as a surrogate endpoint for kidney failure the CKD-EPI Clinical Trials Consortium; Inker, Lesley A.; Collier, Willem; Greene, Tom; Miao, Shiyuan; Chaudhari, Juhi; Appel, Gerald B.; Badve, Sunil V.; Caravaca-Font\u00e1n, Fernando; Del Vecchio, Lucia Published in:  Nature Medicine DOI: 10.1038/s41591-023-02418-0 IMPORTANT NOTE: You are advised to consult the publisher's version (publisher's PDF) if you wish to cite from it. Nature Medicine , 29(7), 1867-1876.  https://doi.org/10.1038/s41591-023-02418-0 Copyright Other than for strictly personal use, it is not permitted to download or to forward/distribute the text or part of it without the consent of the author(s) and/or copyright holder(s), unless the work is under an open content license (like Creative Commons). The publication may also be distributed here under the terms of Article 25fa of the Dutch Copyright Act, indicated by the \u201cTaverne\u201d license. Li\u2009  \u200913, Bart D. Maes\u2009  \u200914,  Brendon L. Neuen4, Ronald D. Perrone1, Giuseppe Remuzzi\u2009  \u200915,  Francesco P. Schena16, Christoph Wanner17, Jack F. M. Wetzels\u2009  \u200918,  Mark Woodward4,19, Hiddo J. L. Heerspink20 & the CKD-EPI Clinical Trials  Consortium* Glomerular filtration rate (GFR) decline is causally associated with kidney  failure and is a candidate surrogate endpoint for clinical trials of chronic kidney disease (CKD) progression. In an analysis of individual participant data, for each of 66 studies (total of 186,312 participants), we estimated treatment effects on the total GFR slope, computed from baseline to 3\u2009years, and chronic slope, starting at 3\u2009months after randomization, and on the clinical endpoint (doubling of serum creatinine, GFR\u2009<\u200915\u2009ml\u2009min \u22121 per 1.73\u2009m2 or kidney  failure with replacement therapy). We used a Bayesian mixed-effects meta-regression model to relate treatment effects on GFR slope with those on the clinical endpoint across all studies and by disease groups (diabetes, glomerular diseases, CKD or cardiovascular diseases). Treatment effects on the clinical endpoint were strongly associated with treatment effects on total slope (median coefficient of determination (R 2)\u2009=\u20090.97 (95% Bayesian  credible interval (BCI) 0.82\u20131.00)) and moderately associated with those on chronic slope (R 2\u2009=\u20090.55 (95% BCI 0.25\u20130.77)). A critical challenge in the evalu - ation of therapies for CKD is that randomized controlled trials (RCTs)  traditionally use kidney failure and doubling of serum creatinine as  clinical endpoints, which are often late events in the progression of  CKD2. There is also robust epidemiological evidence that both a  single measurement of GFR and GFR decline over 1\u20133-year periods are Received: 31 October 2022 Accepted: 24 May 2023 Published online: 17 June 2023  Check for updates A full list of affiliations appears at the end of the paper. e-mail: lesley.inker@tuftsmedicine.orgNature Medicine | Volume 29 | July 2023 | 1867\u20131876 1868 Analysis https://doi.org/10.1038/s41591-023-02418-0studies in diabetes, CKD and GN, but not for CVD, there was an average  benefit of the active treatments on the 3-year total slope and on the  chronic slope compared to the control arms under a random-effects  meta-analysis (Fig. Across the 66 studies, the median follow-up for the clinical end- point was 35\u2009months (25th to 75th percentile, 22\u201352\u2009months), with  shorter follow-up for studies with CVD compared to the three other  disease groups (Extended Data Table 3). Over the full study duration,  a total of 11,396 (6.1%) patients reached the primary clinical endpoint,  defined as the composite of kidney failure with replacement therapy  (KFRT) (initiation of chronic treatment with dialysis or kidney trans - plantation); sustained GFR\u2009<\u200915\u2009ml\u2009min\u22121 per 1.73\u2009m2; or doubling of  serum creatinine (equivalent to 57% decline in GFR). Across all of the stud- ies, as well as for studies in diabetes, CKD and GN, but not for CVD,  an average benefit of treatment was observed (Fig. We also evaluated the treatment effect on the second - ary clinical endpoint, defined as the composite of KFRT or sustained GFR\u2009<\u200915\u2009ml\u2009min \u22121 per 1.73\u2009m2. In the overall set of studies, as well as for  studies in diabetes, CKD, GN and CVD, an average benefit of treatment  was observed (Extended Data Table 3). Trial-level analysis in the overall study population We used Bayesian mixed-effects meta-regression analyses to relate the  treatment effects on the clinical endpoint to the treatment effects on the total and chronic GFR slopes across the 66 studies 41. We observed  a strong agreement between the treatment effects on the 3-year total  slope and on the clinical endpoint, with a coefficient of determination  (R2) of 0.97 (95% Bayesian credible interval (BCI) 0.82\u20131.00) (Fig. The slope of the meta-regression line was \u22120.35\u2009ml\u2009min\u22121 per  1.73\u2009m2 per year (95% BCI \u22120.42 to \u22120.29), indicating that a 0.75\u2009ml\u2009min\u22121  per 1.73\u2009m2 per year greater beneficial effect of the treatment on the  total GFR slope is associated with an average 23.3% lower HR for the clin - ical endpoint (95% BCI 19.3\u201327.2%). The intercept of the meta-regression  was \u22120.04 (95% BCI \u22120.09 to 0.01), indicating that, in the absence of a  treatment effect on the 3-year total slope, the average treatment effect  on the clinical endpoint is likely to be small (that is, 95% probability for  the HR to be between 0.91 and 1.01). The treatment effect on the  2-year total slope similarly showed a strong association with the treat - ment effect on the clinical endpoint (R2 of 0.87 (95% BCI 0.64\u20130.97))  (Extended Data Fig. However, the inter- cept was smaller than 0 (\u22120.11 (95% BCI \u22120.17 to \u22120.06)), indicating that  a small average benefit on the clinical endpoint can be expected even in the absence of a treatment effect on the 2-year total slope. The slope of the meta-regression line dif- fered substantially from 0, at \u22120.33 (95% BCI \u22120.46 to \u22120.20), indicating  that a 0.75\u2009ml\u2009min\u22121 per 1.73\u2009m2 per year greater beneficial treatment  effect on the total GFR slope is associated with an average 21.8% lower  hazard for the clinical endpoint (95% BCI 14.1\u201329.4%). The intercept of  the regression line is nearly indistinguishable from 0 (\u22120.01, 95% BCI  \u22120.10 to 0.10), indicating low risk of a false-negative conclusion of the  absence of a treatment effect on the clinical endpoint when there is no  treatment effect on chronic slope (that is, 95% probability for the HR to be between 0.90 and 1.10). Trial-level analyses for the secondary clinical endpoint We evaluated associations between the treatment effects on GFR  slope with those on the secondary clinical endpoint. For the total  slope, compared to the analysis of the primary clinical endpoint, the  median R2 decreased from 0.97 (0.82, 1.00) to 0.92 (0.56, 0.99). For  the chronic slope, the median R2 increased from 0.55 (0.25, 0.77) to strongly associated with subsequent kidney failure across subgroups of  age, sex and diabetes, hypertension and cardiovascular disease status  failure4\u20137. Using a smaller number of RCTs, we previously demonstrated  that treatment effects on the GFR slope were highly predictive of treat - ment effects on clinical endpoints in hypothetical future RCTs8. On the  basis of this evidence, GFR slope has been used to evaluate efficacy of  treatments in less common conditions, such as glomerular diseases9\u201313,  or when treatments with established efficacy and safety in one popu-lation are expanded to another population 14. However, there remains  uncertainty as to the use of GFR slope in more common causes of kidney  diseases and across severity of kidney disease, further limiting evalua - tion of new therapies in many kidney disease settings. RCTs in populations with earlier stages of CKD or populations at  risk for CKD evaluated the efficacy of novel agents for prevention of  adverse outcomes, including mortality, heart failure events and CKD progression 15\u201333. These studies provide the opportunity to evaluate if  the strong associations between treatment effects on GFR slope with  those on the clinical endpoint persist with inclusion of more diverse  interventions and populations. Together, these data have the potential to strengthen the evidence  for the validity of GFR slope as a surrogate endpoint for kidney failure,  providing support for its use as a primary endpoint for trials of CKD  progression across a broad series of settings. Studies of patients with CKD from other causes or cause not speci - fied reported lower mean estimated glomerular filtration rate (eGFR)  than the studies of patients with diabetes, glomerular diseases (glo - merulonephritis (GN)) or cardiovascular disease (CVD), and studies of  patients with CVD reported lower levels of urine albumin-to-creatinine  ratio (ACR) and slower progression than the other disease groups (Table 1   and Supplementary Table 4). Compared to the previous set of stud - ies, the current studies include populations with higher mean levels  of GFR and populations at high risk for CVD (Extended Data Table 1). Treatment effects on the GFR slope and the clinical endpoint Patterns of change in GFR (GFR slope) after initiation of an interven - tion are often nonlinear, with possibly differing direction and rates  of changes in early follow-up (herein called acute slope) versus  longer-term follow-up (herein called chronic slope)39. The average  rate of decline from the beginning to the end of the study incorporates  both elements (herein called total slope). We used a shared-parameter  mixed-effects model to estimate the effects of the treatment on the  total GFR slope at 3\u2009years (herein called 3-year total slope), as this was the approximate mean length of the included studies and on chronic slope, computed from 3\u2009months after randomization 8,40. Across the full collection of studies and for Nature Medicine | Volume 29 | July 2023 | 1867\u20131876  1869 Analysis https://doi.org/10.1038/s41591-023-02418-00.73 (0.12, 0.98), although BCIs for the secondary clinical endpoint  were wider than for the clinical endpoint, indicating reduced precision  (Extended Data Table 5 and Extended Data Fig. The median inter- cept of the meta-regressions for both chronic slope and total slope  were now negative and had credible intervals that did not overlap 0,  indicating that, on average, a modest benefit on the clinical endpoint  may be present in the absence of a treatment effect on GFR slope. Trial-level analyses by subgroup For the total slope, results were consistent according to CKD severity  and for the subset of participants with baseline levels of ACR greater  than 30\u2009mg\u2009g\u22121 (3.39\u2009mg\u2009mmol\u22121) (Table 2). For chronic slope, there was a small increase in the  median R2 after restricting the patients in the set of studies with ACR  available, although the BCI remained quite wide (0.65 (95% BCI 0.30,  0.88)). There was a steeper relationship between treatment effects on  the chronic slope and those of the clinical endpoint for studies with  higher baseline GFR compared to studies with lower baseline GFR. For both  the 3-year total slope and the chronic slope, for each disease group the  BCIs for the meta-regression slopes do not cross 0, and the BCIs for the  meta-regression intercepts do cross 0 (Table 2  and Fig. For total  slope, there was no evidence of heterogeneity in the meta-regression  slope, intercept or R2 across the disease subgroups. For chronic slope, Table 1 | Clinical characteristics of the population stratified by disease etiology Group Number of studies  (participants)AgeaFemaleaDiabetes GFR ACR Rate of progression Interventions evaluated All studies 66 (186,312) 63.4 (10.5) 58,632 (35) 126,439 (68) 68.0 (109.5) 68 (2,450) \u22123.17 (\u22123.61, \u22122.72) Disease groups CKD 28 (20,149) 57.5 (13.7) 7,621 (38) 4,856 (24) 39.1 (20.5) 235 (6) \u22123.405 (\u22123.90, \u22122.90) RASB v Control, RASB v CCB,   SGLT-2 I, Allopurinol,  Low v Usual Diet,  Nurse-Coordinated Care,  Albuminuria-Targeted  Protocol, Statin +  Ezetimibe, Low v Usual BP Diabetes 21 (101,005) 63.9 (8.7) 35,659 (35) 101,005 (100) 69.3 (88.7) 71 (517) \u22123.37 (\u22124.29, \u22122.44) RASB v Control,   RASB v CCB, SGLT-2 I, DPP-4 I, GLP-1 Ag, MRA,  ERA, Intensive Glucose,  Antiplatelet, Low v Usual BP Glomerular 10 (1,527) 38.6 (14.3) 562 (37) 5 (1) 78.7 (40.5) 1,471 (2) \u22123.51 (\u22124.82, \u22122.20) RASB v Control, Immunosuppression CVD 7 (63,631) 67.1 (9.5) 15,052 (33) 20,573 (32) 74.9 (39.2) 16 (18) \u22121.19 (\u22121.61, \u22120.77) RASB v Control, RASB +  CCB, MRA, Antiplatelet,  Low v Usual BP Rate of progression is indicated by chronic slope in the control arm. The circles represent the meta-analyzed treatment effects, and the horizontal lines represent the associated 95% confidence intervals. CKD refers to diseases other than diabetes or glomerular disease or cause not yet specified.Nature Medicine | Volume 29 | July 2023 | 1867\u20131876 1870 Analysis https://doi.org/10.1038/s41591-023-02418-0there is a suggestion of modest heterogeneity among the posterior  distribution of the meta-regression slopes (range for median slope  from \u22120.24 for diabetes to \u22120.48 for CVD) and intercepts (range for  median intercept from \u22120.12 for GN to 0.04 for CVD) across the dis - ease groups but with widely overlapping BCI. The estimated R2 for  the CVD group is 0.47 (95% BCI 0.01\u20130.99), lower than for the other  diseases, which ranged from 0.71 (95% BCI 0.23\u20130.99) to 0.96 (95% BCI  0.23\u20131.00). Assessment of model adequacy and outliers We evaluated model adequacy and identified potential outliers by  comparing, for each study, the observed HRs for the clinical end - point to the posterior predictive distribution (PPD) for the esti - mated treatment effect on the clinical endpoint computed under  a meta-regression model fit with that study left out. For the 3-year  total slope, nine (13.6%) of 66 studies had observed HRs that fell out - side of the middle 90% intervals of the PPD; for the chronic slope,  10 (15%) of the observed HRs fell outside of the 90% intervals (Sup - plementary Table 8). Under a perfect model, it would be expected that  approximately seven studies would be outside of this range due to  chance variation; the small number of additional studies outside of this range is consistent with adequacy of the model. Prediction intervals and positive predictive value For application of these results to the use of slope as a surrogate end- point in a future trial, we applied the above meta-regression results to  compute Bayesian prediction intervals (BPIs) that provide a 95% prob - ability of including the true treatment effect on the clinical endpoint,  for varying estimated treatment effects on the slope endpoints for  hypothetical large (total sample size of 1,600), medium (total sample size of 800) and small (total sample size of 400) trials.As expected, with greater treatment benefit on GFR slope, the  estimated treatment benefit on the clinical endpoints also increases  (Table 3 and Supplementary Table 11). For example, at a treatment effect  on total GFR slope of 0.5\u2009ml\u2009min\u22121 per 1.73\u2009m2, the estimated HR on the  clinical endpoint is 0.80, whereas, for a treatment effect on GFR slope  of 1\u2009ml\u2009min\u22121 per 1.73\u2009m2, it is 0.68. The BPIs are considerably wider for the chronic slope than for the 3-year total  slope. For example, in a large future trial, the 95% BPI for the HR on  the clinical endpoint associated with an estimated treatment effect  of 0.75\u2009ml\u2009min\u22121 per 1.73\u2009m2 per year definitively excludes an HR of 1 or  greater for the 3-year total slope (0.60\u20130.89) but not the chronic slope  (0.51\u20131.18). Similarly, the threshold for a treatment effect on the GFR  slope to assure the posterior probability of a clinical benefit, which  we defined as an HR for the clinical endpoint of less than 1, of greater  than or equal to 97.5% is substantially smaller for the 3-year total slope  than for the chronic slope (0.44\u2009ml\u2009min\u22121 per 1.73\u2009m2 per year versus  1.26\u2009ml\u2009min\u22121 per 1.73\u2009m2 per year). Within the different disease groups, the 95% BPI for the HR on  the clinical endpoint associated with an estimated treatment effect  of 0.75\u2009ml\u2009min\u22121 per 1.73\u2009m2 per year is also wider for the chronic slope  than for the 3-year total slope (Extended Data Table 7). The magnitude  of the difference is smaller for the CKD (chronic slope versus 3-year  total slope (0.54\u20130.96) versus (0.57\u20130.90)) and diabetes ((0.63\u20131.13)  versus (0.59 to 0.93)) disease groups compared to that for the CVD  ((0.30\u20131.52) versus (0.61\u20130.97)) and GN ((0.34\u20131.12) versus (0.52\u20130.95))  disease groups. 2 | Trial-level analyses for the association between treatment effects on  GFR slope and treatment effects on the clinical endpoint. Circles represent separate studies, with the size of the circle proportional to the number of events (KFRT, GFR\u2009<\u200915\u2009ml\u2009min \u22121 per 1.73\u2009m2 or doubling of serum creatinine). The black line is the line of regression through the studies. Trial-level analyses,  as presented here, are widely regarded as the most important criterion  for demonstrating the validity of a surrogate endpoint and are the  most difficult criterion to establish. We found that with analysis of  GFR slope using a robust method for each study, treatment effects on  the total slope computed at 3\u2009years accounted for an estimated 97%  of the variation between studies in treatment effects on the clinical  endpoint, with similar results across CKD severity and diverse disease  groups. The strength of this trial-level association falls in the range for  a strong surrogate based on one proposed classification (for example,  R2 greater than 0.72) and compares favorably with, and is arguably  stronger than, widely used surrogate endpoints in other fields, such as  progression-free survival for metastatic breast cancer or low-density  lipoprotein cholesterol for major cardiovascular events43, 44. We define  the range of treatment effects on GFR slope that have high confidence  for beneficial treatment effects on the clinical endpoint, providing  guidance on how to interpret treatment effects on GFR slope in future  RCTs. Our results indicate potential concerns with the use of the chronic  slope, which ignores the acute effect of a drug on the GFR slope\u2014an  indication that the acute reduction in GFR due to treatment initiation  might affect the clinical endpoint. Together, these data pro - vide the necessary evidence to support use of total GFR slope in RCTs  evaluating therapies of CKD progression as a valid, fit-for-purpose and  robust surrogate endpoint that can be presented to regulatory agen- cies for approval for marketing authorization, to payors to support funding of these therapies and to healthcare professionals and patients  to inform of their benefit in slowing CKD progression and preventing  kidney failure. The  earlier analyses demonstrated that treatment effects on chronic  slope accounted for a median 96% (95% BCI 63\u2013100%) of the varia - tion between studies in treatment effects on the clinical endpoint,  similar to that of the total slope in those prior analyses (97% (95% BCI  78\u2013100%)) (ref. Because the total slope includes both the acute and  chronic GFR slope, the superior performance of total slope compared  to chronic slope in the current analysis of a more diverse set of studies  suggests that the acute effect might be correlated with a component  of the primary clinical endpoint. Consistent with this hypothesis,  treatment effects on the chronic slope predicted treatment effects  on the secondary clinical endpoint, which does not include doubling  of serum creatinine, more accurately than it predicted treatment  effects on the primary clinical endpoint. We note that, although the  chronic slope exhibited weaker trial-level associations than the 3-year  total slope relative to the primary clinical endpoint, its performance  nonetheless falls within the range of R2 values (0.49\u20130.72) that has  been proposed to represent moderate performance for a surrogate  endpoint43. If there is a suspicion that a positive acute effect may not  indicate a true lasting impact on preservation of the kidney, use of the  total slope may inflate risk of falsely concluding treatment benefit39. Table 2 | Trial-level analysis by subgroups Group Subgroup Number of studies  (number of interventions)Meta-regression slope Intercept R2RMSE Total slope computed over 3\u2009years Overall 66 (17) \u22120.35 (\u22120.42, \u22120.29) \u22120.04 (\u22120.09, 0.01) 0.97 (0.82, 1.00) 0.05 (0.02, 0.12) Disease CKD 28 (9) \u22120.36 (\u22120.52, \u22120.25) \u22120.05 (\u22120.14, 0.04) 0.91 (0.51, 1.00) 0.06 (0.01, 0.14) Diabetes 21 (10) \u22120.32 (\u22120.42, \u22120.21) \u22120.05 (\u22120.12, 0.01) 0.89 (0.49, 1.00) 0.06 (0.01, 0.16) Glomerular 10 (2) \u22120.33 (\u22120.46, \u22120.22) \u22120.06 (\u22120.32, 0.07) 0.99 (0.85, 1.00) 0.06 (0.01, 0.23) CVD 7 (5) \u22120.34 (\u22120.43, \u22120.25) \u22120.02 (\u22120.10, 0.10) 0.98 (0.82, 1.00) 0.05 (0.01, 0.15) GFR <60 40 (14) \u22120.30 (\u22120.44, \u22120.14) \u22120.07 (\u22120.17, 0.01) 0.83 (0.25, 0.99) 0.06 (0.02, 0.13) \u226560 26 (11) \u22120.37 (\u22120.46, \u22120.28) \u22120.02 (\u22120.11, 0.07) 0.98 (0.83, 1.00) 0.06 (0.01, 0.19) Restricted to studies with ACR available55 (14) \u22120.35 (\u22120.45, \u22120.27) \u22120.07 (\u22120.14, \u22120.01) 0.96 (0.75, 1.00) 0.06 (0.02, 0.14) Restricted to studies with ACR available and participants with ACR\u2009>\u200930\u2009mg\u2009g \u2212155 (14) \u22120.34 (\u22120.44, \u22120.23) \u22120.06 (\u22120.13, 0.02) 0.95 (0.69, 1.00) 0.05 (0.02, 0.13) Chronic slope Overall 66 (17) \u22120.33 (\u22120.46, \u22120.20) \u22120.01 (\u22120.10, 0.10) 0.55 (0.25, 0.77) 0.19 (0.13, 0.27) Disease CKD 28 (9) \u22120.33 (\u22120.53, \u22120.15) \u22120.06 (\u22120.19, 0.06) 0.74 (0.18, 0.99) 0.09 (0.01, 0.20) Diabetes 21 (10) \u22120.24 (\u22120.37, \u22120.12) 0.02 (\u22120.09, 0.14) 0.71 (0.23, 0.99) 0.11 (0.02, 0.22) Glomerular 10 (2) \u22120.41 (\u22120.83, \u22120.20) \u22120.12 (\u22120.50, 0.10) 0.96 (0.23, 1.00) 0.12 (0.01, 0.57) CVD 7 (5) \u22120.48 (\u22121.33, \u22120.07) 0.04 (\u22120.14, 0.27) 0.47 (0.01, 0.99) 0.23 (0.05, 0.53) GFR <60 40 (14) \u22120.15 (\u22120.27, \u22120.03) \u22120.12 (\u22120.23, \u22120.03) 0.47 (0.02, 0.94) 0.09 (0.02, 0.18) \u226560 26 (11) \u22120.56 (\u22120.80, \u22120.33) 0.12 (\u22120.03, 0.29) 0.77 (0.35, 0.95) 0.23 (0.11, 0.38) Restricted to studies with ACR  available55 (14) \u22120.30 (\u22120.44, \u22120.18) \u22120.04 (\u22120.14, 0.07) 0.65 (0.30, 0.88) 0.15 (0.08, 0.24) Restricted to studies with ACR available and participants with  ACR\u2009>\u200930\u2009mg\u2009g \u2212155 (14) \u22120.22 (\u22120.35, \u22120.11) \u22120.07 (\u22120.18, 0.03) 0.66 (0.21, 0.94) 0.12 (0.03, 0.21) To convert ACR in mg\u2009g\u22121 to mg\u2009mmol\u22121, multiply by 0.113.Nature Medicine | Volume 29 | July 2023 | 1867\u20131876 1872 Analysis https://doi.org/10.1038/s41591-023-02418-0In such circumstances, the chronic slope may represent the more  conservative endpoint39. This finding may, in part, reflect the average negative direction  of the acute effects across the 66 RCTs included in these analyses. The acute effect has a greater impact on the computation of total  slope when the total slope is computed over shorter time intervals. For future trials, computation of total GFR slope over shorter dura- tions for interventions that do not have a negative acute effect could  be considered.The generalizability for trial-level associations for establishing  the validity of a surrogate is increased when based on a heterogeneous  set of trials, but an often-asked question is whether the overall results  can then be applied to specific subgroups. Similarly, we found that results were consistent in more severe  versus less severe kidney disease as reflected by subgroups by level  of GFR and ACR and supported by a recent publication that looked at these same questions using the more rigorous analysis of examining  interactions with the meta-regression parameters45. In contrast, for  chronic slope, we observed modification of these associations based 0.20.40.611.6 0.20.4Treatment e\ufb00ect on CE (HR)0.611.6 0.20.40.611.6 0.2 \u20132 \u20131 0 1 2 \u20132 \u20131 0 1 2 3 4 3 40.40.611.6 Treatment e\ufb00ect on GFR slope (mean di\ufb00erence, ml min\u20131 per 1.73 m2 per year)3-year total slope Chronic slope CKD trials CKD trials Diabetes trials GN trials GN trials CVD trials CVD trialsDiabetes trialsn study = 28 Intercept = \u20130.05 (\u20130.14, 0.04)Slope = \u20130.36 (\u20130.52, \u20130.25)RMSE = 0.06 (0.01, 0.14) R 2 = 0.91 (0.51, 1.00)n study = 28 Intercept = \u20130.06 (\u20130.19, 0.06)Slope = \u20130.33 (\u20130.53, \u20130.15)RMSE = 0.09 (0.01, 0.20) R 2 = 0.74 (0.18, 0.99) n study = 21 Intercept = 0.02 (\u20130.09, 0.14) Slope = \u20130.24 (\u20130.37, \u20130.12)RMSE = 0.11 (0.02, 0.22) R 2 = 0.71 (0.23, 0.99) n study = 10 Intercept = \u20130.12 (\u20130.50, 0.10) Slope = \u20130.41 (\u20130.83, \u20130.20) RMSE = 0.12 (0.01, 0.57) R2 = 0.96 (0.23, 1.00) n study = 7 Intercept = 0.04 (\u20130.14, 0.27)Slope = \u20130.48 (\u20131.33, \u20130.07)RMSE = 0.23 (0.05, 0.53) R 2 = 0.47 (0.01, 0.99)n study = 21 Intercept = \u20130.05 (\u20130.12, 0.01)Slope = \u20130.32 (\u20130.42, \u20130.21)RMSE = 0.06 (0.01, 0.16) R 2 = 0.89 (0.49, 1.00) n study = 10 Intercept = \u20130.06 (\u20130.32, 0.07)Slope = \u20130.33 (\u20130.46, \u20130.22)RMSE = 0.06 (0.01, 0.23) R 2 = 0.99 (0.85, 1.00) n study = 7 Intercept = \u20130.02 (\u20130.10, 0.10)Slope = \u20130.34 (\u20130.43, \u20130.25)RMSE = 0.05 (0.01, 0.15) R 2 = 0.98 (0.82, 1.00) Fig. 3 | Trial-level analyses for the association between treatment effects on  GFR slope and treatment effects on the clinical endpoint by disease groups. Each circle represents a separate study, with the size of the circle proportional to  the number of events. The observation of weaker associations for chronic slope in the  CVD group might suggest that the less favorable results for the chronic  slope in the current analysis compared to earlier analyses may be  related to inclusion of these studies. The CVD studies were not designed  to evaluate efficacy of these interventions on CKD progression specifi - cally, and the included populations were at lower risk for progression  than those included in the other disease groups, as indicated by higher  GFR and lower ACR. First, these data provide  high confidence in the validity of the 3-year total GFR slope as a surro - gate endpoint for kidney failure, thereby allowing it to be considered  as a primary endpoint for clinical trials evaluating CKD progression  across disease subgroups and CKD severity. The strength of this  evidence is indicated in the lower limit for the 95% credible interval  for the trial-level R2 of 0.82, an increase from 0.78 in our prior results,  now substantially exceeding the threshold of 0.72 proposed for a  strong surrogate43. Although the validity of the 3-year total slope as a sur- rogate endpoint appears quite general, its practical use must consider  the circumstances of each trial. Decisions regarding the trial duration  and the use of the total slope versus chronic slope should be made in  context of the specific trial. For example, we previously demonstrated  that, compared to the clinical endpoint, use of the total slope can  substantially reduce the sample size required for adequate power  for trials without acute effects, particularly when performed in study  populations with high baseline GFR, but may require similar or larger  sample sizes in some situations, particularly for interventions with  negative acute effects46. Second, the consistency in the performance  of the total slope across CKD severity or disease suggests that the  total slope can be used to explore heterogeneous effects among sub - groups within an RCT. Similarly, GFR slope might be an appropriate  endpoint for subsequent studies after initial RCTs showed benefit  on the clinical endpoint, allowing for demonstration of treatment  benefit in different populations9,14.Strengths of this study include a large collection of RCTs with  a diverse range of disease severity, diseases and interventions. Our  application of a robust method for analysis of GFR slope that accounts  for informative censoring and multiple potential sources of variability  in GFR measurements over time allowed us to apply a uniform analysis  of GFR slope across all studies8,40. Because we analyzed individual  patient-level data, we were able to characterize agreement between  the GFR slope and clinical endpoints after adjusting for spurious cor- relations in sampling error that resulted from inclusion of the same  GFR measurements in the GFR slope and clinical endpoints41. First, we evaluated the associa - tion between the treatment effects on GFR slope and on the clinical end - points ascertained over the average follow-up period of approximately  3 years for the trials included in our analysis. Second, our calcula - tions of the prediction intervals and trial-level positive predictive  value (PPV) for a future trial assume that the relationship between the  treatment effects on the slope and clinical endpoints is similar between  the future trial and the previously conducted trials. Studies should evaluate it as  an indicator of treatment effects before widespread use in clinical trials.Table 3 | Application of GFR slope as a surrogate endpoint in a new RCT: predicted treatment effect on clinical endpoint   and PPV GFR slope Observed treatment  effect on change in GFR  slopeLarge RCT Medium RCT Small RCT Median HR and 95% BPIPPV trial Median HR and  95% BPIPPVtrial Median HR and 95% BPIPPV trial Total slope computed at  3\u2009years0.5 0.80 (0.66, 0.98) 0.98 0.80 (0.62, 1.03) 0.96 0.80 (0.58, 1.12) 0.91 0.75 0.74 (0.60, 0.89) 1.00 0.74 (0.57, 0.94) 0.99 0.74 (0.53, 1.02) 0.97 1 0.68 (0.54, 0.82) 1.00 0.68 (0.52, 0.86) 1.00 0.68 (0.48, 0.93) 0.99 Threshold for treatment effect on GFR slope  associated with 97.5% probability of clinical  benefit0.44 0.58 0.79 Chronic slope0.5 0.84 (0.55, 1.28) 0.80 0.84 (0.54, 1.32) 0.78 0.84 (0.51, 1.40) 0.75 0.75 0.78 (0.51, 1.18) 0.89 0.78 (0.49, 1.22) 0.87 0.78 (0.47, 1.29) 0.84 1 0.72 (0.47, 1.09) 0.94 0.72 (0.45, 1.12) 0.93 0.72 (0.43, 1.18) 0.91 Threshold for treatment effect on GFR slope associated with 97.5% probability of clinical  benefit1.26 1.35 1.51 Units of GFR are ml\u2009min\u22121 per 1.73\u2009m2. The PPVtrial should be interpreted in relation to estimated probabilities of clinical benefit of 0.67, 0.63 or 0.60  for large, median and small RCTs, respectively, when there is no estimated treatment effect on the 3-year total slope, or 0.51, 0.51 and 0.52 when there is no estimated treatment effect on the chronic slope.Nature Medicine | Volume 29 | July 2023 | 1867\u20131876 1874 Analysis https://doi.org/10.1038/s41591-023-02418-0In aggregate, the results presented here indicate that GFR slope  can be used as a validated surrogate endpoint for CKD progression. The  selection of total slope versus chronic GFR slope, and the duration of time   over which the total GFR slope is computed, should be made with con- sideration of the study design, population and intervention under inves - tigation and in the context of the specific drug development program. Levey, A. S. et al. GFR decline as an end point for clinical trials in CKD: a scientific workshop sponsored by the National Kidney Foundation and the US Food and Drug Administration. A Study of the Effect and Safety of Sparsentan in the Treatment of Patients With IgA Nephropathy (PROTECT). Levey, A. S. et al. Change in albuminuria and GFR as end   points for clinical trials in early stages of CKD: a scientific workshop sponsored by the National Kidney Foundation in collaboration with the US Food and Drug Administration   and European Medicines Agency. JAMA Intern. Evaluation of variation in the performance of   GFR slope as a surrogate end point for kidney failure in clinical trials that differ by severity of CKD. Springer Nature or its licensor (e.g. a society or other partner) holds  exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the  accepted manuscript version of this article is solely governed by the  terms of such publishing agreement and applicable law. *A list of authors and their affiliations appears at the end   of the paper. Overview Our meta-analytic approach followed seven main steps: (1) search and  identify data from published RCTs using a standardized approach; (2)  obtain agreement for use of individual data and transfer to the Data  Coordinating Center at Tufts Medical Center or identify methods  for cloud-based data access; (3) within each RCT, use intent-to-treat  analyses to estimate (a) the effect of the treatment on GFR slope under  a shared-parameter mixed-effect model and (b) the effect of the treat - ment on clinical endpoint (defined as a composite of kidney failure,  sustained GFR\u2009<\u200915\u2009ml\u2009min\u22121 per 1.73\u2009m2 or doubling of serum creatinine)  under a Cox proportional hazards regression model; (4) quantify the  association between treatment effects on the GFR slope and treatment  effects on clinical endpoints across RCTs using trial-level Bayesian  meta-regression analysis; (5) identify outliers and evaluate model ade- quacy across the individual trials by using a cross-validation approach  to compare the observed treatment effect on the clinical endpoint in  each trial to the PPD for that trial derived from the trial-level model fit to the remaining trials; (6) quantify the consistency in these associa-tions across disease subgroups using Bayesian meta-regression with partial pooling; and (7) assess the utility of using slope as a surrogate  endpoint in a future trial by providing 95% Bayesian prediction intervals  and trial-level PPVs corresponding with a range of possible treatment effects on the slope endpoint in a hypothetical future trial. The analytic approach is based on the causal association frame - work in which the validity of surrogate endpoints is evaluated based on  the relationship between the average causal effect of the treatment on  the surrogate endpoint and the average causal effect of the treatment  on the clinical endpoint across a population of RCTs. This approach  takes advantage of the fact that the average causal effects on the sur-rogate and clinical endpoints can be estimated with little bias within  each RCT by applying intent-to-treat analyses50\u201353. Confidence in the use  of these results for a future new trial is increased when the previously  conducted trials provide evidence that the treatment effect on the  clinical endpoint is consistently predicted from the treatment effect on  the surrogate across a broad collection of RCTs evaluating treatments  that operate through diverse mechanisms and across diverse patient  populations. As a result, the empirical support for a surrogate endpoint  is enhanced when the trial-level meta-regression is conducted over  a heterogeneous collection rather than a homogenous collection of  previously conducted RCTs. In comparison to the prior analyses, the current set of studies includes populations with higher mean levels of GFR and populations at high risk for CVD.For each study, we defined the active treatment as the treatment  hypothesized to produce the greater reduction in the risk of the clini- cal endpoint. Clinical endpoint The clinical endpoint was defined as a composite of KFRT (initiation  of chronic treatment with dialysis or kidney transplantation), a sus- tained GFR\u2009<\u200915\u2009ml\u2009min\u22121 per 1.73\u2009m2 for participants with baseline  GFR\u2009\u2265\u200925\u2009ml\u2009min\u22121 per 1.73\u2009m2 (herein referred to as GFR\u2009<\u200915\u2009ml\u2009min\u22121  per 1.73\u2009m2) or doubling of serum creatinine over the full study dura- tion. The exclusion of doubling of serum creatinine from the  secondary clinical endpoint assures that the remaining components  of the composite endpoint signify the occurrence or near-occurrence  of a kidney failure, considered to be the true clinical event, and reduces  the chance that that endpoint would be affected by large acute effects. Compared to the primary clinical endpoint, the secondary clinical  endpoint poses several challenges for our trial-level analyses, includ-ing: (1) treatment effects are estimated with reduced precision due to  fewer events, reducing statistical power, particularly for subgroup  analyses; (2) trial-level analyses are weighted even more heavily toward  the subgroup of trials with lower levels of baseline GFR, as KFRT and  GFR\u2009<\u200915\u2009ml\u2009min\u22121 per 1.73\u2009m2 are rare events for trials with high baseline  GFR; and (3) the competing risk of death will have a greater impact. For these reasons, the composite of KFRT and GFR\u2009<\u200915\u2009ml\u2009min\u22121 per  1.73\u2009m2 is treated as the secondary clinical endpoint, and the broader  composite, which includes doubling of serum creatinine in addition  to these two events, is designated as the primary clinical endpoint in these analyses. Analyses of the total and chronic GFR slope (surrogate  endpoints) Changes in GFR after intervention are often nonlinear, with possi - bly differing direction and rates of change in early follow-up (herein  called acute slope) and longer-term follow-up (herein called chronic  slope). Differences  between the randomized groups in the mean GFR levels at 3-month  follow-up, the mean slopes from 3\u2009months onward and the estimated  mean changes from baseline to either 2-year or 3-year follow-up fac - tored by the follow-up duration represented the treatment effects on  the acute, chronic and total slopes, respectively. Our primary analyses Nature Medicine Analysis https://doi.org/10.1038/s41591-023-02418-0focused on the total GFR slope computed at 3\u2009years (herein called 3-year  total slope), the approximate mean length of the included studies and  on the chronic slope. A power of the mean (POM) model for the  residual variance is used to allow for greater variation in individual GFR  measurements at higher GFR. The model allows for non-uniform treat - ment effects in which treatments slow progression by a greater extent  among patients with faster GFR decline than for patients with slower  GFR decline by allowing different between-patient slope variances  in the treatment and control groups8,40. In studies in which at least 15  individuals died or reached KFRT, the model accounts for informative  censoring by these events by incorporating the mixed model for the  GFR measurements within a shared-parameter model in which the risk  of KFRT or death was assumed to be related to the random slopes and  intercepts of the GFR part of the model62,63. As described above, we applied the mixed-effects models to estimate the treatment effects on GFR slope  within each study by treatment arm, with treatment effects expressed  as differences in the mean GFR slopes between the treatment versus  control groups in units of ml\u2009min\u22121 per 1.73\u2009m2 per year. We next applied a trial-level Bayesian  mixed-effects meta-regression model to relate the treatment effects  on the clinical endpoint to the treatment effects on GFR slope with  study as the unit of analysis41,50. The model relates the treatment effects  on the two endpoints after accounting for the standard errors in the  estimated effects in each study and the correlation of these errors  with each other. This approach takes advantage of the fact that the  average causal effects on the surrogate and clinical endpoints can be  estimated with little bias within each randomized trial by applying  intent-to-treat analyses51\u201353. The model includes two stages, where the  first stage relates the estimated treatment effects to the true latent  treatment effects within each trial and the second stage describes  the relationships between the true latent treatment effects across the  different trials. For simplicity,  as most trials included a single treatment comparison, we abuse the  notation slightly and write that the index i refers to the ith trial. We let  \u03b8i and \u03b3i denote the true latent treatment effects on the clinical endpoint  and on the GFR slope in the i th trial and use \u0302\u03b8i and \u0302\u03b3i to indicate the  estimated effects obtained as described above. The first stage of   the model relates the estimated and true latent treatment effects in  the ith trial by: [\u0302\u03b8i \u0302\u03b3i]\u223cMVN([\u03b8i \u03b3i],[\u03c32 iri\u03c3i\u03b4i ri\u03c3i\u03b4i\u03b42 i]) Here, \u03c3i is the standard error of the estimated treatment effect on  the clinical endpoint; \u03b4i is the standard error of the estimated treatment effect on GFR slope in the i th trial; and ri is the correlation between these  estimated treatment effects. For two trials that included  two different comparisons of active treatments to control, we also  accounted for the correlations between the two treatment contrasts  with the same control group. The notation MVN() indicates that the  estimated treatment effects are assumed to follow a bivariate normal  distribution given the true treatment effects within each trial; this  assumption is satisfied to a high degree of accuracy due to the central limit theorem. The second stage of the model characterizes the variation in the  true latent treatment effects on GFR slope and on the clinical endpoint  across the trials. This second stage is expressed as [\u03b8i \u03b3i]\u223cN([\u03bc\u03b8 \u03bc\u03b3],[\u03c32 \u03b8R\u03c3\u03b8\u03c3\u03b3 R\u03c3\u03b8\u03c3\u03b3\u03c32 \u03b3]) where \u03bc\u03b8 and \u03bc\u03b3 are, respectively, the means of the true latent treatment  effects on the clinical endpoint and on GFR slope in the population of  trials represented by this meta-regression; \u03c3\u03b8 and \u03c3\u03b3 are the standard  deviations of the true latent treatment effects across the population of trials; and R is the correlation between the true latent treatment effects  on the two endpoints. Based on this two-stage model, the slope and  intercept of the meta-regression line predicting the true latent treatment  effect on the clinical endpoint from the true latent treatment effect on  the surrogate endpoint are given by \u03b2\u2009=\u2009R\u03c3\u03b8/\u03c3\u03b3 and \u03b1\u2009=\u2009\u03bc\u03b8\u2009\u2212\u2009\u03b2\u03bc\u03b3, respec - tively, and the residual standard deviation or RMSE, which defines the  uncertainty in the treatment effect on the clinical endpoint given a par- ticular treatment effect on the surrogate endpoint, is RMSE = \u03c3\u03b8\u00d7(1\u2212R2)1 2. The two-stage model was fit using a Bayesian Monte Carlo Markov  chain sampling, using diffuse prior distributions for the model param - eters that we selected so that the final results would depend primarily  on the data with little influence of the prior distributions. The prior  on the meta-regression slope was uniform with a range of \u22125 to 5. The priors for the mean treatment effects on the clinical endpoint  (expressed as a log HR) and on each GFR slope endpoint (expressed as  difference between treatment arms in ml\u2009min\u22121 per 1.73\u2009m2 per year)  were taken to be normal distributions each with mean 0 and variance  10,000; the priors for the variances of the treatment effects on the  clinical endpoint and on the GFR slope endpoints were each taken to  be inverse gamma distributions with shape parameter 0.261. The scale  parameter was 0.000408 for the clinical endpoint and 0.005 for the  slope endpoints. The prior distribution for the clinical endpoint was  selected by the investigators to assign 1/3 prior probabilities each to  low treatment effect heterogeneity (which we defined as a treatment effect standard deviation on the log-scale \u22640.05), medium treatment  effect heterogeneity (defined as a treatment effect standard deviation  on the log-scale between 0.05 and 0.20) and high treatment effect  heterogeneity (defined as a treatment effect standard deviation on  the log-scale >0.20). For slope, the prior assigns a 1/3 prior probability  to slope standard deviations\u2009\u2264\u20090.175\u2009ml\u2009min\u22121 per 1.73\u2009m2 per year, 1/3  to a slope standard deviation between 0.175\u2009ml\u2009min\u22121 per 1.73\u2009m2 per  year and 0.70\u2009ml\u2009min\u22121 per 1.73\u2009m2 per year and 1/3 to a slope standard  deviation\u2009>\u20090.70\u2009ml\u2009min\u22121 per 1.73\u2009m2 per year. We checked that the  prior distributions had only a small influence on the results by verifying  that the results of each analysis were similar under alternative inverse  gamma (0.001, 0.001) prior distributions for the variances for the  treatment effects on the clinical endpoint and on GFR slope. The meta-regression supports the validity of GFR slope  as a surrogate endpoint if (1) the slope of the meta-regression line has a large magnitude with a BCI that does not cross 0; (2) the intercept is  close to 0 and with a BCI that crosses 0, implying absence of a substan - tial average effect on the clinical endpoint when the treatment does Nature Medicine Analysis https://doi.org/10.1038/s41591-023-02418-0not affect GFR slope; and (3) the R2 of the meta-regression is large,  indicating strong associations (for example, >0.72) (ref. 43 ), so that  treatment effects on GFR slope account for most of the variation in  treatment effects on the clinical endpoint. The meta-regression slope provides the mean difference in the log  HR for the clinical endpoint associated with each 1\u2009ml\u2009min\u22121 per 1.73\u2009m2  per year increment in the treatment effect on the GFR slope endpoint. The meta-regression intercepts provide the estimated mean log HR  for the clinical endpoint when the mean treatment effect on the GFR  slope endpoint is equal to 0. To assist with interpretation, we express  the impact under the meta-regression model of a 0.75\u2009ml\u2009min\u22121 per  1.73\u2009m2 difference in the treatment effect on GFR slope on the HR for  the treatment effect on the clinical endpoint using the formula: %DifferenceinHRforclinicalendpoint =100\u00d7(1\u2212exp(0.75 \u00d7(meta\u2212regressionslope ))) For example, a 0.75\u2009ml\u2009min\u22121 per 1.73\u2009m2 per year greater beneficial  effect of the treatment on the 3-year total GFR slope is associated with  an average 23%\u2009=\u2009100\u2009\u00d7\u2009(1\u2009\u2212\u2009exp(0.75\u2009\u00d7\u2009\u22120.35)) lower HR for the clinical  endpoint. We wanted to evaluate whether the strength of the associations differed for studies with higher versus lower levels of GFR. Urine albumin higher than 30\u2009mg\u2009g\u22121 (3.39\u2009mg\u2009mmol\u22121) is the  threshold for diagnosis of CKD, regardless of the level of GFR. This approach adds a third stage to the basic two-stage mixed-effects meta-regression model described for the analysis of the full cohort of 66 stud-ies described above. In the extended partial-pooling model, the first stage again describes the relationship between the estimated treatment effects and the true latent treatment effects on the slope and clinical endpoints within each trial. The second stage includes separate meta-regressions that describe the relationship between the true latent treatment effects on the slope and clinical endpoints across the studies within each of the designated subgroups of trials. The third stage describes the variation in the meta-regression terms of the second stage between the designated subgroups. When the data suggest a small amount of variation in a particular meta-regression term (such as the meta-regression slope or intercept) between the subgroups, then the subgroup-specific posterior distributions for that term will be more distinct between subgroups due to a lesser degree of information sharing. Alternatively, if the data do not suggest a large degree of heterogeneity in any given term across subgroups, then there will be more information sharing across subgroups, and the posterior distributions for that term will be more similar. For each study, we pro - duced a PPD for the estimated treatment effect on the clinical endpoint  based on the estimated treatment effect on GFR slope in that study and  the meta-regression model fit to the remaining trials with that study  held out. The PPD takes into account uncertainty in the meta-regression  parameter estimates as well as the sampling error of the estimated  treatment effects within the held-out trial. We graphically displayed the  actual observed effect on the clinical endpoint to an interval extending  from the 5th to the 95th percentile of the PPD for each trial, to display  how well the trial-level model predicts the treatment effect in a \u2018new  trial\u2019 based on a model developed from the remaining trials. We defined  potential outliers as trials where the observed effects fell in the top 5%  or the bottom 5% of the PPD, recognizing that, on average, 10% of tri-als would be flagged as outliers by chance under a perfect model. We computed  posterior medians and 95% credible intervals for the meta-regression intercept, slope, RMSE and R 2 to identify whether, after an individual  trial was removed, the interpretation of results meaningfully changed. We obtained 95% prediction  intervals for the treatment effect on the clinical endpoint given a par- ticular value for the true latent treatment effect on GFR slope by simu- lating the posterior distribution of \u03b1  + \u03b2  \u00d7 True.Effslope  + \u03940, where True. Effslope  is the true latent treatment effect on the GFR slope endpoint;   \u03b1 + \u03b2 \u00d7 True.Effslope  represents the predicted mean true latent treatment  effect on the clinical endpoint based on the meta-regression model;  and \u03940 is normally distributed with mean 0 and standard deviation  given by the RMSE from the meta-regression. Here, \u03940 represents the  variation in the true latent treatment effects on the clinical endpoint  across different trials with the same treatment effect on GFR slope. This prediction interval accounts for uncertainty in the estimation  of \u03b1 and \u03b2  and in the RMSE that defines the meta-regression, as well  as uncertainty due to variation in the treatment effects on the clinical endpoint about the regression line for different trials. When the trial-level meta-regression is applied to a newly con - ducted randomized trial, there is an additional source of uncertainty  that results from imprecision in the estimation of the treatment effect  on GFR slope in the new trial. This added uncertainty depends on the  sample size and is smaller when the sample size for the new trial is large. We obtained 95% prediction intervals for the true latent treatment  effect on the clinical endpoint in a new trial that take into account this additional uncertainty by again sampling from the posterior distribu- tion of \u03b1  + \u03b2  \u00d7 True.Effslope  + \u03940, but now assume that True.Effslope  has a  random distribution to reflect the uncertainty in its estimation in the  new trial instead of taking True.Effslope  to be a fixed value. Specifically, we  assumed that the posterior distribution of True.Effslope  is normally dis- tributed with mean equal to the estimated treatment effect on GFR slope  and standard deviation given by the standard error for the estimated  treatment effect on GFR slope based on the sample size. We  performed this calculation for a trial with a between-patient standard  deviation in chronic GFR slopes of 4.0\u2009ml\u2009min\u22121 per 1.73\u2009m2 per year, with  residual GFR variance equal to 0.67 \u00d7 (mean baseline GFR), where the mean baseline GFR is 40\u2009ml\u2009min \u22121 per 1.73\u2009m2. We used a similar sampling approach for the posterior distribution  of \u03b1 + \u03b2  \u00d7 True.Effslope  + \u03940 to estimate the probability that the treatment  effect of the clinical endpoint in the new trial, expressed as a log HR,  would fall below 0 (corresponding to a non-zero treatment benefit  with an HR for the clinical endpoint less than 1) given either the true  or the estimated treatment effects on GFR slope in the new trial. We  refer to the probability of a clinical benefit in the new trial associated  with a particular treatment effect on GFR slope as the trial-level PPV,  which we denoted by PPVtrial. By considering the PPV as a function of  the estimated treatment effect on GFR slope, we determined the size of the smallest treatment effect on GFR slope that would be required to assure a PPV of at least 0.975 for a benefit on the clinical endpoint. Most of the 66 participating studies included sample size calculations that  are described in the protocols of the respective studies. We used a pre - defined protocol to identify studies for inclusion in the analyses of this  report using systematic review of the literature with predefined inclu - sion and exclusion criteria (Study Protocol and Extended Data Fig. Because  our analyses were applied to all available studies that met the entry  criteria, we did not perform sample size calculations for the number of studies to be included in the trial-level analyses. The following datasets can be requested through data-sharing plat - forms: Vivli: CANVAS (NCT01032629 ), CANVAS-R (NCT01989754 ), CRE - DENCE (NCT02065791 ), EMPA-REG Outcome (NCT01131676), EXAMINE  (NCT00968708), Harmony Outcome (NCT02465515 ) and FIDELIO-DKD  (NCT02540993); NIDDK: AASK (NCT04364139 ), FSGS/FONT  (NCT00135811 ), HAL T-PKD Study A and Study B (NCT00283686) and  MDRD (NCT03202914 ); NHLBI BioLINCC: TOPCAT (NCT00094302) and  SPRINT (NCT01206062 ); Clinical Study Data Request: PARADIGM-HF  (NCT01035255 ) and ACCOMPLISH (NCT00170950 ); and sponsors\u2019  website: LEADER (NCT01179048).Code availability The statistical code used for the primary analysis can be found at  https://github.com/UofUEpiBio/ckdepict. The effects of dietary protein restriction and blood-pressure control on the progression of chronic renal disease. Skali, H. et al. Prognostic assessment of estimated glomerular filtration rate by the new Chronic Kidney Disease Epidemiology Collaboration equation in comparison with the Modification of Diet in Renal Disease Study equation. This work also received support from the Utah Study Design and Biostatistics Center, with funding in part from the National Center for Advancing Translational Sciences of the National Institutes of Health under award number UL1TR002538. The support and resources from the Center for High Performance Computing at the University of Utah are also gratefully acknowledged. This analysis includes SPRINT and TOPCAT research materials obtained from the National Heart, Lung and Blood Institute (NHLBI) Biologic Specimen and Data Nature Medicine Analysis https://doi.org/10.1038/s41591-023-02418-0Repository Information Coordinating Center and does not necessarily  reflect the opinions or views of SPRINT or TOPCAT or the NHLBI. The FSGS/FONT trial, HALT-PKD Study A and HALT-PKD Study B were conducted by the investigators of the respective studies and were supported by the National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK). All authors contributed to the interpretation of the   data, provided critical feedback on paper drafts and approved   the final draft. reports funding from the National Health and Medical Research Council of Australia; has served on the advisory boards of Bayer, AstraZeneca and Vifor Pharma; and has received speaker\u2019s honoraria from Bayer, AstraZeneca, Pfizer and Vifor Pharma, all fees paid to the George Institute for Global Health. reports lecture and consulting honoraria from Alynlam, Amgen, AstraZeneca, Bayer, Boehringer Ingelheim, Calliditas, Fresenius, Novartis, Omeros, Travere and Vifor and is a member of the data safety monitoring board in trials for Novo Nordisk and Visterra. reports personal funding from the UK Medical Research Council (MC_UU_00017/3) and Kidney Research UK (MR/R007764/1) and grants to the University of Oxford from Boehringer Ingelheim and Eli Lilly. is the National Leader of the ASCEND-D and ASCEND-ND trials of GlaxoSmithKline and the PROTECT and DUPLEX trials of Travere and is a steering committee member for the Novartis APPLAUSE trial and the Vera Therapeutics ATACICEPT trial. received research support from Otsuka, Reata, Kadmon, Sanofi Genzyme and the US Department of Defense (TAME PKD); has been a member of steering committees for Sanofi Genzyme, Otsuka and TAME PKD, with fees paid to employing institutions; and has provided consultancy for Navitor, Palladiobio, Reata and Otsuka. 2 | Trial-level analyses for the association between  treatment effects on GFR slope and treatment effects on the clinical endpoint. The black line is the line of regression through the studies. 3 | Trial-level analyses for the association between  treatment effects on GFR slope and treatment effects on the secondary clinical endpoint. The black line is the line of regression through the studies. Red triangles indicate studies where the estimated treatment effects are beyond the margins.Nature Medicine Analysis https://doi.org/10.1038/s41591-023-02418-0Extended Data Table 1 | Categories of disease by interventionNature Medicine Analysis https://doi.org/10.1038/s41591-023-02418-0Extended Data Table 2 | Mean slopes in treatment and control and treatment effect by intervention, causal disease and  subgroups\u2014total slope computed at 3\u2009years, total slope computed at 2\u2009years, chronic slope and acute slopeNature Medicine Analysis https://doi.org/10.1038/s41591-023-02418-0Extended Data Table 3 | Treatment effects on the clinical endpointNature Medicine Analysis https://doi.org/10.1038/s41591-023-02418-0Extended Data Table 4 | Trial-level analyses for the association between treatment effects on GFR slope over 2\u2009years and  treatment effects on the clinical endpoint by subgroupsNature Medicine Analysis https://doi.org/10.1038/s41591-023-02418-0Extended Data Table 5 | Trial-level results using secondary endpoint (that is, dialysis or GFR\u2009<\u200915 ml\u2009min\u22121 per 1.73\u2009m2)Nature Medicine Analysis https://doi.org/10.1038/s41591-023-02418-0Extended Data Table 6 | Trial-level analyses for the association between treatment effects on GFR slope and treatment  effects on the clinical endpoint by sensitivity excluding disease groupsNature Medicine Analysis https://doi.org/10.1038/s41591-023-02418-0Extended Data Table 7 | Application of GFR slope as surrogate endpoint in a new RCT: predicted treatment effect on clinical  endpoint and PPV for subgroup-specific analyses"
    },
    {
      "file": "C:\\temp\\Units\\UNIT_0 - Pre-Reading Materials (Kelly Only)\\Unit-6-gfr_in_healthy_aging__an_individual_participant.24.pdf",
      "summary": "CLINICAL EPIDEMIOLOGY www.jasn.org GFR in Healthy Aging: an Individual Participant Data Meta-Analysis of Iohexol Clearance in European Population-Based Cohorts Bj\u00f8rn O. Eriksen ,1,2Runolfur Palsson ,3,4Natalie Ebert,5Toralf Melsom,1,2 Markus van der Giet,6Vilmundur Gudnason ,4,7Olafur S. Indridason ,3Lesley A. Inker,8 Trond G. Jenssen,1,9Andrew S. Levey,8Marit D. Solbu,1,2Hocine Tighiouart,10,11and Elke Schaeffner5 Due to the number of contributing authors, the af \ufb01liations are listed at the end of this article. Methods We investigated the cross-sectional association between measured GFR, age, and health in persons aged 50 \u201397 years in the general population through a meta-analysis of iohexol clearance mea- surements in three large European population-based cohorts. The mean GFR was lower in older age by 20.72 ml/min per 1.73 m 2per year (95% con\ufb01dence interval [95% CI], 20.96 to 20.48) for men who were healthy versus 21.03 ml/min per 1.73 m2 per year (95% CI, 21.25 to 20.80) for men who were unhealthy, and by 20.92 ml/min per 1.73 m2per year (95% CI, 21.14 to 20.70) for women who were healthy versus 21.22 ml/min per 1.73 m2per year (95% CI, 21.43 to 21.02) for women who were unhealthy. For healthy and unhealthy people of both sexes, both the 97.5th and 2.5th GFR percentiles exhibited a negative linear association with age. However, both the mean and 97.5 percentiles of the GFR distribution are lower in older persons whoare healthy than in middle-aged persons who are healthy. JASN 31: 1602 \u20131615, 2020. doi: https://doi.org/10.1681/ASN.2020020151 The Global Burden of Disease study found that aging was responsible for 43% of the increasedloss of disability-adjusted life-years caused byCKD between 1990 and 2016. Email: bjorn.od var.eriksen@unn.no Copyright \u00a9 2020 by the American Society of Nephrology 1602 ISSN : 1046-6673/3107-1602 JASN 31:1602 \u20131615, 2020 Downloaded from http://journals.lww.com/jasn by BhDMf5ePHKav1zEoum1tQfN4a+kJLhEZgbsIHo4XMi0hCywCX1AW nYQp/IlQrHD3i3D0OdRyi7TvSFl4Cf3VC1y0abggQZXdtwnfKZBYtws= on 04/25/2025or improved rather than lowered GFR in a signi \ufb01cant pro- portion of aging persons.5Although this suggests that good health may prevent age-related GFR decline, studiesof kidney biopsy specimens from living kidney donors dem-onstrate that a reduction in nephron number occurs from ayoung age even in the absence of disease. 6 Our current knowledge about aging and GFR in the general population mainly comes from cross-sectional studies per- formed decades ago, which have been summarized in a de-tailed review by Delanaye et al. 7Few, if any, of these studies were population based, and the number of participants olderthan 65 years was very small. 7Because the prevalence of chronic disease is higher at older age, we have little knowledgeabout the effects of natural aging versus disease on GFR in thelast decades of life. Although there are some large population- based studies based on GFR estimated from serum creati- nine, 8\u201315this approach is problematic in older people because of confounding by sarcopenia and other non-GFR \u2013related factors.16\u201320 We performed a meta-analysis of individual participant data from four population-based studies in three Europeancohorts where GFR had been measured using plasma iohexolclearance in persons aged between 50 and 97 years. Our aims were to study the association of GFR with age in healthy people and predict reference intervals for GFR in healthy aging acrosst h es t u d i e da g er a n g e .B e c a u s ei tw o u l dn o tb ep o s s i b l et operform a population-based study with a high number of trulyhealthy individuals in the oldest age groups, we designed thestudy to use a generalized additive regression model to adjustfor the association of comorbidity and risk factors with GFRand to predict the distribution of GFR in hypothetically healthy persons. Three eligiblecohorts were identi \ufb01ed: the Renal Iohexol Clearance Survey (RENIS) in Troms\u00f8 6 (RENIS-T6; n51632); 21the Berlin Ini- tiative Study (BIS; n5610);22and the Age, Gene/Environment Susceptibility \u2013Kidney Study (AGES-Kidney; n5819) (Figure 1).23 The RENIS cohort included a repeated GFR measurement in the RENIS follow-up study (RENIS-FU) after a mean follow- up of 5.6 years ( n51329).24The examinations in RENIS-T6 and RENIS-FU were both included in this investigation. The RENIS cohort included participants between 50 and 63 years of age at baseline from the sixth wave of a series ofpopulation surveys in the municipality of Troms\u00f8 in northernNorway. AGES-Kidney is a substudy of the AGES-II-Reykjavik Study, which was a follow-up of the population-based Reykjavik Study in Iceland.23T h er e s p o n s er a t ei nt h eA G E S - I I - R e y k j a v i kS t u d yw a s 71%,26and for those eligible for AGES-Kidney the response rate was 65%.23 The inclusion criteria for the three cohorts were similar, except AGES-Kidney excluded individuals receiving active cancer treatment, and BIS excluded persons that requirednursing care during daytime and nighttime. The people invited to AGES-Kidney and RENIS comprised random samples of the general population, and those invited to BIS comprised a random sample from the Allgemeine Ortskrankenkasse Nordost , which provides insurance cover- age to almost 50% of people older than 70 years in Berlin.22 Although participation was voluntary, the three cohorts ofexamined persons were all representative of their source pop-ulations. 27The mean eGFR, calculated using the Modi- \ufb01cation of Diet in Renal Disease equation, was 89.3 versus 90.6 ml/min per 1.73 m2for women, and 93.1 versus 93.2 ml/min per 1.73 m2for men in RENIS and the Troms\u00f8 Study, respectively. The prevalence of the most important chronic diseases in BIS was similar to that of all persons older than 70 years in the Allgemeine Ortskrankenkasse Nordost .28Also, the prevalence of diabetes,29myocardial infarction,30angina pectoris,30 stroke,31and cancer32were of a similar order of magnitude as in other German studies of chronic diseases in older adults. The participants in AGES-Kidney were younger, had lower systolic BP, and were less likely to be current smokers or have cardiovascular disease or diabetes than participants in AGES-II-Reykjavik who were not included.23The meanSigni \ufb01cance Statement In populations, mean GFR is lower in older age, but whether healthy aging is associated with preserved rather than lower GFR in some individuals is unknown. In a meta-analysis of three large European- based cohorts, the authors investigated the cross-sectional asso-ciation of being healthy (de \ufb01ned as having no major chronic disease or risk factors for CKD), age, and iohexol clearance measurements. The mean and the 97.5th percentile of the GFR distribution were higher in older persons who were healthy than in those who wereunhealthy, but lower than in middle-aged people who were healthy.The GFR-age association was more negative in women than in men. 21\u201324 Technically unsuccessful measurements were excluded from the investigations (RENIS-T6, n55; RENIS-FU, n55; AGES-Kidney, n514; BIS, n540; Figure 1).21,23,24,33 This study was approved by the ethical review boards of the three respective investigations, and the study was approved bythe Icelandic National Bioethics Committee (VSN 00-063). The useof individual classes of antihypertensive medications was reg-istered in the following dichotomous variables: b-blockers,AGES-Kidney, investigated (n=819)BIS,  investigated (n=610)RENIS-FU,  investigated (n=1329) AGES-Kidney (n=805) Missing data  (n=23)BIS (n=570) RENIS-FU  (n=1324) Pooled study population, 4209  observations in 2885 persons Technically unsuccessful GFR- measurements (n=40) Missing data  (n=89)Technically unsuccessful GFR- measurements (n=5)GFR- measurements with incomplete data (n=14)RENIS-T6,  investigated (n=1632) RENIS-T6  (n=1627) Missing data  (n=5)Technically unsuccessful GFR- measurements (n=5) BIS,  complete data (n=547)AGES-Kidney,  complete data (n=716)RENIS-FU,  complete data  (n=1324)RENIS-T6,  complete data  (n=1622) Figure 1. The dashed arrow from RENIS-T6 to RENIS-FU indicates the repeated measurements of GFR in the RENIS cohort after a mean follow-up of 5.6 years. Details about urinary creatinine and albumin measure- ments in all three cohorts have been given previously.23,28,34 The urinary albumin-creatinine ratio (ACR) was classi \ufb01ed in the categories ,10,$10 and,30, and $30 mg/g, corre- sponding to the categories optimal, high normal, andhigh/very high/nephrotic. 35 In RENIS and AGES-Kidney, BP was measured as described previously.23,36 BP in BIS was measured according to the European Society of Cardiology/European Society of Hyper-tension (ESC/ESH) recommendations. 38The iohexol assays of the three studies were calibrated by reanalyzing thawed samples in the laboratory of the Department of Medical Biochemis- try, University Hospital of North Norway (UNN; Troms\u00f8,Norway). 38No calibration was found to be necessary for the BIS and RENIS samples, but the following equation was usedto calibrate the AGES-Kidney results to the UNN laboratory:log(iohexol UNN)520.09111.0253log(iohexol AGES).38This calibration resulted in a mean difference in GFR of only 0.87 ml/min from the original results. 33,36,40 eGFR was calculated from serum creatinine(eGFRcrea) using the CKD Epidemiology Collaboration equation.41 Statistical Analyses Characteristics of the study participants were provided at thetime of the GFR measurements, i.e., characteristics for the RENIS cohort was reported for both the baseline (RENIS-T6) and the follow-up (RENIS-FU) examination. 42The associations between GFR indexed for body surface area andage, sex, health status, and cohort membership were exploredin generalized additive regression models for location, scale, and shape (GAMLSS) using the gamV procedure from the mgcViz-package in R. 43,44 GAMLSS is a new regression method suitable for modeling the age-dependent distribution of variables. A detailed explanation of the method can befound in the Supplemental M ethods (Supplemental Figure 1, Supplemental References). In brief, the mean andSD of the GFR distribution are modeled as separate functionsof the independent variables in the same regression model. In addition to investigating the association of the independent variableswith the mean of the GFR distribution as in ordinary regres-sion methods, GAMLSS makes it possible to investigate theirassociation with the SD of the distribution. We analyzed the associations between GFR indexed for body surface area as the dependent variable and age, sex,and health status as independent variables. We adjusted forthe correlation between the \ufb01rst (RENIS-T6) and second (RENIS-FU) GFR measurement for the same individual inthe RENIS cohort by including a random intercept for eachparticipant in all models. The GFR-age associ-ation was de \ufb01ned as the difference in mean GFR per 1-year older age and is represented by the coef \ufb01cient for the age vari- able in the regression model. The re- gression coef \ufb01cient for the interaction between age and health status represents the association between health status and theGFR-age association. 45This criterion scores the models for \ufb01t to the data, but penalizes the score for the number of independent variables and complexity of the model. We\ufb01rst analyzed both the mean and the SD of the GFR distribution as functions of the main effects of age, sex, and thehealth status variable. We also included the interaction be-tween age and health status and a random sex-speci \ufb01c effect for cohort membership to adjust for possible differences inGFR distribution between the cohorts. In this model, statistically signi \ufb01cant main effects of all of the independent variables and the age-health status interac-tion were found for the mean of GFR ( P,0.05). Accordingly, in the SD part of the model, the sex-speci \ufb01c cohort random effect was removed and a common cohort random effect for both sexes ( P,0.05) was retained. To investigate the possibility of different associations be- tween age and GFR distributions across the cohorts, we in-cluded random effects for the interaction between age andcohort in the functions for both the mean and the SD ofGFR. This means that the GFR-age associations may differ between the cohorts, and adding this effect to the func-tion for the mean of the model improved the \ufb01t (AIC533306.61). Next, we tested interactions between age and sex, and health status and sex, for both the mean and SD in this model.Only the interaction between age and sex for the mean wasstatistically signi \ufb01cant ( P50.005). Possible nonlinear effects of age were tested by adding smooth terms for the interaction between age and health sta-tus, and age and sex in separate models. This was done for boththe mean and SD of GFR, but model \ufb01t was no better for these models than for the model without nonlinear terms(AIC533302.98 for the interaction between age and healthstatus; AIC 533304.35 for age and sex). In a separate model, we also tested the effect of the three-way interaction betweenage, sex, and health status on the GFR mean, which was notstatistically signi \ufb01cant ( P50.34). The random cohort effects in the model were set to zero inthese predictions, which means that the predictions representan average between the three cohorts. RENIS-T6 and RENIS-FU combined covered theage range 50 \u201370 years, and the age ranges for BIS and AGES- Kidney were 70 \u201397 and 74 \u201393 years, respectively. The dif- ferences between the cohorts shown in Table 1 re \ufb02ect the variation in age and in inclusion criteria. AGES-Kidney andBIS are similar, except for higher prevalence of diabetes and ACR$3 0m g / gi nB I St h a ni nA G E S - K i d n e y .T h en u m b e ro f persons with body mass index ,20 kg/m 2was 26 (1.6%) in RENIS-T6, 31 (2.3%) in RENIS-FU, 4 (0.7%) in BIS, and15 (2.1%) in AGES-Kidney. A scatterplot of all of the GFR measurements versus age according to health status is presented in Figure 2. Table 2 shows the observed median and the 2.5th and 97.5th percentiles for GFR according to the health statusvariable for each of the cohorts. Using GAMLSS regression, we analyzed the mean and the SD of the GFR distribution as functions of age, sex, cohort,and the health status variable (Table 3, Supplemental Table 3).The intercept of the model corresponds to the GFR estimatefor a 50-year-old woman who is unhealthy (Table 3). Beinghealthy was associated with a slightly lower mean GFR of3.25 ml/min per 1.73 m 2at age 50 years, but with a markedly higher GFR-age association of 0.30 ml/min per 1.73 m2per year, 1606 JASN JASN 31:1602 \u20131615, 2020CLINICAL EPIDEMIOLOGY www.jasn.org Downloaded from http://journals.lww.com/jasn by BhDMf5ePHKav1zEoum1tQfN4a+kJLhEZgbsIHo4XMi0hCywCX1AW nYQp/IlQrHD3i3D0OdRyi7TvSFl4Cf3VC1y0abggQZXdtwnfKZBYtws= on 04/25/2025as represented by the coef \ufb01cient for the interaction between age and health status in Table 3 ( P,0.001). The GFR-age association was less negative for menthan for women, being respectively 20.72 (95% CI, 20.96 to 20.48) versus 20.92 (95% CI, 21.14 to20.70) ml/min per 1.73 m 2per year for people who were healthy, and 21.03 (95% CI,21.25 to20.80) versus 21.22 (95% CI, 21.43 to21.02)ml/min per 1.73 m2per year for those who were unhealthy (P50.005 for the interaction between sex and the GFR-age association). The difference in GFR-age association betweenmen and women was also observed when we strati \ufb01ed by age younger or older than 70 years (Supplemental Figure 2) and by cohort, although this was not statistically signi \ufb01cant (0.12, 0.12, and 0.16 ml/min per 1.73 m 2per year in RENIS, AGES-Kidney, and BIS, respectively).Table 1. Characteristics of the population-based cohorts Characteristic RENIS-T6aRENIS-FUaBIS AGES-Kidney Number of participants, n(%) 1622 (100.0) 1324 (100.0) 547 (100.0) 716 (100.0) Age, yr (SD) 58.1 (3.8) 63.6 (4.0) 78.4 (6.2) 80.3 (4.1) Male sex, n(%) 797 (49.1) 657 (49.6) 311 (56.9) 317 (44.3) Body weight, kg (SD) 79.7 (14.4) 79.4 (14.3) 77.3 (14.0) 77.1 (14.1)Height, cm (SD) 170.6 (8.7) 170.6 (8.7) 166.2 (8.5) 167.7 (9.4) Body mass index, kg/m 2(SD) 27.3 (4.0) 27.2 (4.1) 27.9 (4.3) 27.4 (4.3) Body surface area, m2(SD) 1.9 (0.2) 1.9 (0.2) 1.9 (0.2) 1.9 (0.2) Cardiovascular disease, n(%) Myocardial infarction 1 (0.1) 18 (1.4) 83 (15.2) 89 (12.4) Myocardial revascularization 5 (0.3) 26 (2.0) 93 (17.0) 113 (15.8)Angina pectoris 2 (0.1) 12 (0.9) 56 (10.2) 60 (8.4)Stroke 3 (0.2) 24 (1.8) 42 (7.7) 53 (7.4) Diabetes, n(%) 19 (1.2) 42 (3.2) 136 (24.9) 81 (11.3) Cancer, n(%) 76 (4.7) 120 (9.1) 123 (22.5) 134 (18.7) Hypertension, n(%) b692 (42.7) 693 (52.3) 503 (92.0) 623 (87.0) Systolic BP, mm Hg (SD) 129.7 (17.6) 130.7 (17.0) 144.9 (21.5) 142.3 (20.3) Diastolic BP, mm Hg (SD) 83.4 (9.8) 81.9 (9.3) 82.3 (13.0) 69.6 (10.7)Antihypertensive medication, n(%) 298 (18.4) 420 (31.7) 425 (77.7) 524 (73.2) Digoxin or digitoxin, n(%) 1 (0.1) 6 (0.5) 18 (3.3) 24 (3.4) Lipid-lowering medication, n(%) 106 (6.5) 232 (17.5) 202 (36.9) 287 (40.1) Antidiabetic medication, n(%) 0 (0.0) 11 (0.8) 99 (18.1) 44 (6.1) Smoking, n(%) Never 503 (31.0) 432 (32.6) 263 (48.1) 295 (41.2)Current 344 (21.2) 177 (13.4) 32 (5.9) 42 (5.9)Previous 775 (47.8) 715 (54.0) 252 (46.1) 379 (52.9) Absolute GFR, ml/min (SD) 104.0 (20.1) 98.5 (19.8) 64.8 (19.2) 66.7 (19.4) Body surface area \u2013indexed GFR, ml/min per 1.73 m 2(SD) 94.0 (14.4) 89.1 (14.5) 60.5 (16.3) 61.9 (16.6) CKD-EPI estimate of GFR based on creatinine, ml/min per 1.73 m3(SD) 94.9 (9.5) 88.2 (10.5) 68.8 (17.1) 65.5 (17.1) Urinary ACR $30.0 mg/g, n(%) 24 (1.5) 26 (2.0) 126 (23.0) 110 (15.4) Urinary ACR $300.0 mg/g, n(%) 1 (0.1) 2 (0.2) 19 (3.5) 15 (2.1) Data are shown as mean (SD) or n(%). GFR (ml/min per 1.73 m2) according to health status of participants in the population-based cohorts StudyNumber of Participants Median 2.5th Percentile 97.5th Percentile Unhealthy HealthyaUnhealthy HealthyaUnhealthy HealthyaUnhealthy Healthya RENIS-T6b1109 513 94.2 93.1 65.6 63.0 123.1 118.9 RENIS-FUb964 360 89.2 90.1 57.5 66.5 117.5 115.3 BIS 529 18 60.4 69.8c29.7 44.2 88.7 96.2 AGES-Kidney 672 44 62.5 72.4c26.2 42.4 90.5 98.4 Total 3274 935 82.6 90.8 34.0 59.7 117.8 118.0 aHealthy was de \ufb01ned as no cardiovascular disease, cancer, diabetes, hypertension, smoking, lipid-lowering medication, or digoxin, as well as body mass index ,30 kg/m2and urinary ACR ,30 mg/g. JASN 31:1602 \u20131615, 2020 GFR in Healthy Aging 1607www.jasn.org CLINICAL EPIDEMIOLOGY Downloaded from http://journals.lww.com/jasn by BhDMf5ePHKav1zEoum1tQfN4a+kJLhEZgbsIHo4XMi0hCywCX1AW nYQp/IlQrHD3i3D0OdRyi7TvSFl4Cf3VC1y0abggQZXdtwnfKZBYtws= on 04/25/2025There was no statistically signi \ufb01cant association between the SD of the GFR distribution and older age (Table 3). Thism e a n st h a ta l t h o u g hm e a nG F Rw a sl o w e ra th i g h e ra g e ,t h evariation in GFR at any given age for a given health status wasmore or less the same. The random effects in the model demonstrate there were differences between the cohorts with regard to the associationbetween GFR and male sex ( P50.003), in the GFR-age asso- ciation ( P50.03), and in the SD of the GFR distribution (P,0.001) (Supplemental Table 3). Because we had only three different cohorts, which may be considered few for the esti- mation of random effects, we replaced the random cohort effects with \ufb01xed effects and reanalyzed the model. The point estimates of the \ufb01xed effects model were very similar to the random effects model (Supplemental Table 4). We repeated the analyses of the model shown in Table 3 with nonindexed GFR, measured in ml/min, as the dependentvariable and height and weight added as independent vari-ables. The effect of being healthy on the GFR-age association was very similar to that in the primary analysis (0.32 [95% CI, 0.20 to 0.45] versus 0.30 [95% CI, 0.18 to 0.43] ml/min per1.73 m 2per year; Table 3). The 95% CI for the GFR-age association for persons whowere healthy was consistently lower than zero in all of thesubgroups. The predicted median and 95% reference intervals for GFR in a healthy person based on the \ufb01nal GAMLSS model (Table 3) have been plotted against age for men and women inFigure 3. In people who were healthy, the 60 ml/min per 1.73 m 2threshold intersects the 2.5th percentile at age 67.1 years for women and 70.8 years for men (Figure 3). Table 4presents predicted median GFR values and 95% referenceranges for 5-year intervals in healthy men and women.Predicted tenth, 25th, 75th, and 90th percentiles of theGFR distribution can be found in Supplemental Table 5. Measurements for both the baseline (RENIS-T6) and the follow-up examinations (RENIS-FU) of the same persons in the RENIS cohort are shown. In potential living kidney donors, point estimates between20.5 and 20.9 ml/min per 1.73 m 2per year have been found.51\u201353,56 \u201358 If adjustment for health status had eliminated the GFR-age association, this would have supported the hypothesis thatpoor health fully accounts for the \ufb01nding of lower mean GFR in old age. 5It is noteworthy that the SD of GFR did not increase with age in persons who were healthy, as would be expected if a subset of this group had unrecognized kidneydisease, whereas others aged with preserved GFR. If even aminority of exceptionally healthy individuals aged with pre-served rather than lower GFR, one would expect no or only aweak age association for the 97.5th percentile of the GFR dis-tribution. Although the absence of these \ufb01ndings in this study does not refute the hypothesis, it suggests that age or other factors may contribute to the association. A similar \ufb01nding regarding the 95th percentile was observed in a study of po-tential living kidney donors by Chakkera et al. 59The results a r ea l s oc o n s i s t e n tw i t hh i s t o l o g i c \ufb01ndings in biopsies from living kidney donors, which indicate that nephron number islower at older age even in people who are apparently healthy,although the study by Denic et al. 6only included persons younger than 75 years and may not be representative of older healthy people in the general population. The observation of a high proportion of persons with non- declining GFR in some longitudinal studies may be explainedby a failure to use statistical methods that take measurementerror into account. 8,10,12,60 \u201362This will overestimate the var- iability of the rate at which GFR declines and the proportion ofpersons with nondeclining GFR. Also, none of these studiesused precise methods to measure GFR, 8\u201315,60 \u201366and, exceptfor one small Swedish study,67they were not population based.60,61,63 \u201366The Baltimore Longitudinal Study of Aging is often referred to as having found that a third of a group ofpersons who were healthy experienced no decline in GFR. However, the study may have been biased by the use of creat- inine clearance for assessing GFR and the inclusion of patientswith diabetes in the healthy group. 60 Being unhealthy was associated with a slightly higher GFR in the regression model (Table 3), which indicates that meanGFR for persons who were unhealthy was higher thanfor those who were healthy among the youngest participants(Figure 3). Previous population-based studies,9,12 small studies using measured GFR,51,52,54,75 \u201377and studies of persons with established CKD62,78 \u201381have yielded mixed results about sex differences in GFR-age associations, but mostof them did not fully adjust for comorbidity and risk factors.Our\ufb01ndings may partly explain the higher prevalence of CKD in women than in men, but do not explain why more men thanwomen initiate RRT. General additive mixed model analysis of GFR mean and SD in three population-based cohorts Variable b(95% CI) PValue Effect of independent variables on mean GFR 50-year-old female who was unhealthy (intercept) 98.91 (97.43 to 100.39) ,0.001 Age, per yr 21.22 (21.43 to 21.02) ,0.001 Healthy (yes/no)a23.25 (24.86 to 21.63) ,0.001 Male sex 20.82 (28.54 to 6.90) 0.84 Interaction between age and being healthya0.30 (0.18 to 0.43) ,0.001 Interaction between age and male sex 0.20 (0.06 to 0.34) 0.005 Effect of independent variables on the SD of GFR 50-year-old female who was unhealthy (intercept) 12.40 (10.53 to 14.61) ,0.001 Percentage change associated with each independent variable Age, per yr 20.1% (20.6% to 0.3%) 0.52 Healthy (yes/no)a218.6% ( 223.9% to 213.0%) ,0.001 Male sex 1.4% (23.5% to 6.6%) 0.59 GFR measured in ml/min per 1.73 m2. JASN 31:1602 \u20131615, 2020 GFR in Healthy Aging 1609www.jasn.org CLINICAL EPIDEMIOLOGY Downloaded from http://journals.lww.com/jasn by BhDMf5ePHKav1zEoum1tQfN4a+kJLhEZgbsIHo4XMi0hCywCX1AW nYQp/IlQrHD3i3D0OdRyi7TvSFl4Cf3VC1y0abggQZXdtwnfKZBYtws= on 04/25/2025lower, but comparisons with the source population indi- cate the cohort was representative.28The most probable explanation for the heterogeneity may be unmeasuredconfounders. Predicted percentiles of GFR (ml/min per 1.73 m2) for healthy women and men according to age group Age Group (yr)Women Men Number of GFR MeasurementsMedian2.5th Percentile97.5th PercentileNumber of GFR MeasurementsMedian2.5th Percentile97.5th Percentile 50\u201354 226 93.4 73.7 113.1 217 93.0 73.1 113.0 55\u201359 405 88.8 69.2 108.3 423 89.4 69.6 109.3 60\u201364 566 84.2 64.7 103.6 521 85.8 66.1 105.5 65\u201369 296 79.6 60.3 98.9 293 82.2 62.7 101.8 70\u201374 129 75.0 55.8 94.1 102 78.6 59.2 98.0 75\u201379 253 70.4 51.4 89.4 225 75.0 55.7 94.3 80\u201384 164 65.8 46.9 84.7 188 71.4 52.2 90.6 85\u201389 68 61.2 42.4 79.9 79 67.8 48.8 86.8 $90 20 56.6 38.0 75.2 34 64.2 45.3 83.1 Estimates corresponding to Figure 3.020406080100120 50Glomerular filtration rate, mL/min/1.73 sq.m55 60 65 70 75Women 80 85 90 95 020406080100120 Men 50 55 60 65 70 75 80 85 90 95 Age, years Figure 3. One study of CKD prevalence found that Norway had the lowest and Northeastern Germany the high-est prevalence, 83and that these differences could not be ex- plained by variations in hypertension, diabetes, or obesity. One method for de \ufb01ning reference intervals for physiologic or biochemical parameters is to take the 95th interpercentilerange of their distribution in a population of persons who arehealthy. 7Accordingly, most previous studies of reference intervals for GFR have in- cluded few individuals older than 70 years and have made no distinction between healthy subjects and others.53,84 \u201386By con- trast, we used a statistical model to estimate the effects of diseaseand risk factors and to predict percentiles of GFR in personswho were healthy and between age 50 and 95 years. Because weadjusted for the heterogeneity between the cohorts, the predic-tions represent an average of the observations in three differentgeographic regions.A comparison between classi \ufb01cation of low GFR based on the 2.5th percentile for persons who are healthy in thisstudy and the currently accepted criterion for CKD stage 3\u20135( G F R ,60 ml/min per 1.73 m 2) shows that the criterion underestimates the prevalence of low GFR in women youn-ger than 67.1 years and in men younger than 70.8 years, andoverestimates the prevalence at higher ages (Figure 3).However, this is the result of applying a low percentile andar a t h e rs t r i c td e \ufb01nition of \u201chealthy \u201dto assess the effect of age on GFR under optimal physiologic conditions. In addi-tion to reference intervals for GFR in people who are healthy, an optimal classi \ufb01cation system for CKD should be based on the risk of adverse outcomes at different levelsof GFR. Predicted medians (bold lines) and 2.5th and 97.5th percentiles (dashed lines) of iohexol clearance (black) and eGFR based on creatinine (red) as functions of age for healthy women (upperpanel) and men (lower panel). The principal strength of this study is that we used mea- sured GFR instead of eGFR and that the cohorts are popula- tion based. The number of included persons far exceeds that of pre-vious studies using measured GFR, especially in the oldest agegroups. Although the low number of persons who werehealthy in the highest age groups may have limited the powerof statistical tests for the interaction between age, health status, and other variables, it seems unlikely that we have failed to include a signi \ufb01cant number of very old persons who were healthy with preserved GFR that would have changed ourconclusion. The heterogeneity between the three cohorts is a reason for caution in applying the results to other populations. We can- not exclude a survivor bias in the estimates of the GFR distri-bution, especially in the older age groups. Due to the cross-sectionaldesign of our study, the GFR-age associations apply at thepopulation level, whereas the rates of change at the individuallevel could not be estimated. 88However, even allowing for great variation and possible nonlinear individual GFR trajectories, the \ufb01nding that the 2.5th percentile in 50-year-old people who are healthy is sim-ilar to the 97.5th percentile in 95-year-old people who arehealthy (Figure 3) suggests that aging with preserved GFRacross this age span must be very uncommon. We conclude that there is a negative linear GFR-age asso- ciation in people who are healthy and aged between 50 and 95 years, and an even more negative association for peoplewith chronic diseases and CKD risk factors. S .L e v e y ,T .M e l s o m ,R .P a l s s o n ,E. Schaeffner, and M. van der Giet organized the GFR measurementsand collected the data; B.O. All authors approved the \ufb01nal version of the manuscript and agreed to be accountable for all aspects of the work. Levey reports grants from National Institute of Diabetes and Digestive and Kidney Diseases during the conduct of the study, grants from the National Kidney Foundation, and grants from Siemens, outside the sub-mitted work. Dr. Schaeffner reports grants from DDn\u00c4 Institut f\u00fcr Disease Management, grants from Kuratorium f\u00fcr Dialyse und Nie rentransplantation foundation of preventive medicine, during the conduct of the study, and reports receiving honoraria for lectures from Fresenius Kabi, Fresenius Medical Care, and Siemens Healthineers, outside the submitted work. 1612 JASN JASN 31:1602 \u20131615, 2020CLINICAL EPIDEMIOLOGY www.jasn.org Downloaded from http://journals.lww.com/jasn by BhDMf5ePHKav1zEoum1tQfN4a+kJLhEZgbsIHo4XMi0hCywCX1AW nYQp/IlQrHD3i3D0OdRyi7TvSFl4Cf3VC1y0abggQZXdtwnfKZBYtws= on 04/25/2025FUNDING AGES-Kidney was funded by National Institutes of Health, National Institute on Aging (NIA) contract N01-AG-1-2100; the NIA Intramural Re-search Program; Hjartavernd (the Icelandic Heart Association); and Althingi (the Icelandic Parliament). Xie Y, Bowe B, Mokdad AH, Xian H, Yan Y, Li T, et al.: Analysis of the Global Burden of Disease study highlights the global, regional,and national trends of chronic kidney disease epidemiology from 1990to 2016. Schaeffner ES, van der Giet M, Gaedeke J, T\u00f6lle M, Ebert N, Kuhlmann MK, et al.: The Berlin initiative study: The methodology of exploring kidney function in the elderly by combining a longitudinal and cross-sectional approach. Melsom T, Schei J, Stefansson VT, Solbu MD, Jenssen TG, Mathisen UD, et al.: Prediabetes and risk of glomerular hyper \ufb01ltration and albu- minuria in the general nondiabetic population: A prospective cohortstudy. Ebert N, Jakob O, Gaedeke J, van der Giet M, Kuhlmann MK, Martus P, et al.: Prevalence of reduced kidney function and albuminuria in older adults: The Berlin initiative study. G\u00f6\u00dfwald A, Schienkiewitz A, Nowossadeck E, Busch MA: [Prevalence of myocardial infarction and coronary heart disease in adults aged 40-79 years in Germany: Results of the German health interview andexamination survey for adults (DEGS1)]. Busch MA, Schienkiewitz A, Nowossadeck E, G\u00f6\u00dfwald A: [Prevalence of stroke in adults aged 40 to 79 years in Germany: Results of the German health interview and examination survey for adults (DEGS1)].Bundesgesundheitsblatt Gesundheitsforschung Gesundheitsschutz 56: 656 \u2013660, 2013 32. Melsom T, Stefansson V, Schei J, Solbu M, Jenssen T, Wilsgaard T, et al.: Association of increasing GFR with change in albuminuria in the general population. Bj\u00f6rk J, Grubb A, Gudnason V, Indridason OS, Levey AS, Palsson R, et al.: Comparison of glomerular \ufb01ltration rate estimating equations derived from creatinine and cystatin C: Validation in the age,gene/environment susceptibility-reykjavik elderly cohort. Cohen E, Nardi Y, Krause I, Goldberg E, Milo G, Garty M, et al.: A longitudinal assessment of the natural rate of decline in renal function with age. Eriksen BO, Ingebretsen OC: The progression of chronic kidney dis- ease: A 10-year population-based study of the effects of gender and age. Gerontology 48: 22\u201329, 2002 AFFILIATIONS 1Metabolic and Renal Research Group, University of Troms\u00f8 \u2013The Arctic University of Norway, Troms\u00f8, Norway 2Section of Nephrology, Clinic of Internal Medicine, University Hospital of North Norway, Troms\u00f8, Norway 3Division of Nephrology, Landspitali \u2014The National University Hospital of Iceland, Reykjavik, Iceland 4University of Iceland, Reykjavik, Iceland 5Institute of Public Health, Charit\u00e9 \u2013Berlin University of Medicine, Berlin, Germany 6Department of Nephrology, Charit\u00e9 \u2013Berlin University of Medicine, Berlin, Germany 7Icelandic Heart Association, Kopavogur, Iceland 8Division of Nephrology, Tufts Medical Center, Boston, Massachusetts 9Department of Organ Transplantation, Oslo University Hospital and University of Oslo, Oslo, Norway 10Institute for Clinical Research and Health Policy Studies, Tufts Medical Center, Boston, Massachusetts 11Tufts Clinical and Translational Science Institute, Tufts University, Boston, Massachusetts JASN 31:1602 \u20131615, 2020 GFR in Healthy Aging 1615www.jasn.org CLINICAL EPIDEMIOLOGY Downloaded from http://journals.lww.com/jasn by BhDMf5ePHKav1zEoum1tQfN4a+kJLhEZgbsIHo4XMi0hCywCX1AW nYQp/IlQrHD3i3D0OdRyi7TvSFl4Cf3VC1y0abggQZXdtwnfKZBYtws= on 04/25/2025"
    },
    {
      "file": "C:\\temp\\Units\\UNIT_0 - Pre-Reading Materials (Kelly Only)\\Unit-6-meta.pdf",
      "summary": "\u2022Meta analysis is a speci\ufb01c type of research synthesis in which the aim is to quantify a speci\ufb01c parameter or test a speci\ufb01c hypothesis of interest, using results from multiple quantitative sources, and objectively assessing uncertainty for all claims. \u2022One version of the pooled estimate is the simple average of the estimates, $\\hat{\\theta}_a = (\\hat{\\theta}_1 + \\hat{\\theta}_2)/2$. The factor $\\sqrt{(s_1\u02c62 + s_2\u02c62)/2}$ is the standard deviation derived from the average variance. The factor of 1$1/\\sqrt{2}$ is the improvement in precision that results from pooling evidence from two independent studies. \u2022If the standard errors are equal, i.e. $s_1 = s_2$, then the unweighted and weighted pooled estimators are the same. If $H$ is the harmonic mean of the study-level variances, then the standard deviation of the inverse variance weighted average is $H\u02c6{1/2}/\\sqrt{2}$. \u2022If we are pooling $k$ independent estimates, the standard error of the inverse variance weighted average is $H\u02c6{1/2}/\\sqrt{k}$, where $H$ is the harmonic mean of the $k$ study-level variances. \u2022We can pool the data and re-analyze it as a single data set, but it is important to be aware of heterogeneity (systematic di\ufb00erences) among the studies. \u2013Include both negative and positive \ufb01ndings \u2013Reporting of quantitative \ufb01ndings (point estimates, standard errors) \u2013Reporting of methods \u2013De\ufb01nition of target study population, and inclusion/exclusion criteria \u2013Handling of confounding factors \u2013Generally exclude studies that are themselves research syntheses \u2013Multiple studies of the same subject pool are not independent \u2013Multiple studies by the same research team may not be independent \u2013Is it possible to include unpublished studies (pre-registration may make this possible) \u2013Publication language \u2013Select by date? This is a formalized process in which a null hypothesis is speci\ufb01ed and we quantify evidence against the null hypothesis using a test statistic. \u2013The fact that underpins most p-value combining procedures is that if the null hypothesis is true, the p-value follows a uniform distribution on the interval $(0, 1)$. \u2013The classical method for non-independent p-values is the Bonferroni method, which uses the value of $m\\times {\\rm min}(p_1, \\ldots, p_m)$ as a meta p-value. Under the global null hypothesis, $T$ follows a standard Cauchy distribution, and therefore can be transformed to a p-value using the CDF of the reference distribution. \u2022In some cases, it may be reasonable to presume that there is a single com- mon treatment e\ufb00ect, that is, all of the $\\theta_i$ are equal to a common value $\\theta$. As discussed above, the value of $\\theta$ is e\ufb03ciently estimated using the inverse variance weighted mean of the individual study estimates, which we denote $\\hat{\\theta}$. \u2022To account for study heterogeneity, a common approach to meta-analysis is therandom e\ufb00ects orhierarchical approach, in which we posit the model $\\hat{\\theta}_i = \\theta_i + s_i\\epsilon_i$, where the $\\theta_i$ are treated as random variables from a distribution with mean $\\theta$ and variance $\\tau\u02c62$, and the $\\epsilon_i$ are also random variables that are independent of the $\\theta_i$ and that follow a distribution with mean $0$ and variance $1$. \u2013Sometimes we allow the $\\epsilon_i$ to have variance $\\sigma\u02c62$, to account for systematic biases in the reported standard errors. According to the law of total variation, the population variance of the $\\hat{\\theta}_j$ is $\\sigma\u02c62 + \\tau\u02c62$. The statistic $I\u02c62 = \\tau\u02c62 / (\\tau\u02c62 + \\sigma\u02c62)$ is a measure of the proportion of total variance due to study heterogeneity. In a small meta-analysis (with few studies), the sample estimate $\\hat{I}\u02c62$ is biased and its con\ufb01dence interval under-covers the true $I\u02c62$, see here for more details. If truncation is performed, the bias is positive, and is around 0.15 for very small meta-analyses, and becomes smaller as the number of studies in the meta-analysis grows. 6Meta regression \u2022When study characteristics have been quanti\ufb01ed in a consistent way across studies, it is possible to \"partial out\" the contributions of speci\ufb01c study characteristics to the overall heterogeneity. This model should be \ufb01t with generalized least squares using inverse variance weights, with the variances being the squares of the reported standard error for each study e\ufb00ect. Then the treatment e\ufb00ect for populations excluding subjects with prior treatment is $\\beta_1 + \\beta_3$, while the treatment e\ufb00ect for populations that do not make this exclusion is $\\beta_1$. Network meta-analysis \u2022In a basic two-arm study, the treatment e\ufb00ect is often estimated by taking the average response among treated subjects and subtracting from it the average response among control subjects. If study or study/arm characteristics are known, let $Z_{ij}$ be a vector of characteristics for arm $j$ in study $i$, and use a mean structure model such as $E[Y_{ij}] = \\mu + \\alpha_i + \\beta_j + \\gamma\u02c6\\prime Z_{ij}$ The overall model would be $Y_{ij} = E[Y_{ij}] + s_{ij}\\epsilon_{ij}$, where $s_{ij}$ is the reported standard error for $Y_{ij}$ and the $\\epsilon_{ij}$ are independent unit-standard deviation random terms capturing unexplained variation. If sample sizes are reported but standard errors are not, it is possible to model the variance as ${\\rm Var}[Y_{ij}] \\propto 1/n_{ij}$, where $n_{ij}$ is the sample size. When there are large number of treatments, we can visualize the data as a graph, where treatments $j$ and $k$ are connected in the graph if these two treatments are ever present in the same trial. \u2022One potential source of selection bias in meta-analysis at the level of the studies (not the subjects) is publication bias. This refers to the possibility that inconclusive studies or studies that contradict the dominant narrative are less likely to be published than studies that support the consensus point of view. \u2022If all studies are assessing the same e\ufb00ect, then the scatter of the standard error of the estimated treatment e\ufb00ect (on the vertical axis) against the estimated treatment e\ufb00ect (on the horizontal axis) can be used to assess publication bias. \u2022Some funnel plots use the precision as the vertical coordinate rather than the standard error (the precision is the reciprocal of the standard error). \u2022The logic behind a funnel plot is that any given quantile of the e\ufb00ect estimates should scale linearly with the standard error. \u2022Publication bias may be re\ufb02ected in an empty region in the funnel plot, usually corresponding to studies with higher standard error that contradict the hoped-for narrative."
    },
    {
      "file": "C:\\temp\\Units\\UNIT_0 - Pre-Reading Materials (Kelly Only)\\Unit-6-Statistics in Medicine - 2015 - Hoaglin - Misunderstandings about Q and  Cochran s Q test  in meta\u2010analysis.pdf",
      "summary": "FeaturedArticle Received19 March 2015, Accepted31 July 2015 Published online 24 August 2015 inWileyOnline Library (wileyonlinelibrary.com) DOI: 10.1002/sim.6632 Misunderstandingsabout Qand \u2018Cochran\u2019s Qtest\u2019inmeta-analysis DavidC.Hoaglin *\u2020 Many meta-analyses report using \u2018Cochran\u2019s Qtest\u2019 to assess heterogeneity of effect-size estimates from the individualstudies.SomeauthorsciteworkbyW.G.Cochran,withoutrealizingthatCochrandeliberatelydidnotuse Qitselftotestforheterogeneity.Further,whenheterogeneityisabsent,theactualnulldistributionof Qisnot the chi-squared distribution assumed for \u2018Cochran\u2019s Qtest\u2019. It then discusses derivations of the asymptotic approximation for the null distribution of Q,a sw e l la sw o r kt h a t hasderivedfinite-samplemomentsandcorrespondingapproximationsforthecasesofspecificmeasuresofeffectsize. Software that outputs QandI2should use the appropriate reference value of Qfor the particular measure of effect size and the current meta-analysis. Qis a key element of the popular DerSimonian\u2013Laird procedure for random-effects meta-analysis, but the assumptions of that procedure and related procedures do not reflect the actual behaviorof Qandmayintroducebias.TheDerSimonian\u2013Lairdprocedureshouldberegardedasunreliable.Copyright\u00a9 2015John Wiley & Sons,Ltd. Careful investigation of departures from homogeneity includesgraphicaldisplaysandconsiderstherelationoftheeffectstocharacteristicsofthestudies.Manymeta-analyses include a statistical test of the null hypothesis of homogeneity. In what follows, in the hope of correcting the misunderstandings, Section 2 gives the definition of Q and discusses its customary use in meta-analysis. Department of Quantitative Health Sciences, University of Massachusetts Medical School, 368 Plantation Street, Worcester, MA 01605, U.S.A. *Correspondence to: David C. Hoaglin, Department of Quantitative Health Sciences, University of Massachusetts Medical School, 368 Plantation Street, Worcester, MA 01605, U.S.A. \u2020E-mail: dchoaglin@gmail.com Copyright \u00a92015 John Wiley&Sons, Ltd. Statist. Definition of Q In a meta-analysis of kstudies, the basic ingredients for Qare the studies\u2019 estimates of the effect and the estimated standard errors of those estimates. In the notation of Cochran [1], the effect estimate fromStudy iisx i,andthecorrespondingestimatedstandarderroris si(i=1,\u2026,k).Qmeasuresheterogeneity of the xiby summarizing their variation around a weighted mean, \u0304xw, in which the weight for xiis the reciprocal ofits estimated variance, wi=1\u2215s2 i: \u0304xw=\u2211k i=1wixi/\u2211k i=1wi. Often, Qserves as the basis for a formal test of homogeneity, in which (under the null hypothesis) Qis assumed to follow a chi-squared distribution with k\u22121 degrees of freedom. Under the heading \u2018Mistakes to avoid when selecting a model\u2019, Borenstein et al.[2]comment,\u2018Thisisabadideaformanyreasons.\u2019Theydiscussfourreasonsandthen emphasizethepointintheirsummary:\u2018Thestrategyofstartingwithafixed-effectmodelandthenmovingto a random-effects model if the test for heterogeneity is significant relies on a flawed logic and shouldbe strongly discouraged\u2019. In the context of a high-profile example, a meta-analysis of the incidence ofmyocardialinfarctionandcardiacdeathinrandomizedtrialsinvolvingthedrugrosiglitazone[3],Shusteret al. [4] demonstrate that that strategy can lead to potentially misleading results. Cochran\u2019s1954 paper For an understanding of Q, it is helpful to examine the setting in William Cochran\u2019s 1954 paper, \u2018The Combination of Estimates from Different Experiments\u2019 [1]: \u2018This paper discusses one of the simpleraspects of the problem, in which there is sufficient uniformity of experimental methods so that the ith experiment provides an estimate x iof\ud835\udf07and an estimate siof the standard error of xi. The experiments may be, for example, determinations of a physical or astronomical constant by different scientists, orbioassayscarriedoutindifferentlaboratories,oragriculturalfieldexperimentslaidoutindifferentpartsofaregion.Thequantity x imaybeasimplemeanoftheobservations,asinaphysicaldetermination,or the difference between the means of two treatments, as in a comparative experiment, or a median lethaldose,oraregressioncoefficient.\u2019If\u2018thevaluesofthe x iagreeamongthemselveswithinthelimitsoftheir experimental errors\u2019,Cochran postulates \u2018anunderlying mathematical model of theform xi=\ud835\udf07+ei, where eiis the experimental error of xi. If the values of the xidiffer by more than can be accounted for bytheir experimental errors,werequirea model of theform xi=\ud835\udf07i+ei, where\ud835\udf07i, which might be called the \u201ctrue value\u201d in the ith experiment, varies from one experiment to another.\u2019Ifsuchvariationexists,Cochranreferstoitas\u2018an interaction [italicsoriginal]oftheeffectwith experiments\u2019. Cochran makes clear that the process is not a mechanical one (page 102): \u2018If an interaction exists, the type of combined estimate that is wanted requires careful consideration. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License D.C.HOAGLIN account the purpose for which the combined estimate is to be made and the reasons for the presence of interaction,insofarasthesecanbediscovered. [illustrationsofinteractionsomitted]thecombinationofthe individual estimates is not a routine matter, but requires clear thinking about both the nature of the data and the function of the combined estimate. However, unless it is decided that no type of combined estimatewillserveausefulpurpose,wedofacetheproblemofcombiningatleastovercertainsubgroupsofthe experiments.\u2019 Considering the difference in setting and the passage of time, Cochran\u2019s advice differs little from that given by, for example, Borenstein et al. [2]. Theconservativedecision,whenindoubt,istoassumeinteractionspresent,sincethetechniquesforthissituationremain valid even ifinteractions are absent.\u2019 Throughout the paper, Cochran associates with each estimated variance, s 2 i, a number of degrees of freedom, ni. Thus, the niare an inherent part of the definition of Q. For the sorts of xithat Cochran discusses, the degrees of freedom ordinarily come from a mean square (e.g., the sample variance or the residual mean square of a regression). The development in Cochran\u2019s paper has implications for the distribution of Q, the standard error of \u0304xw, estimationofthe variance of the \ud835\udf07i, and potential bias in \u0304xw. Cochran defines Qas in Section 2 and then comments (page 114): \u2018If the degrees of freedom niare large, Qfollows the \ud835\udf122distribution with ( k\u22121) degrees of freedom.\u2019 (Today one might write\u2018approaches\u2019insteadof\u2018follows\u2019.Itislikelythat,in1954,readersinterpreted\u2018follows\u2019inthatway.) Because nis the number of degrees of freedom for each of the s2 i, however, this approximation is unlikely to be useful for meta- analysis. (\u0304xw)=\u221a 1 w{ 1+4 w2\u22111 n\u2032 iwi(w\u2212wi)} , where w=\u03a3wiandn\u2032 i=ni\u22124(k\u22122)\u2215(k\u22121).\u2018Theterminsidethebracketsisanadjustmentwhichtakes accountofsamplingerrorsintheweights1/s2 iasestimatesofthetrueweights1/\ud835\udf0e2 i,andalsoofthefact that the principal term inside the square root, 1 \u2215w, tends to be an underestimate of the corresponding population expression. For the approximate number of degrees of freedom neto beattached tothis standarderror,Meier (1953) [8]suggests ne=w2 \u2211(w2 i/ni).\u2019 Themeta-analysisliterature,however, generally uses1/ wtoestimatethe variance of \u0304xw. Estimation of the variance of the \ud835\udf07i. When interactions are present (corresponding to random-effects meta-analysis), Cochran does not use Qas the basis for an estimate of the variance of the \ud835\udf07i. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License D. C.HOAGLIN where\u0304xis the(unweighted) mean of the xiand\u0304s2is themean of the s2 i. Hedecomposes \ud835\udf122 total=k\u2211 i=1wix2 i according to \ud835\udf122 total=\ud835\udf122 homog+\ud835\udf122 assoc,i nw h i c h \ud835\udf122 homogassesses the degree of homogeneity among the k measures of association, and \ud835\udf122 assocassesses the significance of the average degree of association. The approxima- tion involved is the usual large-sample one that applies to the behavior of the chi-squared statistic for a2 \u00d72 table. Kulinskaya et al. [13] point out that the distribution of Qdepends on themeasureofeffect.Theydiscussthecaseinwhichtheeffectandtheweightforanindividualstudymaydepend on two parameters, the effect \ud835\udf03 iand a nuisance parameter \ud835\udf0di, both of which must be estimated. They present fairly general expansions for the first two moments of Qin terms of the weights and the moments of the estimators of the two parameters. (Even for the mean of Q, the expansion is sufficiently complicatedthatIdonotquoteithere. )Asanexample,theyapplytheirexpansionstotheriskdifference.On the basis of extensive simulations, they recommend approximating the null distribution of Qby a gammadistributionwhosefirsttwomomentsequaltheestimatesderivedfromtheexpansions.Theyfindthat the resulting test for homogeneity is substantially more accurate than the usual chi-squared test and other tests,especially when thestudies\u2019samplesizes aresmall or moderate. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License D.C.HOAGLIN TheresultsofKulinskaya et al.[13]andtheirresultsforstandardizedmeandifference[14]mayseem toconflictwithpublicationsthatpresent\u2018theexactdistributionof Q\u2019.Thosepublications,however,obtain their \u2018exact\u2019 resultsbymakingtwokeyassumptions: \u2022 Each xifollows anormal distribution. Inanotherpaperwhosetitleincludes\u2018theexactdistributionofCochran\u2019s Q\u2019,PreussandZiegler[16]build onthe\u2018exact\u2019cumulativedistributionfunctionofCochran\u2019s QderivedbyBiggerstaffandJacksonanduse partial integration to avoid evaluating that CDF numerically in order to obtain a confidence interval for thebetween-studyvariance inrandom-effectsmeta-analysis.PreussandZiegler(page56)say,\u2018Inthesederivations, we have made the conventional assumption that the within study variances are known. If, for example, the measure of effect size is the log of the odds ratio, they can (for each replication) calculate the study-specificestimates and their estimated variances, as the definition of Qrequires. Almostallsucheffortsreportedintheliterature,however,assumethateachstudy-specificestimatehasanormaldistributionandthatthevariancesofthoseestimatesareknown.Thus,theyforgoanopportunitytoexaminetheactualnullandnon-nulldistributionsof Qinvarioussettings.Forexample,Mittlb\u00f6ckand Heinzl [17] assume that each study-specific estimate has a normal distribution; \u2018make the conventionalassumption that the precisions are known, although in reality they are estimated from the data in eachstudy\u2019;andsaythat\u2018[u]nderthenullhypothesisofhomogeneity,thestatistic Qfollowsa \ud835\udf12 2-distribution withdfdegreesoffreedom, df=k\u22121\u2019.Simulationswithsuchlimitingassumptionsareunlikelytohave much relevance to actual meta-analyses. On the other hand, in a study of confidence intervals (based on Qand other approaches) for the amount of heterogeneity in random-effects meta-analysis, with log-odds-ratio as the measure of effectsize,Viechtbauer[18]generated2 \u00d72tablesbysamplingfromapairofbinomialdistributions.Inapply- ing the asymptotic variance formula to estimate the within-study variances, he added 0.5 to each cell;but,surprisingly,theformulaforthe effect sizeestimatedid not includeany suchadjustment. Often given as a percentage, I2has the advantage that its scaling does not change with the scaling of Q. In the setting of a random-effects model in which the within-study variance of each xiis\ud835\udf0e2and the between-study heterogeneity is \ud835\udf0f2, Higgins and Thompsonintroduced I2as an estimate of \ud835\udf0f2 \ud835\udf0f2+\ud835\udf0e2 (in practice, the within-study variance differs among studies, and \ud835\udf0e2is estimated by a \u2018typical\u2019 within- studyvariance,discussedinthesucceedingtext).Thequantity \ud835\udf0f2/(\ud835\udf0f2+\ud835\udf0e2)\u2018hasanappealinginterpre- tation as the proportion of total variation in the estimates of treatment effect that is due to heterogeneitybetweenstudies.\u2026Thisinterpretationmaybemadeapproximatelyforthestatistic I 2inthegeneralcase Copyright \u00a92015 John Wiley&Sons, Ltd. Statist. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License D. C.HOAGLIN [inwhichthewithin-studyvariancediffersamongstudies].\u2019Withthisformulationof I2,thenullhypoth- esis that\ud835\udf0f2=0 implies that I2=0, and it is easy to interpret a positive value of I2as representing heterogeneity. Even if the null distribution of Qwere chi-squared on k\u22121 degrees of freedom, the probability of values exceeding the mean ( k\u22121)would not be small. That is, values of I2that would be interpreted as evidence of heterogeneity are quite likely under the assumed chi-squared null distribution, which corresponds to absence of heterogeneity. Referring that value of Qto the chi-squared distribution on 4 degrees of freedom produceda P-valueof0.049,whereasthegammadistributionfittedtothefirsttwomomentsoftheactual null distribution of Qproduced a P-value of 0.076. If one desires to retain E[ Q] under the null hypothesis as the threshold for positive values of I2,t h e nE [ Q] will have a separate expression for each measure of effect, and, further, each meta-analysis (with the same measure of effect) will potentially yield a different estimate of E[ Q] (with variability totakeintoaccount). Because theyusethe moment-basedestimateof \ud835\udf0f2[20], \u0302\ud835\udf0f2 DL=Q\u2212(k\u22121) \u2211wi\u2212\u2211w2 i\u2211wi (with\u0302\ud835\udf0f2 DL=0w h e n Q<k\u22121), the definition of s2is precisely the one that makes the two expressions forI2equivalent: \u0302\ud835\udf0f2 \u0302\ud835\udf0f2+\u0302\ud835\udf0e2=Q\u2212(k\u22121) Q. Itisinstructivetoconsiderthesenseinwhich s2isa\u2018typical\u2019within-studyvariance.Onewouldnaturally expect it to be some sort of weighted average of the s2 i=1/wi.I nf a c t , s2is a weighted harmonic mean of the s2 iwith weights(\u2211wi\u2212wi)/[(k\u22121)\u2211wi]: 1 s2=1 k\u22121\u22111 s2 i( 1\u2212wi\u2211wi) . \u2018For practical application, [they] recommend a simple construction for an interval (method III in the Appendix) involving only Qandk, derived from a test-basedstandarderrorforln( H).Intervals are ofthe form exp(lnH\u00b1Z\ud835\udefc\u00d7SE[ln(H)]), where Z\ud835\udefcis the(1\u2212\ud835\udefc\u22152)quantileof thestandardnormal distributionand 490 Copyright \u00a9 2015 John Wiley& Sons, Ltd. Statist. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License D.C.HOAGLIN SE[ln(H)] =1 2ln(Q)\u2212ln(k\u22121)\u221a (2Q)\u2212\u221a (2k\u22123)ifQ>k \u221a{ 1 2(k\u22122)( 1\u22121 3(k\u22122)2)} ifQ\u2a7dk.\u2019 Method III in the appendix (pages 1554 and 1555) \u2018is based on a normal approximation to the \ud835\udf122 distributionforlargedegreesoffreedom: Z=\u221a (2Q)\u2212\u221a (2k\u22123)hasapproximatelyastandardnormal distribution (formula 26.4.13 in Abranowitz [sic] and Stegun [22]), so equating Zwith(ln(Q)\u2212ln(k\u2212 1))\u2215SE[ln(Q)],[they] estimateastandarderrorfor ln( Q)using SE[ln(Q)] =ln(Q)\u2212ln(k\u22121)\u221a (2Q)\u2212\u221a (2k\u22123) Now, since Q=(k\u22121)H2,a n d kis a constant, [they] have var[ln(Q)] =4var[ln(H)]and hence a test-basedstandarderrorfor ln( H)is SE1[ln(H)] =1 2ln(Q)\u2212ln(k\u22121)\u221a (2Q)\u2212\u221a (2k\u22123) Adrawbacktothetest-basedstandarderroristhatitapproacheszeroas Happroaches1,whereas[they] define Hto be 1 whenever Q\u2a7dk\u22121. To overcome this, for small H, [they] take a standard error based on the approximate variance of ln (Q\u2215(k\u22121)) =2ln(H)when Qtruly has a \ud835\udf122distribution with k\u22121 degrees offreedom(formula26.4.36in Abranowitz [sic]andStegun [22]): SE0[ln(H)] =\u221a{ 1 2(k\u22122)( 1\u22121 3(k\u22122)2)} Usingone or theother of thesestandarderrors,a95per cent uncertainty interval for Hfollows as exp(lnH\u00b11.96\u00d7SE[ln(H)])\u2019. Although the null distribution of Qis not chi- squared on k\u22121 degrees of freedom, the following comments retain that assumption because Higgins andThompsonusedit. (1) Miettinen[23]devisedtest-basedconfidencelimitstoavoidcomplexcalculations.Heobtainedthe standard error by standardizing a variance-stabilized function of the point estimate and equatingthe square of that quantity and the value of another statistic that tested the same null hypothesis andhadthechi-squareddistributionon1degreeoffreedomasitsnulldistribution.Inthisinstance, thetwostatisticshave(approximately)thestandardnormaldistribution.Theright-handsideoftheequation, (ln(Q)\u2212ln(k\u22121))\u2215SE[ln(Q)],arisesfromremovingsomeoftheskewnessinthedistri- bution of Qby taking the logarithm, subtracting the mean, and dividing by the standard deviation toobtainaquantitywhoseasymptoticdistribution(asthenumberofdegreesoffreedombecomeslarge)isstandardnormal.ThensolvingforSE [ln(Q)]yieldsthetest-basedestimateofthestandard error.Unfortunately,Halperin[24]andGreenland[25]showedthatthetest-basedapproachisfal-lacious.Theconfidenceintervalinvolvingthetest-basedstandarderrorisvalidonlyunderthenullhypothesis. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License D. C.HOAGLIN HigginsandThompsonusedsimulationtocomparecoverageofseveralmethodsofobtainingconfidence intervals for H. (They did not, however, say precisely what data they generated or how.) Those results may reflect bias in the test-based estimate of the standard error of ln(H). Thus, considering also the fact that the null distribution of Qis not chi-squared on k\u22121 degrees of freedom, the width of the resulting confidence interval for I2may not reflect heterogeneity when heterogeneity is present. Fortwoversionsofthestandardizedmeandifference,Huedo-Medina et al.[28]studiedaspectsofthe performanceof I 2inrelationtothenumberofindividualstudies,theaveragesamplesizeinthosestudies, the between-studies variance, the ratio of the within-study variance in the experimental group to that inthe control group, and the distribution of individual scores in the two groups. Their simulation sampled true means for the experimental group ( \ud835\udf07 E=\ud835\udf03i)from the random-effects distribution \ud835\udf03i\u223cN(0.5,\ud835\udf0f2), generated random samples of scores for the experimental and control groups (with nE=nC),a n dt h e n calculatedthegroupmeansandwithin-groupstandarddeviations.Thesestepsplausiblyparalleltheway in which data for a meta-analysis might arise. The basic data came from a meta-analysis of 70 trials givenbyOlkin[30].Theirrandom-effectsmodelintroducedan M-foldincreaseintheprecisionofeach study-level estimate (correspondingtoan M-foldincreaseinsamplesize): xM,i=\ud835\udf07+\u221a \ud835\udf0e2 i/M+\ud835\udf0f2ti; for\ud835\udf07, they used the overall estimate from the initial meta-analysis; for \ud835\udf0e2 i,t h e yu s e d s2 i(the within-trial sampling variance); for \ud835\udf0f2, they used the DerSimonian\u2013Laird estimate of the heterogeneity parame- ter (\u0302\ud835\udf0f2 DL),a n d ti\u223cN(0,1). This approach may be reasonable for the intended purpose, but it involves approximations that create some distance from the process underlying the data in the trials. According tolarge-sampletheory,thedistributionofthelogoftheoddsratioapproachesanormaldistribution,butthe total sample sizes of the 70 trials range from 18 to 17,187 with a median of 111.5 and quartiles of 52 and 313. In order to use the customary formula for the asymptotic variance of log(OR), one must modify 2 \u00d72 tables that contain a zero cell. The data for 4 of the 70 trials contain a zero cell, and the oddsratiosandconfidencelimitsinFigure4of[30]areconsistentwithadding0.5toeachofthecellsof those2\u00d72ta b le s. Inordertointerpret I 2effectively,itisnecessarytohaveinformationonhowaccuratelyitestimatesthe percentageofvariationassociatedwithheterogeneityinavarietyofpracticalsituations.Suchinformationgenerally appears to belacking. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License D.C.HOAGLIN Forthe random-effectsmodel xi=\ud835\udf07+\ud835\udeffi+\ud835\udf00i, withE(\ud835\udeffi)=0,var(\ud835\udeffi)=\ud835\udf0f2,E(\ud835\udf00i)=0,var(\ud835\udf00i)=\ud835\udf0e2 i,andindependenceamongthe \ud835\udeffi,amongthe \ud835\udf00i,and betweenthe \ud835\udeffiandthe\ud835\udf00i,andwith wi=1/\ud835\udf0e2 i,thederivationof E(Q)isstraightforward.Whenoneuses wi=1/s2 iinsteadof wi=1/\ud835\udf0e2 i,asDerSimonianandLairddo,thederivationisnolongervalid.Inview of the modest study-level sample sizes in many meta-analyses, such as the example in [30], one cannot take refuge in asymptotic arguments. DerSimonian and Kacker [31] proposed a general method-of-moments estimate for \ud835\udf0f2.F o ra n yfi x e d positive constants a1,\u2026,ak, they define ya=\u03a3iaiyi/\u03a3iai; equate Qa=\u03a3iai(yi\u2212ya)2to its expected value, expressed in terms of \ud835\udf0f2,t h e\ud835\udf0e2 i, and the ai;s o l v ef o r \ud835\udf0f2; and substitute s2 1,\u2026,s2 kfor\ud835\udf0e2 1,\u2026,\ud835\udf0e2 k. In most of their illustrative special cases, however, aiis a function of s2 i, rendering the derivation of the expected value of Qainvalid. For example, Hartung and Knapp [34] and, separately, Sidik and Jonkman [35] developed an alternative procedure for the overall mean effect and based their test and confidence interval on the t-distributionwith k\u22121degreesoffreedom.IntHout et al.[36]showed,bysimulation,thattheHartung\u2013 Knapp\u2013Sidik\u2013Jonkman method consistently had more-adequate coverage than the DerSimonian\u2013Lairdmethod. Unfortunately, the Hartung\u2013Knapp\u2013Sidik\u2013Jonkman method uses the DerSimonian\u2013Lairdpoint estimate of the overall mean effect, which has potential for bias, and the analysis of IntHout et al.w a s unableto reveal bias. Discussion Theimpetusforthispapercamefromreadingseveralarticlesthatused\u2018Cochran\u2019s Qtest\u2019;someofthem citedCochran\u2019s1954paper.Havingreadthatpaper,Iwassurprised.Itseemedclearthatthoseauthorshad notstudiedthatpaper.Iftheyhad,theywouldhaveknownthatCochrandeliberatelydidnotuse Qitself to test for the presence of heterogeneity (in the form of interactions\u2014actually experiment effects) and that thenull distributionof Qonlyapproaches chi-squaredon k\u22121degrees of freedomasymptotically. Ofcourse,useof Qtotestforheterogeneityhasbecomewidespread,andmanybooksandpaperssay that the null distribution is chi-squared on k\u22121 degrees of freedom (and do not mention that it is only approximate,needinglargewithin-studysamplesizes).Thus,readers,seeingareferencetoCochran[1], caneasilyarriveat\u2018Cochran\u2019s Qtest\u2019.InformationfromCochran\u2019spaper,togetherwithresultsfromthe workofKulinskaya et al.[13,14],providesanaccuratedescriptionofthebehaviorof Qandshouldhelp tocounteract someof the widespreadmisinformation. What they should do instead is the subject of ongoing research.As mentioned in Section 4, however, reasonable answers are available for several common measures of effect. For the standardized mean difference, one can refer Qto a chi-squared distribution whose degreesoffreedom(usuallynotaninteger)isestimatedbyusingacomplicatedexpansiontoapproximate E[Q], the theoretical expected value of Q. To minimize the computational burden, Kulinskaya et al. [14] developed software in R. In their examples, E[ Q] is somewhat less than k\u22121, and the decrease is enough to affect the result of the test. For the risk difference, the null distribution of Qrequires a differentapproximation,basedonagammadistributionwhosemeanandvariancematchthoseof Q[13]. The log of the odds ratio poses a greater challenge, but it has a simpler answer. As a basis for approximating the distribution of Q by a gamma distribution, the expansion for E[ Q] is satisfactory (after an adjustment), but the expansion Copyright \u00a92015 John Wiley&Sons, Ltd. Statist. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License D. C. HOAGLIN for E[ Q2] is woefully inaccurate. Instead, they use the results of extensive simulations to approximate var[Q] closely as a quadratic function of E[ Q]. Then matching on the mean and variance produces a gamma-distribution approximation for the null distribution of Q. The distribution of Qis approximately (in large samples), and not exactly, chi-squared because the weights are 1/s 2 iand not 1/\ud835\udf0e2 i. In theoretical work, it is customary to assume that one can use the s2 i as if they were the \ud835\udf0e2 i; but this assumption can lead to problems, and it is supported by little, if any, empirical evidence in moderate and small samples. Thus, results that rely on this assumption (such as those claiming to derive the exact distribution of Cochran\u2019s Q)are mainly of theoretical interest; their relevance to actual meta-analyses remains unproven. These unnecessarily restric- tive choices are unfortunate, because they prevent the simulation from producing desirable empiricalinformation on the behavior of Q(and,often, ofother statistics). And, even if the null distribution of Qwere chi-squared on k\u22121 degrees of freedom, it would be an overstatement to interpret all values of Qthat exceed its mean ( k\u22121)as evidence of heterogene- ity. That choice tends to produce biased estimates of \ud835\udf0f2(e.g., Malzahn et al. [40], Sidik and Jonkman [41]) and has the potential for substantial bias in the pooled estimate of effect size (e.g.,Emerson et al.[32],Raghunathan and Ii[42],B\u00f6hning et al. [43], and Hamza et al.[44]). From the starting point of Qand \u2018Cochran\u2019s Qtest\u2019, the path of this paper has led naturally to two otherwidelyusedmethods, I2andtheDerSimonian\u2013Lairdprocedure.Allthreehaveshortcomings,some potentially serious, that point to a need for modifications in the current practice of meta-analysis. Problems arising in the analysis of a series of similar experiments. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License D.C.HOAGLIN 12. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License"
    }
  ]
}