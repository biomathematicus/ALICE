{
  "CONFIG": {
    "general_instructions": "This project is an AI assistant for a given lesson. The output of each request must be response that guides an student in understanding a topic.  \\n",
    "description": "The Data and AI Intensive Research with Rigor and Reproducibility (DAIR\u00b3) program is funded by Award 5R25GM151182 of the National Institute of General Medical Sciences, one of the 27 institutes of the National Institutes of Health of the United States. The principal investigators are Jing Liu (University of Michigan) and Juan B. Guti\u00e9rrez (University of Texas at San Antonio). \\nThe rigor of scientific research and the reproducibility of research results are essential for the validity of research findings and the trustworthiness of science. However, research rigor and reproducibility remains a significant challenge across scientific fields, especially for research with complex data types from heterogeneous sources, and long data manipulation pipelines. This is especially critical as data science and artificial intelligence (AI) methods emerge at lightning speed and researchers scramble to seize the opportunities that the new methods bring.  \\n\\nWhile researchers recognize the importance of rigor and reproducibility, they often lack the resources and the technical know-how to achieve this consistently in practice. With funding from the National Institutes of Health, a multi-university team offers a nationwide program to equip faculty and technical staff in biomedical sciences with the skills needed to improve the rigor and reproducibility of their research, and help them transfer such skills to their trainees. \\n\\nTrainees will then be guided over a one-year period to incorporate the newly acquired mindset, skills and tools into their research; and develop training for their own institutions.  \\n\\nThe DAIR3 team and instructors include faculty and staff research leaders from the University of Michigan, the College of William and Mary, Jackson State University, and University of Texas San Antonio. This highly diverse team will model the culture of diversity that we promote, and will support trainees who are demographically, professionally and scientifically diverse, and are from a diverse range of institutions, including those with limited resources.",
    "harmonizer_code": "gpt-4o",
    "harmonizer_temperature": 0.1,
    "harmonizer_name": "Harmonizer"
  },
  "MODELS": [
    {
      "model_code": "gpt-4o",
      "model_name": "OpenAI GPT 4o",
      "temperature": 0.15,
      "agent_name": "ALICE AI Agent"
    }
  ],
  "knowledgeBase": [
    {
      "file": "C:\\temp\\Units\\UNIT_3 - Rigorous Statistical Design - Kerby\\design.pdf",
      "summary": "\u2013For those interested, here is a readable account of the work of the very in\ufb02uential philosopher of science Karl Popper. \u2013There are many other lines of work contributing to the philososphy of science including the work of Greeks such as Aristotle and Persians including Ibn Sina. \u2013Design refers to the process of developing a rationale and goals for the research, stating (falsi\ufb01able) hypotheses, and documenting how the data will be collected, analyzed, and interpreted. \u2013Data analysis refers to the execution of a data analysis plan, which in- cludes carrying out the analysis, interpreting the results, and formally assessing the uncertainty of all \ufb01ndings. For example, if we are interested in a certain type or category of human beings, all the people in this category are viewed as \"replicates\" 1of the person of interest. The set of all humans of interest is the population and the speci\ufb01c humans that we observe and (potentially) manipulate is the sample. \u2013Acontrolled study involves a comparison to a group (arm) that is untreated, or that is treated with a conventional well-understood treatment (e.g. \"standard of care\" in clinical research). \u2013A randomized controlled study (or \"trial\", RCT) is a controlled study in which the subjects are assigned to the treatment or control group by randomization. Exposures happen naturalistically and we can assess at the end of the study which exposures occurred and what outcomes followed the occurrence of these exposures. 2\u2013In a case/control study units are deliberately selected at the outset to belong to contrasting states of a factor of interest. \u2013An adaptive interventional study is one in which the treatment as- signments depend on the results of previously collected data within the same study, based either on an interim analysis or oncontinual reassessment of \ufb01ndings. For example, in the context of measuring blood pressure, there is a well-known systematic error known as \"white coat hypertension\" in which some people\u2019s blood pressure rises due to the stress of interacting with the person doing the measurement. Using blood pressure again as an example, there are substantial random errors due to the skill required to accurately detect a pulse as the blood pressure cu\ufb00 is depressurized. The variance of these random errors may depend on the skill of the person doing the measurement. A typical example is where we wish to study the yield of a chemical reaction at a speci\ufb01c temperature. We set the temperature to a desired level $T$ and use a heating device to produce the desired temperature. On warm days with little rain, people enjoy eating ice cream and these are the same conditions that might lead people to go to the pool or the beach. \u2013When designing a study, one or more of the following strategies are usually employed to mitigate the risks of confounding. They are listed 5in decreasing order of rigor: \u2217Randomization is usually the best way to limit or eliminate the risk of confounding. If the treatment is assigned at random, it is impossible for it to be causally in\ufb02uenced by any factor (regardless of whether the factor is known or unknown, measured or unmeasured). \u2013Both of the above examples can be viewed in a stylized way by considering the partial correlation coe\ufb03cient, which in the population satis\ufb01es the relationship ${\\rm Cor}(X, Y | Z) = ({\\rm Cor}(X, Y) - {\\rm Cor}(X, Z)\\cdot{\\rm Cor}(Y,Z))/\\sqrt{(1-{\\rmCor}(X,Z)\u02c62)\\cdot(1-{\\rmCor}(Y,Z)\u02c62)}$ Suppose that $X$ is $BMI$ and $Y$ is mortality risk, with marginal correlation ${\\rm Cor}(X, Y) > 0$. If the latter two correlations are strong compared to the \ufb01rst, conditioning on smoking can swing the apparent association between BMI and mortality from positive to negative. 6\u2022Formal analysis of confounding often takes place in the context of the Neyman-Rubin causal model, which posits the existence of potential out- comes. \u2013In mediation analysis, a third variable $M$ called the mediator is posited as a causal intermediary between the exposure and the out- come. As a simple example, $Z$ could be sex, and it may be that the relationship between the exposure and outcome di\ufb00ers between females and males. Confounding will generally lead to a lack of balance, but even if there is no confounding, there can be a lack of balance, especially when the sample size is small. \u2022The most basic type of randomization is simple randomization , where each unit is independently assigned to a treatment group, either with uniform probabilities (equal probability of assignment to each arm) or with probabilities that are pre-determined to achieve desired relative group sizes (e.g. assigning to treatment with twice the probability of assigning to control). In 1:1 strati\ufb01ed randomization we would assign exactly 30 of the people with hypertension to the treatment arm, and exactly 15 of the people without hypertension to the treatment arm. \u2013Minimization is a class of methods that addresses the practical issue that in many research studies, subjects are recruited over time, and we do not have a listing of the subjects and their measured confounders at the outset of the study. \u2022A concern in studies where units are recruited sequentially is bias on the part of the research team in any decisions that could in\ufb02uence recruitment of subjects. If subjects do not actually receive the treatment to which they are randomized, one of the following three approaches can be adopted: \u2013In an intention to treat analysis, subjects are analyzed as belonging to 8thetreatmentgrouptowhichtheywereassigned, regardlessofwhether they received and fully complied with the treatment. It is the only option that retains the bene\ufb01ts of randomization in terms of eliminating confounding from both measured and unmeasured variables. \u2013In anas-treated analysis, subjects are analyzed as belonging to the treatment that they received and complied with, even if this treatment is di\ufb00erent from the one to which they were assigned. That is, we posit that our data are a random sample from a probability distribution $P_\\theta$, where $\\theta$ is a parameter that captures aspects of the scienti\ufb01c research question. Formally, this in- volves devising a function $\\hat{\\theta}(D)$, where $D$ is the observed data, such that $\\hat{\\theta}(D)$ is likely to be close to the true parameter value $\\theta$. \u2022It is common to refer to $P_\\theta$ as the population, $D$ as the sample, and $\\hat{\\theta}$ as the parameter estimate . \u2022There are many ways to obtain parameter estimates, two of the most common are the method of moments, and maximum likelihood analysis. It is the standard deviation of the sampling 9distribution of the random variable $\\hat{\\theta}$, which is induced by the underlying distribution of the data, $P(D)$. \u2022If the sampling distribution of $\\hat{\\theta}$ is approximately Gaussian, then the standard error is all one needs to fully characterize the estimation errors of an unbiased estimator. Statistical power \u2022Statistical power is a measure of how likely a study is to yield a positive \ufb01nding, if a positive \ufb01nding is the true state of the system being studied. Usually this refers to the power of a hypothesis test, referring to the probability of rejecting the null hypothesis when the null hypothesis is false. For example, we could have high power to reject the null hypothesis in a situation where, due to bias, rejecting the null does not re\ufb02ect the claimed level of evidence. \u2022Power analysis is strongly linked to the notion of e\ufb00ect size, which quanti\ufb01es the strength of an e\ufb00ect relative to all sources of variation. Typically this involves one of the following: \u2013Find the sample size necessary to have a de\ufb01ned power (usually 80%) to detect a given e\ufb00ect size. \u2022Using statistical theory, it is possible to develop a detailed understanding of the factors that in\ufb02uence the power in any given setting. \u2022As a basic example, the standard error of the mean is $\\sigma/\\sqrt{n}$, where $\\sigma\u02c62$ is the variance and $n$ is the sample size. Based on this expression, we know that the only factors that determine the standard error for mean estimation are the sample size and the variance, and that they combine as a speci\ufb01c rational function. While greater sample sizes in any group corresponds to improved power, the sample size of the smallest group generally has the greatest impact on power. That is, if the treatment group has sample size $n_t$ and residual variance $\\sigma_t\u02c62$, the power for many tests is related to the ratio $n_t/\\sigma_t\u02c62$. Doubling the sample size or reducing the residual variance by a factor of two have equal impacts on the power. To address this, each plot can be divided in half, and the two halves are randomly assigned, one to the treatment and one to the control. Surveys \u2022Asample survey is a research tool in which the goal is to quantify the state of a population, with a primary focus on achieving low bias for a de\ufb01ned target population. In the case of surveys that involve interviewing human subjects, a questionnaire ,survey form , orsurvey instrument refers to the actual assessment items that are used for each unit. the goal of a survey is not usually to assess the e\ufb00ect of an exposure or intervention, and a survey does not usually have \"arms\" corresponding to di\ufb00erent treatments or exposures. \u2022A special type of survey is a census, which aims to measure the entire population, rather than measuring only a sample of a population. Sampling \u2022In surveys, as well as in some other contexts, it is important to carefully sample units from a population in such a way that unbiased and precise results can be obtained from the sample. 12\u2022The most basic type of sampling is to obtain a simple random sample (SRS), which is a sample of size $k$ from a population of size $n$ in which any subset of size $k$ is equally likely to be selected. For example, if we want to sample the employees of a company or the students enrolled in a school, a sampling frame would generally be available and it would be practical to obtain a simple random sample from it. \u2022In some cases sampling probabilities are adjusted to increase statistical power for comparisons of interest, even if this produces a sample that is not representative of the overall population. For example, suppose that one of the research goals is to compare outcomes between Black and White subpopulations, in a setting where the White population is, say, 3 times greater than the Black population. One possibility would be to sample the same number of Black and White subjects, which would maximize statistical power for comparisons between these two races (if the variances within the two races are equal -- if the variances are unequal we would want $n_b/\\sigma_b\u02c62 = n_w/\\sigma_w\u02c62$, where $n_b$ and $n_w$ are the Black and White sample sizes and $\\sigma_b\u02c62$ and $\\sigma_w\u02c62$ are the response variances for Black and White subjects). If we, say, over-sample Black compared to White subjects at a rate of 3 to 1, then when estimating population parameters (not 13race-speci\ufb01c parameters), we would weight the White respondents 3 times more than the Black respondents to compensate for the biased sampling. Left truncation refers to a form of selection bias in which an observation cannot be made unless the event of interest occurs after a truncation time. For example, if we are considering death due to a speci\ufb01c disease, using records from a particular health care system, we can never observe people who died of the disease without enrolling in the health care system. \u2013Right censoring refers to a form of partially observed data in which we know that an event of interest did not occur before a speci\ufb01c time, but we do not know when the event occurred (if ever). Right censoring is very common in health studies where some units in the sample have not yet had an event at a particular time, e.g. the time when the data were obtained for analysis. \u2013Left censoring is less common than right censoring, but it can occur, for example, if a measurement falls below a \"limit of detection\" (e.g. the concentration of a chemical in a blood sample is below the limit of detection for the assay used to assess the chemical). \u2013Multiple imputation is a framework in which multiple datasets are created, each one of which has the missing data imputed randomly from a distribution that correctly re\ufb02ects its conditional mean and variance, i.e. from $P(X_{\\rm miss} | X_{\\rm obs})$. The analysis is them conducted independently on each imputed data set, and the point estimates and sampling variances for each imputed data set are pooled to a single point estimate and single sampling variance using acombining rule . \u2013Full information maximum likelihood (FIML) is an approach in which a likelihood for the \"complete data\" is marginalized to a likelihood for the observed data. \u2022Since ${\\rm Var}(d) = {\\rm Var}(a) + {\\rm Var}(b) - 2{\\rm Cov}(a, b)$, if $a$ and $b$ are positively correlated, the variance of $d$ is less than the sum of the variance of $a$ and the variance of $b$. The value of $\\beta_0$ captures the systematic change from baseline to follow- up, and the term $\\beta_1 a$ adjusts for baseline severity. \u2022Let $a\u02c6c_i$ and $b\u02c6c_i$ denote the baseline and follow-up measurements for subjects in the control arm and let $a\u02c6t_i$ and $b\u02c6t_i$ denote the baseline and follow-up measurements for subjects in the treated arm. \u2022The most basic way to conduct a strati\ufb01ed analysis is to compute the parameter of interest separately within each stratum, and then to pool these stratum-level estimates into an overall estimate. A com- mon situation is that the treatment e\ufb00ect estimate is attenuated (shifted toward the null value) when comparing the estimate obtained using strati- \ufb01cation to the naive estimate. Regression adjustment \u2022Regression analysis is a broad class of techniques that can be used to relate the value of an outcome to the values of one or more explanatory variables . The most basic form of regression analysis is linear regression , usingordinary least squares to \ufb01t the models to data (i.e. to estimate the model parameters). \u2022A basic example is the linear model $E[Y|X,Z] = \\beta_0 + \\beta_1 X + \\beta_2 Z$, where $Y$ is the outcome, $X$ is the treatment or exposure, and $Z$ is a potential confounder. Under certain rather strong assumptions, an estimate of the coe\ufb03cient $\\beta_1$ can be used to assess the relationship between the exposure $X$ and the outcome $Y$, while controlling for the confounder $Z$. One may argue that this is a natural experiment since the factors that led to earlier introduction of cable services may be primarily driven by logistical factors so that the people who gained access to cable TV in, say, 1984 may not be systematically di\ufb00erent from those who gained access to cable TV in 1985. However, we can consider only the subset of people with, say, baseline SBP equal to 129 compared to the subset with baseline SBP equal to 130. Given the inherent measurement error in a single SBP reading, we might argue that for people with baseline SBP between 129 and 130, the treatment assignment is e\ufb00ectively random. Thegoalof1:1matching is to identify a set of pairs $1 \\le j_{1i}, j_{2i} \\le n$ such that subject $j_{1i}$ is exposed, i.e. $x_{j_{1i}}=1$, subject $j_{2i}$ is not exposed, i.e. $x_{j_{2i}}=0$, and subjects $j_{1i}$ and $j_{2i}$ are similar in terms of the covariates, i.e. $z_{j_{1i}}\\approx z_{j_{2i}}$. \u2022As a consequence of the matching, the matched treated subjects should be approximately balanced with respect to the matched untreated subjects. \u2022Matching requires the treated and untreated subjects to have a common supportin the domain of the covariates. If the treated units are much older than the untreated units, and only 5 of the treated units are younger than the oldest untreated unit, then there is very little common support and matching is unlikely to be e\ufb00ective. \u2022In a matching analysis, some of the controls may be unused, and some of the controls may be used in multiple matched sets. \u2013Strati\ufb01ed analyses can be conducted in which we partition the sample into, say, 5 strata based on the propensity scores, estimate treatment e\ufb00ects within each stratum, and then pool the results to yield an overall estimate. \u2013Propensity scores can be used as weights in inverse probability weight- ing Synthetic controls \u2022A synthetic control is a calculated value for each treated unit that estimates the \"counterfactual\" value that the unit would have had had it not been treated. \u2022Let $x\u02c6\\star$ denote the $p$-dimensional vector of covariates associated with a treated unit and let $X$ denote the $m\\times p$ matrix of covariate valuesforallunitsassignedtothecontrolarm(inmanycasesonlyuntreated units with covariates \"su\ufb03ciently similar\" to $x\u02c6\\star$ are included). \u2022Once the weight vector $w$ is constructed, we estimate the counterfactual control value $y\u02c6*$ using $w\u02c6\\prime y$, where $y$ is the $m\\times 1$ vector containing the response values for the controls in $X$. However when using linear regression, the weights $w\u02c6*$ can be negative, and therefore the synthetic control may not lie within the domain of the data (i.e. it may be an extrapolation)."
    }
  ]
}