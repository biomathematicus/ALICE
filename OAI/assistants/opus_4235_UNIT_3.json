{
  "CONFIG": {
    "general_instructions": "This project is an AI assistant for a course. The output of each request must be a response that guides a student in understanding a topic.",
    "description": "The Data and AI Intensive Research with Rigor and Reproducibility (DAIR-3) program is funded by Award 5R25GM151182 of the National Institute of General Medical Sciences, one of the 27 institutes of the National Institutes of Health of the United States. The principal investigators are Jing Liu (University of Michigan) and Juan B. Guti\u00c3\u00a9rrez (University of Texas at San Antonio). \\nThe rigor of scientific research and the reproducibility of research results are essential for the validity of research findings and the trustworthiness of science. However, research rigor and reproducibility remains a significant challenge across scientific fields, especially for research with complex data types from heterogeneous sources, and long data manipulation pipelines. This is especially critical as data science and artificial intelligence (AI) methods emerge at lightning speed and researchers scramble to seize the opportunities that the new methods bring.  \\n\\nWhile researchers recognize the importance of rigor and reproducibility, they often lack the resources and the technical know-how to achieve this consistently in practice. With funding from the National Institutes of Health, a multi-university team offers a nationwide program to equip faculty and technical staff in biomedical sciences with the skills needed to improve the rigor and reproducibility of their research, and help them transfer such skills to their trainees. \\n\\nTrainees will then be guided over a one-year period to incorporate the newly acquired mindset, skills and tools into their research; and develop training for their own institutions.  \\n\\nThe DAIR3 team and instructors include faculty and staff research leaders from the University of Michigan, the College of William and Mary, Jackson State University, and University of Texas San Antonio. This highly diverse team will model the culture of diversity that we promote, and will support trainees who are demographically, professionally and scientifically diverse, and are from a diverse range of institutions, including those with limited resources.",
    "harmonizer_code": "gpt-4o",
    "harmonizer_temperature": 0.1,
    "harmonizer_name": "Harmonizer"
  },
  "MODELS": [
    {
      "model_code": "gpt-4o",
      "model_name": "OpenAI GPT 4o",
      "temperature": 0.15,
      "agent_name": "ALICE AI Agent"
    }
  ],
  "knowledgeBase": [
    {
      "file": "C:\\temp\\Units\\UNIT_3 - Rigorous Statistical Design - Kerby\\design.pdf",
      "summary": "\u2013Design refers to the process of developing a rationale and goals for the research, stating (falsi\ufb01able) hypotheses, and documenting how the data will be collected, analyzed, and interpreted. \u2013Data analysis refers to the execution of a data analysis plan, which in- cludes carrying out the analysis, interpreting the results, and formally assessing the uncertainty of all \ufb01ndings. The set of all humans of interest is the population and the speci\ufb01c humans that we observe and (potentially) manipulate is the sample. For example, in the context of measuring blood pressure, there is a well-known systematic error known as \"white coat hypertension\" in which some people\u2019s blood pressure rises due to the stress of interacting with the person doing the measurement. \u2013Both of the above examples can be viewed in a stylized way by considering the partial correlation coe\ufb03cient, which in the population satis\ufb01es the relationship ${\\rm Cor}(X, Y | Z) = ({\\rm Cor}(X, Y) - {\\rm Cor}(X, Z)\\cdot{\\rm Cor}(Y,Z))/\\sqrt{(1-{\\rmCor}(X,Z)\u02c62)\\cdot(1-{\\rmCor}(Y,Z)\u02c62)}$ Suppose that $X$ is $BMI$ and $Y$ is mortality risk, with marginal correlation ${\\rm Cor}(X, Y) > 0$. \u2022The most basic type of randomization is simple randomization , where each unit is independently assigned to a treatment group, either with uniform probabilities (equal probability of assignment to each arm) or with probabilities that are pre-determined to achieve desired relative group sizes (e.g. assigning to treatment with twice the probability of assigning to control). In 1:1 strati\ufb01ed randomization we would assign exactly 30 of the people with hypertension to the treatment arm, and exactly 15 of the people without hypertension to the treatment arm. \u2013Minimization is a class of methods that addresses the practical issue that in many research studies, subjects are recruited over time, and we do not have a listing of the subjects and their measured confounders at the outset of the study. If subjects do not actually receive the treatment to which they are randomized, one of the following three approaches can be adopted: \u2013In an intention to treat analysis, subjects are analyzed as belonging to 8thetreatmentgrouptowhichtheywereassigned, regardlessofwhether they received and fully complied with the treatment. It is the standard deviation of the sampling 9distribution of the random variable $\\hat{\\theta}$, which is induced by the underlying distribution of the data, $P(D)$. \u2022If the sampling distribution of $\\hat{\\theta}$ is approximately Gaussian, then the standard error is all one needs to fully characterize the estimation errors of an unbiased estimator. Statistical power \u2022Statistical power is a measure of how likely a study is to yield a positive \ufb01nding, if a positive \ufb01nding is the true state of the system being studied. Usually this refers to the power of a hypothesis test, referring to the probability of rejecting the null hypothesis when the null hypothesis is false. For example, we could have high power to reject the null hypothesis in a situation where, due to bias, rejecting the null does not re\ufb02ect the claimed level of evidence. \u2022As a basic example, the standard error of the mean is $\\sigma/\\sqrt{n}$, where $\\sigma\u02c62$ is the variance and $n$ is the sample size. Based on this expression, we know that the only factors that determine the standard error for mean estimation are the sample size and the variance, and that they combine as a speci\ufb01c rational function. To address this, each plot can be divided in half, and the two halves are randomly assigned, one to the treatment and one to the control. Surveys \u2022Asample survey is a research tool in which the goal is to quantify the state of a population, with a primary focus on achieving low bias for a de\ufb01ned target population. the goal of a survey is not usually to assess the e\ufb00ect of an exposure or intervention, and a survey does not usually have \"arms\" corresponding to di\ufb00erent treatments or exposures. 12\u2022The most basic type of sampling is to obtain a simple random sample (SRS), which is a sample of size $k$ from a population of size $n$ in which any subset of size $k$ is equally likely to be selected. For example, if we want to sample the employees of a company or the students enrolled in a school, a sampling frame would generally be available and it would be practical to obtain a simple random sample from it. For example, suppose that one of the research goals is to compare outcomes between Black and White subpopulations, in a setting where the White population is, say, 3 times greater than the Black population. One possibility would be to sample the same number of Black and White subjects, which would maximize statistical power for comparisons between these two races (if the variances within the two races are equal -- if the variances are unequal we would want $n_b/\\sigma_b\u02c62 = n_w/\\sigma_w\u02c62$, where $n_b$ and $n_w$ are the Black and White sample sizes and $\\sigma_b\u02c62$ and $\\sigma_w\u02c62$ are the response variances for Black and White subjects). If we, say, over-sample Black compared to White subjects at a rate of 3 to 1, then when estimating population parameters (not 13race-speci\ufb01c parameters), we would weight the White respondents 3 times more than the Black respondents to compensate for the biased sampling. \u2013Left censoring is less common than right censoring, but it can occur, for example, if a measurement falls below a \"limit of detection\" (e.g. the concentration of a chemical in a blood sample is below the limit of detection for the assay used to assess the chemical). \u2022Since ${\\rm Var}(d) = {\\rm Var}(a) + {\\rm Var}(b) - 2{\\rm Cov}(a, b)$, if $a$ and $b$ are positively correlated, the variance of $d$ is less than the sum of the variance of $a$ and the variance of $b$. \u2022Let $a\u02c6c_i$ and $b\u02c6c_i$ denote the baseline and follow-up measurements for subjects in the control arm and let $a\u02c6t_i$ and $b\u02c6t_i$ denote the baseline and follow-up measurements for subjects in the treated arm. A com- mon situation is that the treatment e\ufb00ect estimate is attenuated (shifted toward the null value) when comparing the estimate obtained using strati- \ufb01cation to the naive estimate. Regression adjustment \u2022Regression analysis is a broad class of techniques that can be used to relate the value of an outcome to the values of one or more explanatory variables . The most basic form of regression analysis is linear regression , usingordinary least squares to \ufb01t the models to data (i.e. to estimate the model parameters). \u2022A basic example is the linear model $E[Y|X,Z] = \\beta_0 + \\beta_1 X + \\beta_2 Z$, where $Y$ is the outcome, $X$ is the treatment or exposure, and $Z$ is a potential confounder. Under certain rather strong assumptions, an estimate of the coe\ufb03cient $\\beta_1$ can be used to assess the relationship between the exposure $X$ and the outcome $Y$, while controlling for the confounder $Z$. One may argue that this is a natural experiment since the factors that led to earlier introduction of cable services may be primarily driven by logistical factors so that the people who gained access to cable TV in, say, 1984 may not be systematically di\ufb00erent from those who gained access to cable TV in 1985. Thegoalof1:1matching is to identify a set of pairs $1 \\le j_{1i}, j_{2i} \\le n$ such that subject $j_{1i}$ is exposed, i.e. $x_{j_{1i}}=1$, subject $j_{2i}$ is not exposed, i.e. $x_{j_{2i}}=0$, and subjects $j_{1i}$ and $j_{2i}$ are similar in terms of the covariates, i.e. $z_{j_{1i}}\\approx z_{j_{2i}}$. If the treated units are much older than the untreated units, and only 5 of the treated units are younger than the oldest untreated unit, then there is very little common support and matching is unlikely to be e\ufb00ective. \u2022Once the weight vector $w$ is constructed, we estimate the counterfactual control value $y\u02c6*$ using $w\u02c6\\prime y$, where $y$ is the $m\\times 1$ vector containing the response values for the controls in $X$."
    }
  ]
}