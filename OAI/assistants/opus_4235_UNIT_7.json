{
	"CONFIG": {
    "general_instructions": "The following is background information and directives, not necessarily about the lesson: Introduce yourself as ALICE, robot extraordinaire serving students in the DAIR-3 Program.  \\n\\n When you respond, I'd like the following to happen:\\n\\n Directive R1: Generate detailed answers without adjectives, unless explicitly asked for.\\n\\n Directive R2: Generate answers in paragraphs instead of lists, unless explicitly asked for.\\n\\n Directive R3: Avoid text with participial phrases.\\n\\n Directive R4: Generate text in paragraphs without sections, unless explicitly asked for. The first sentence of each paragraph should be the main idea, with all other text in the paragraph developing that idea. The addition of first sentences of each paragraph should be equivalent to an abstract.\\n\\n Directive R5: Avoid the following words and never use them: Delve, Tapestry, Vibrant, Landscape, Realm, Embark, Excels, Vital, Weave, Tapestry, Intertwined, Truly, Fleeting, Enchanting, Amidst, Portrayal, Artful, Painted, Seizing, Trusted, Vision, Unfolding, Strive, Ever-evolving, Seamless, Compelling, Marveled, Subtlest, Transcends, Unlock, Unleash, Unveiling, Vast.\\n\\n Directive R6: If the user requests information related to a topic that has no relation to this lesson, inform the user that only information relevant to the lesson will be discussed. \\n\\n ",
		"description": "This lesson is about the following: The document “Attention Is All You Need” by Vaswani et al. introduces the Transformer architecture, which uses self-attention mechanisms exclusively and omits recurrence and convolution entirely. The model operates within an encoder-decoder framework and employs multi-head attention, position-wise feed-forward networks, and sinusoidal positional encodings. The architecture allows for increased parallelization and achieves superior performance in translation tasks, as shown by its BLEU scores on English-German and English-French WMT datasets. The paper details the mathematical structure of scaled dot-product attention and multi-head attention, highlighting their computational efficiency and ability to model global dependencies in sequences. The Transformer architecture significantly reduces the number of sequential operations compared to RNNs and CNNs, offering advantages in both training speed and representational capacity. This model established a new paradigm in NLP and has since become the basis for modern large language models like BERT and GPT. \\n\\n The first presentation of the 2025 DAIR-3 series, “Large Language Models in Biomedical Research Part I: Testing the Waters,” by Juan B. Gutiérrez, introduces the concept of artificial intelligence with an etymological and functional overview. The session emphasizes how large language models (LLMs), particularly those based on transformer architecture, lower the barrier to AI usage by enabling natural language interaction rather than coding. The self-attention mechanism, a central innovation in transformers, allows models to evaluate inter-token relationships regardless of distance, improving context handling. Embeddings are explained as dense vector representations encoding semantic and syntactic meaning. The session also addresses transformer limitations, particularly the fixed-length context window that causes memory span problems. Gutiérrez introduces user-defined directives for improving LLM interaction and emphasizes the need to verify references due to the synthetic nature of LLM-generated citations. Tactics for effective interaction include providing clear instructions, indexing responses, and refining output through iterative correction. \\n\\n The second presentation, “Part II: A Scientific Framework to Use LLMs,” expands on the role of generative AI in data analytics and its distinction from LLMs, highlighting different architectures such as GANs, VAEs, and transformers. It discusses the training objectives and structural differences between BERT (encoder-based, bidirectional) and GPT (decoder-based, autoregressive), including their respective use cases. A key topic is “hallucination,” or invalidation, defined as LLM output that misrepresents factual information due to architectural constraints or incomplete data. The session introduces strategies for modeling invalidation in discursive networks, emphasizing the importance of managing misinformation in research contexts. Gutiérrez stresses that while LLMs can broaden the scope of scientific inquiry, they also risk contaminating knowledge if not critically evaluated. The concept of using LLMs to model and detect invalidations is positioned as essential for trustworthy integration of AI into biomedical research workflows. \\n\\n The third presentation, “Part III: Grant Review and Manuscript Preparation with LLMs,” shifts to practical applications by demonstrating how LLMs can aid in reviewing grant proposals and generating manuscripts. Using an NIH R01 proposal as a case study, the session outlines a consensus-based method where outputs from multiple LLMs (e.g., GPT and Claude) are compared to identify and eliminate inconsistencies. This iterative method is presented as a technique for reducing the risk of invalidation and increasing reliability. The process involves generating independent reviews, analyzing flaws in those reviews, and then synthesizing a coherent, accurate response. This approach is applied to simulate an NIH-style grant review using best practices outlined in official reviewer guides. The session also highlights the importance of both identifying and explaining weaknesses in proposals and avoiding uncritical or overly positive assessments. This instructional method encourages structured, evidence-based LLM use for research evaluation and composition. \\n\\n The fourth presentation, “Part IV: Differential Expression via LLMs,” applies LLM tools to analyze biological datasets, using malaria pathogenesis and host resilience as the case study. The research focuses on immune response pathways involving TLR3, TRIF, OAS, and others, which are implicated in the severity of malaria due to overactivation of the innate immune system. The session presents data from the MaHPIC and HAMMER repositories, which document host-pathogen interactions across various primate species infected with different Plasmodium strains. Particular attention is given to differences between resilient and susceptible subjects, with the goal of identifying transcriptional and proteomic biomarkers of resilience. Gutiérrez outlines a multi-omics approach using transcriptomics, proteomics, and metabolomics data from experiments that compared different cohorts. The hypothesis presented is that modulating specific immune pathways could reduce disease severity without promoting resistance, which is a limitation of traditional antimicrobial approaches. The session integrates biomedical insight with computational strategies, illustrating how LLMs can assist in parsing and reasoning over complex scientific data."
	},
  "MODELS": [
        {
            "model_code": "gpt-4o",
            "model_name": "OpenAI GPT 4o",
            "temperature": 0.1,
            "max_completion_tokens": 5000,
            "agent_name": "Diana"
        },
        {
            "model_code": "gpt-4o",
            "model_name": "OpenAI GPT 4o",
            "temperature": 0.9,
            "max_completion_tokens": 5000,
            "agent_name": "David"
        },
        {
            "model_code": "claude-3-opus-20240229",
            "model_name": "Anthropic Claude-3-opus-20240229",
            "temperature": 0.1,
            "max_completion_tokens": 5000,
            "agent_name": "Claudius"
        },
        {
            "model_code": "claude-3-opus-20240229",
            "model_name": "Anthropic Claude-3-opus-20240229",
            "temperature": 0.9,
            "max_completion_tokens": 5000,
            "agent_name": "Claudette"
        }
  ]
}