{
  "CONFIG": {
    "general_instructions": "The DAIR-3 program, funded by the National Institute of General Medical Sciences, is led by Jing Liu and Juan B. Guti√©rrez. It aims to improve scientific rigor and reproducibility, especially in research involving complex data and long processing pipelines. These challenges are heightened by the fast pace of developments in data science and AI, which many researchers are not fully equipped to handle with consistent methodological soundness. \\n To address this, DAIR-3 offers a year-long national training program for faculty and technical staff in biomedical sciences. Participants gain skills to enhance the quality of their research and train others at their institutions. The program is delivered by a diverse team from multiple universities, with a focus on supporting individuals from varied backgrounds and under-resourced institutions. \\n The output of each request must be a response that guides the student in understanding a topic. When answering with LaTeX notation, use $ and $$ to enclose expressions; this include single commands, e.g. \\sigma, which should appear as $\\sigma$ do NOT use any other enclosure for equations. When using LaTeX notation, ensure that variables, cefficients and contants are enclosed by blank spaces. "
  },
  "knowledgeBase": [
    {
      "summary": "The DAIR-3 files present an overview of AI learning paradigms, focusing on supervised, unsupervised, reinforcement, and semi-supervised learning. They explain key AI methodologies including neural networks, statistical machine learning, probabilistic methods, symbolic AI, and evolutionary computation. The materials describe application domains such as natural language processing, computer vision, robotics, and decision support. The lectures emphasize error minimization through gradient descent and backpropagation. Large Language Models (LLMs) are introduced as neural networks trained on text for language tasks, highlighting their structure, uses, and challenges like hallucinations. Strategies for improving LLM accuracy, including consensus methods, are discussed, with practical exercises included."
    }
  ]
}